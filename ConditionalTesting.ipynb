{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loading required libraries\n",
    "\n",
    "First we load all the libraries needed for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy, chi2_contingency, norm, ttest_1samp\n",
    "import scipy.spatial.distance as distance\n",
    "import statsmodels.stats.multitest as multitest\n",
    "from folktables import ACSDataSource\n",
    "import statsmodels.api as sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Testing for distribution shifts\n",
    "\n",
    "In terms of hypothesis testing problems, the presence of distribution shifts can be modelled as testing for significant differences/effects. Asumme we observe two samples over variables $V$. We are now interested in understanding whether both samples are drawn from the same underlying distribution (i.e. if there is a distribution shift). Thus, we test the following hypothesis:\n",
    "$$H_0:P_a(V) = P_b(V) \\text{ vs } H_1:P_a(V) \\neq P_b(V)\\text{, where a and b denote the two different samples} $$\n",
    "\n",
    "\n",
    "Let's examine this problem by focussing on an example. Taken from example 4.1 in [Plečko and Bareinboim (2024)](https://causalai.net/r90.pdf), we assume we have access to the hiring data of two startups from location A. Additionally, we observe the data for one startup at location B. The variables are given by:\n",
    "- $Y \\in \\{0,1\\}$, denoting the hiring decision {no,yes} \n",
    "- $X \\in \\{0,1\\}$, denoting the applicant's gender {female,male}\n",
    "- $Z \\in \\{0,1\\}$, denoting the applicant's age group {young,old}\n",
    "- $W \\in \\{0,1\\}$, denoting whether the applicant holds a Phd {no,yes}\n",
    "\n",
    "The underlying data-generating structural causal model (SCM) for location A is given by:\n",
    "\\begin{align*}\n",
    "U &\\leftarrow \\textit{N}(0,1)\\\\\n",
    "X &\\leftarrow \\textit{Bernoulli}(\\text{expit}(U))\\\\ \n",
    "Z &\\leftarrow \\textit{Bernoulli}(\\text{expit}(U))\\\\ \n",
    "W &\\leftarrow \\textit{Bernoulli}(0.3|X-Z| + 0.1)\\\\\n",
    "Y &\\leftarrow \\textit{Bernoulli}(0.2(X+Z-2XZ) + 0.25W + 0.02) \n",
    "\\end{align*}\n",
    "\n",
    "Note that we adapted the original example such that:\n",
    "- $W$ has parents $X$ and $Z$ \n",
    "- $Y$ has always non-zero probability of $Y=1$\n",
    "\n",
    "Additionally, location B has a different mechanism for the hiring decision $Y$, given by:\n",
    "\\begin{align*}\n",
    "Y &\\leftarrow \\textit{Bernoulli}(0.2(X+Z-2XZ) + 0.25W + 0.11) \n",
    "\\end{align*}\n",
    "\n",
    "In reality we usually only have access to the observational distribution over $V = \\{X,Z,W,Y\\}$ and not the underlying SCM. We now generate two samples for location A and one for location B.  \n",
    "All samples are of size $n=1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sample hiring data\n",
    "\n",
    "def hiring_data(shift, n=5000,d=0.09):\n",
    "    \n",
    "    np.random.seed(36)\n",
    "    \n",
    "    U = np.random.normal(0, 1, n)\n",
    "    X = np.random.binomial(1,np.exp(U)/(1+(np.exp(U))),n)\n",
    "    Z = np.random.binomial(1,np.exp(U)/(1+np.exp(U)),n)\n",
    "    W = np.random.binomial(1,0.3*(X+Z) + 0.1,n)\n",
    "    Y = np.random.binomial(1,(1/5)*(X+Z)+(1/4)*W+0.02,n)\n",
    "    sample_1 = pd.DataFrame({'X':X, 'Z':Z, 'W':W, 'Y':Y})\n",
    "\n",
    "    if shift == 1:\n",
    "        U = np.random.normal(0, 1, n)\n",
    "        X = np.random.binomial(1,np.exp(U)/(1+(np.exp(U))),n)\n",
    "        Z = np.random.binomial(1,np.exp(U)/(1+np.exp(U)),n)\n",
    "        W = np.random.binomial(1,0.3*(X+Z) + 0.1,n)\n",
    "        Y = np.random.binomial(1,(1/5)*(X+Z)+(1/4)*W+0.02+d,n)\n",
    "        sample_2 = pd.DataFrame({'X':X, 'Z':Z, 'W':W, 'Y':Y})\n",
    "    \n",
    "    else:       \n",
    "        U = np.random.normal(0, 1, n)\n",
    "        X = np.random.binomial(1,np.exp(U)/(1+(np.exp(U))),n)\n",
    "        Z = np.random.binomial(1,np.exp(U)/(1+np.exp(U)),n)\n",
    "        W = np.random.binomial(1,0.3*(X+Z) + 0.1,n)\n",
    "        Y = np.random.binomial(1,(1/5)*(X+Z)+(1/4)*W+0.02,n)\n",
    "\n",
    "        sample_2 = pd.DataFrame({'X':X, 'Z':Z, 'W':W, 'Y':Y})\n",
    "    \n",
    "    return sample_1, sample_2\n",
    "\n",
    "sample_1, sample_2 = hiring_data(0)\n",
    "_ , sample_3 = hiring_data(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tv 0.32\n",
      "x_de 0.2\n",
      "x_ie -0.08\n",
      "x_se -0.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3227726488784822"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tv_sample_1 = sample_1[sample_1['X']==1]['Y'].mean() - sample_1[sample_1['X']==0]['Y'].mean()\n",
    "print(\"tv\",np.round(tv_sample_1,2))\n",
    "x_de = 0\n",
    "x_ie = 0\n",
    "x_se = 0\n",
    "for z in [0,1]:\n",
    "    p_z_x0 = sample_1[sample_1['X']==0][sample_1['Z']==z].shape[0]/sample_1[sample_1['X']==0].shape[0]\n",
    "    p_z_x1 = sample_1[sample_1['X']==1][sample_1['Z']==z].shape[0]/sample_1[sample_1['X']==1].shape[0]\n",
    "    for w in [0,1]:\n",
    "        p_w_x0_z  = sample_1[sample_1['X']==0][sample_1['Z']==z][sample_1['W']==w].shape[0]/sample_1[sample_1['X']==0][sample_1['Z']==z].shape[0]\n",
    "        p_w_x1_z  = sample_1[sample_1['X']==1][sample_1['Z']==z][sample_1['W']==w].shape[0]/sample_1[sample_1['X']==1][sample_1['Z']==z].shape[0]\n",
    "        p_y_x0_zw = sample_1[sample_1['X']==0][sample_1['Z']==z][sample_1['W']==w]['Y'].mean()\n",
    "        p_y_x1_zw = sample_1[sample_1['X']==1][sample_1['Z']==z][sample_1['W']==w]['Y'].mean()\n",
    "\n",
    "        x_de += (p_y_x1_zw - p_y_x0_zw)*p_z_x0*p_w_x0_z \n",
    "        x_ie += p_y_x1_zw*(p_w_x0_z-p_w_x1_z)*p_z_x0\n",
    "        x_se += p_y_x1_zw*p_w_x1_z*(p_z_x0-p_z_x1)\n",
    "\n",
    "print(\"x_de\",np.round(x_de,2))\n",
    "print(\"x_ie\",np.round(x_ie,2))\n",
    "print(\"x_se\",np.round(x_se,2))\n",
    "\n",
    "sum = x_de - x_ie - x_se\n",
    "sum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in testing whether the probability of an applicant being hired based on gender, age and education level is the same between pairs:\n",
    "- Sample 1 vs Sample 2\n",
    "- Sample 1 vs Sample 3  \n",
    "\n",
    "As we have access to the true SCM, we know that the first pair represents location A and is drawn from the same distribution. The latter pair is drawn from different distributions. However, we will now try to answer the question solely based on observational data. Specifically we test the hypotheses:\n",
    "\\begin{align}\n",
    "H_{0}: P_{s_1}(Y|X,Z,W) &= P_{s_2}(Y|X,Z,W)\\quad \\text{vs} \\quad H_{1}: P_{s_1}(Y|X,Z,W) \\neq P_{s_2}(Y|X,Z,W)\\\\\n",
    "H_{0}: P_{s_1}(Y|X,Z,W) &= P_{s_3}(Y|X,Z,W)\\quad \\text{vs} \\quad H_{1}: P_{s_1}(Y|X,Z,W) \\neq P_{s_3}(Y|X,Z,W)\\\\\n",
    "\\end{align}\n",
    "Generally, there is not a one-size-fits all solution to the problem and we need to choose task-specific procedures. In this case all variables are discrete and binary. Thus, one simple method for assessing the hypotheses is to perform a bootstrapped based test.\n",
    "\n",
    "## 1.1 Bootstrap test\n",
    "\n",
    "Note that in this case, both (1) and (2) can be rewritten to testing:\n",
    "\\begin{align*}\n",
    "H_{0}: P_{s_1}(Y=1|X,Z,W) - P_{s_2}(Y=1|X,Z,W) &= 0 \\quad \\text{vs} \\quad H_{1}: P_{s_1}(Y=1|X,Z,W) - P_{s_2}(Y=1|X,Z,W) \\neq 0\\\\\n",
    "H_{0}: P_{s_1}(Y=1|X,Z,W) - P_{s_3}(Y=1|X,Z,W) &= 0 \\quad \\text{vs} \\quad H_{1}: P_{s_1}(Y=1|X,Z,W) - P_{s_3}(Y=1|X,Z,W) \\neq 0\\\\\n",
    "\\end{align*}\n",
    "Also, there are $2^3=8$ combinations of $X,Z$ and $W$. For each of these we can test the hypothesis whether the observed difference is significantly different from 0. However, let's first compute the corresponding conditionals for all samples.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional probabilities of Y=1 given X,Z,W for sample 1, sample 2 and their difference:\n",
      "\n",
      "   X  Z  W    Y_s1    Y_s2  Difference\n",
      "0  0  0  0  0.0118  0.0199     -0.0081\n",
      "1  0  0  1  0.2195  0.3333     -0.1138\n",
      "2  0  1  0  0.2795  0.2787      0.0008\n",
      "3  0  1  1  0.4227  0.4894     -0.0667\n",
      "4  1  0  0  0.2989  0.1920      0.1069\n",
      "5  1  0  1  0.5000  0.5714     -0.0714\n",
      "6  1  1  0  0.0256  0.0183      0.0073\n",
      "7  1  1  1  0.1667  0.1739     -0.0072\n",
      "\n",
      "The average difference between sample 1 and sample 2 is -0.019\n",
      "\n",
      "Conditional probabilities of Y=1 given X,Z,W for sample 1, sample 3 and their difference:\n",
      "\n",
      "   X  Z  W    Y_s1    Y_s3  Difference\n",
      "0  0  0  0  0.0118  0.1315     -0.1197\n",
      "1  0  0  1  0.2195  0.4286     -0.2091\n",
      "2  0  1  0  0.2795  0.3525     -0.0730\n",
      "3  0  1  1  0.4227  0.5426     -0.1199\n",
      "4  1  0  0  0.2989  0.2560      0.0429\n",
      "5  1  0  1  0.5000  0.4615      0.0385\n",
      "6  1  1  0  0.0256  0.1209     -0.0952\n",
      "7  1  1  1  0.1667  0.2174     -0.0507\n",
      "\n",
      "The average difference between sample 1 and sample 3 is -0.0733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#compute conditionals of Y=1 on X,Z,W for all samples\n",
    "y_conditional_on_xzw_sample_1 = sample_1.groupby(['X','Z','W'])['Y'].mean().reset_index()\n",
    "y_conditional_on_xzw_sample_2 = sample_2.groupby(['X','Z','W'])['Y'].mean().reset_index()\n",
    "y_conditional_on_xzw_sample_3 = sample_3.groupby(['X','Z','W'])['Y'].mean().reset_index()\n",
    "\n",
    "#compare difference in conditionals\n",
    "#between sample 1 vs sample 2\n",
    "y_conditional_difference_sample_1_sample_2 = y_conditional_on_xzw_sample_1.merge(y_conditional_on_xzw_sample_2, on=['X','Z','W'], suffixes=('_s1','_s2'))\n",
    "y_conditional_difference_sample_1_sample_2['Difference'] = y_conditional_difference_sample_1_sample_2['Y_s1'] - y_conditional_difference_sample_1_sample_2['Y_s2']\n",
    "#between sample 1 vs sample 3\n",
    "y_conditional_difference_sample_1_sample_3 = y_conditional_on_xzw_sample_1.merge(y_conditional_on_xzw_sample_3, on=['X','Z','W'], suffixes=('_s1','_s3'))\n",
    "y_conditional_difference_sample_1_sample_3['Difference'] = y_conditional_difference_sample_1_sample_3['Y_s1'] - y_conditional_difference_sample_1_sample_3['Y_s3']\n",
    "\n",
    "#Store the conditional probabilities as index\n",
    "index = ['P(Y=1|X=0,Z=0,W=0)', 'P(Y=1|X=0,Z=0,W=1)', 'P(Y=1|X=0,Z=1,W=0)', 'P(Y=1|X=0,Z=1,W=1)', 'P(Y=1|X=1,Z=0,W=0)', 'P(Y=1|X=1,Z=0,W=1)', 'P(Y=1|X=1,Z=1,W=0)', 'P(Y=1|X=1,Z=1,W=1)']\n",
    "\n",
    "print(f\"Conditional probabilities of Y=1 given X,Z,W for sample 1, sample 2 and their difference:\\n\\n{np.round(y_conditional_difference_sample_1_sample_2,4)}\\n\")\n",
    "print(f\"The average difference between sample 1 and sample 2 is {np.round(y_conditional_difference_sample_1_sample_2['Difference'].mean(),4)}\\n\")\n",
    "print(f\"Conditional probabilities of Y=1 given X,Z,W for sample 1, sample 3 and their difference:\\n\\n{np.round(y_conditional_difference_sample_1_sample_3,4)}\\n\")\n",
    "print(f\"The average difference between sample 1 and sample 3 is {np.round(y_conditional_difference_sample_1_sample_3['Difference'].mean(),4)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a aggregated level we can see that the absolute difference is higher between sample 1 and sample 3 than between sample 1 and sample 2. This is in line with our knowledge of the underlying data-generating mechanisms. However, there are a few things to note.    \n",
    "\n",
    "Although sample 1 and sample 2 are generated by the same SCM, the individual and average differences are $\\neq0$. This is due to sampling variation as the SCM is not a fully deterministic process, but involves sampling from some probability distributions.    \n",
    "\n",
    "This is also the reason as to why some absolute differences between sample 1 and sample 2 are higher than for sample 1 and sample 3.  \n",
    "\n",
    "We now computed $P(Y=1| X,Z,W)$ for all samples and combinations of $X,Z$ and $W$. We now can test the initial hypothesis (1) and (2) by testing whether the observed differences for each individual conditional is significantly different from $0$. If there exists at least one individual conditional for which we can reject the null, we conclude that there must be a distribution shift between samples for $P(Y=1|X,Z,W)$.  \n",
    "\n",
    "For instance, we test the following for sample 1 and sample 2.\n",
    "\\begin{align*} \n",
    "H_{0,1}: P_{s_1}(Y=1|X=0,Z=0,W=0) - P_{s_2}(Y=1|X=0,Z=0,W=0) = 0 \\quad &\\text{vs} \\quad H_{1,1}: P_{s_1}(Y=1|X=0,Z=0,W=0) - P_{s_2}(Y=1|X=0,Z=0,W=0) \\neq 0\\\\\n",
    "H_{0,2}: P_{s_1}(Y=1|X=0,Z=0,W=1) - P_{s_2}(Y=1|X=0,Z=0,W=1) = 0 \\quad &\\text{vs} \\quad H_{1,2}: P_{s_1}(Y=1|X=0,Z=0,W=1) - P_{s_2}(Y=1|X=0,Z=0,W=1) \\neq 0\\\\\n",
    "...\\\\\n",
    "...\\\\\n",
    "H_{0,8}: P_{s_1}(Y=1|X=1,Z=1,W=1) - P_{s_2}(Y=1|X=1,Z=1,W=1) = 0 \\quad &\\text{vs} \\quad H_{1,3}: P_{s_1}(Y=1|X=1,Z=1,W=1) - P_{s_2}(Y=1|X=1,Z=1,W=1) \\neq 0\n",
    "\\end{align*}\n",
    "The same is applied for sample 1 vs sample 3. We will reject the null (samples coming from same distribution) if at least one of the individual conditionals is rejected.\n",
    "\n",
    "We will leverage bootstrapping [[Tibshirani and Efron (1994)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=duBlF_YAAAAJ&citation_for_view=duBlF_YAAAAJ:u5HHmVD_uO8C)] to asses this problem. Specifically, we will proceed as follows:    \n",
    "\\begin{align*}\n",
    "\\text{} \\quad & \\textbf{Bootstrap test} \\ \\\\\n",
    "\\text{} \\quad & \\textbf{Input:} \\ \\text{samples a and b and observational distributions} P_a(V) \\text{ and } P_b(V)\\\\\n",
    "\\text{} \\quad & \\textbf{Output:} \\ \\text{array of p-values, array of bootstrapped differences}  \\\\\n",
    "\\text{} \\quad & \\\\\n",
    "\\text{1.} \\quad & \\text{for  all combinations of } X,Z,W \\text{ do} \\\\\n",
    "\\text{2.} \\quad & \\quad \\text{compute and store } d=P_a(Y=1|x,z,w) -P_b(Y=1|x,z,w)  \\\\\n",
    "\\text{3.} \\quad & \\text{for } 1:n_{bootstrap} \\text{ do} \\\\\n",
    "\\text{4.} \\quad & \\quad  \\text{Create pooled sample } \\text{ from } a \\text{ and } b\\\\\n",
    "\\text{5.} \\quad & \\quad \\text{With replacement draw bootstrap samples } a^{boot},b^{boot} \\text{ from pooled sample}\\\\\n",
    "\\text{6.} \\quad & \\quad \\text{for  all combinations of } X,Z,W \\text{ do} \\\\\n",
    "\\text{7.} \\quad & \\quad \\quad \\text{compute and store } d^{boot}=P_{a^{boot}}(Y=1|x,z,w) -P_{b^{boot}}(Y=1|x,z,w)  \\\\\n",
    "\\text{8.} \\quad & \\text{for  all combinations of } X,Z,W \\text{ do} \\\\\n",
    "\\text{9.} \\quad & \\quad \\text{compute and store } p=\\frac{\\#\\{|d^{boot}| \\geq |d|\\}}{n_{bootstrap}}  \\\\\n",
    "\\end{align*} \n",
    "We rely on the notion of exchangeability to simulate the data under the null by pooling the two samples. The intuition being that if $a$ and $b$ are drawn from the same distribution, shuffling the data should not affect the overall statistical properties.  \n",
    "\n",
    "Note that in our example, testing whether two conditional probabilities are the same, became a problem of performing 8 individual tests. Thus, we need to adjust for multiple testing which we will do by applying the Benjamini-Hochberg correction for the p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_test(sample_a,sample_b, B=10000):\n",
    "\n",
    "    '''\n",
    "    Function to perform bootstrap test for difference in conditional probabilities of Y=1 given X,Z,W for two samples\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    sample_a: pandas DataFrame containing sample A\n",
    "    sample_b: pandas DataFrame containing sample B\n",
    "    B: int, number of bootstrap iterations\n",
    "    \n",
    "    Returns:\n",
    "\n",
    "    bootstrapped_diff: numpy array containing bootstrapped differences in conditional probabilities of Y=1 given X,Z,W between sample_a and sample_b\n",
    "    p_values: numpy array for the individual hypothesis tests\n",
    "    '''\n",
    "\n",
    "    #compute difference in conditionals between sample_a and sample_b\n",
    "    y_conditional_on_xzw_sample_a = sample_a.groupby(['X','Z','W'])['Y'].mean().reset_index()\n",
    "    y_conditional_on_xzw_sample_b = sample_b.groupby(['X','Z','W'])['Y'].mean().reset_index()\n",
    "    y_conditional_difference_sample_a_sample_b = y_conditional_on_xzw_sample_a['Y'].values - y_conditional_on_xzw_sample_b['Y'].values\n",
    "    #empty array to store bootstrapped differences (8 x B)\n",
    "    bootstrapped_diff = np.empty((8,B))\n",
    "    #empty array to store p-values (8)\n",
    "    p_values = np.zeros(8)\n",
    "    #loop through B iterations\n",
    "    for i in tqdm.tqdm(range(B), desc='Bootstrapping'):       \n",
    "        #pool the samples\n",
    "        pooled = pd.concat([sample_a, sample_b])\n",
    "        #shuffle pooled data\n",
    "        bootstrap_sample = pooled.sample(n=len(pooled), replace=True)\n",
    "        #create bootstrap samples\n",
    "        bootstrap_sample_a = bootstrap_sample.iloc[:len(sample_a)]\n",
    "        bootstrap_sample_b = bootstrap_sample.iloc[len(sample_a):]\n",
    "        #compute difference in conditionals between bootstrap_sample_a and bootstrap_sample_b\n",
    "        y_conditional_on_xzw_bootstrap_sample_a = bootstrap_sample_a.groupby(['X','Z','W'])['Y'].mean().reset_index()\n",
    "        y_conditional_on_xzw_bootstrap_sample_b = bootstrap_sample_b.groupby(['X','Z','W'])['Y'].mean().reset_index()\n",
    "        y_conditional_difference_bootstrap_sample_a_sample_b = y_conditional_on_xzw_bootstrap_sample_a['Y'].values - y_conditional_on_xzw_bootstrap_sample_b['Y'].values\n",
    "        #store bootstrapped difference\n",
    "        bootstrapped_diff[:,i] = y_conditional_difference_bootstrap_sample_a_sample_b\n",
    "\n",
    "    #compute p-values: fraction of absolute bootstrapped differences greater than absolute observed difference\n",
    "    for i in range(8):\n",
    "        p_values[i] = np.sum(np.abs(bootstrapped_diff[i]) >= np.abs(y_conditional_difference_sample_a_sample_b[i]))/B\n",
    "    #Need to adjust for multiple testing\n",
    "    bh_p_values = multitest.multipletests(p_values, method='fdr_bh')[1]\n",
    "\n",
    "    #return differences and p-values\n",
    "    return bootstrapped_diff, p_values, bh_p_values\n",
    "\n",
    "#perform the bootstrap test for sample 1 vs sample 2 and sample 1 vs sample 3\n",
    "bootstrapped_diff_sample_1_sample_2, p_values_sample_1_sample_2,bh_p_values_sample_1_sample_2 = bootstrap_test(sample_1, sample_2)\n",
    "bootstrapped_diff_sample_1_sample_3, p_values_sample_1_sample_3, bh_p_values_sample_1_sample_3 = bootstrap_test(sample_1, sample_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first look at the comparison of sample 1 and sample 2. Given that they are generated by the same SCM, there should not be any significant differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of bootstrapped differences for each tested conditional probability\n",
    "\n",
    "fig, axes = plt.subplots(2,4, figsize=(20,10))\n",
    "#Sample 1 vs Sample 2\n",
    "for i in range(8):\n",
    "    ax = axes[i//4,i%4]\n",
    "    ax.hist(bootstrapped_diff_sample_1_sample_2[i], bins=50, color='blue', alpha=0.5, label='Bootstrapped')\n",
    "    ax.axvline(y_conditional_difference_sample_1_sample_2['Difference'][i], color='red', label='Observed')\n",
    "    ax.set_title(f\"No shift: differences in {index[i]}\")\n",
    "    #Add p-value\n",
    "    ax.legend(title=f'p-value: {np.round(bh_p_values_sample_1_sample_2[i],4)}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sample 1 vs sample 2, the p-values for the differences in conditional probabilities of $Y=1$ given $X,Z,W$ are all greater than $0.05$, indicating that the differences are not statistically significant. This is consistent with the fact that they are generated from the same SCM. As this holds for all individual $P(Y=1|X,Z,W)$ we would conclude that we cannot detect a statistically significant distribution shift between the two samples with respect to the conditional probability of Y.  \n",
    "\n",
    "We now examine the results for sample 1 vs sample 3. Given that they are generated from different SCMs we would expect to see at least one significant difference. Looking at the two different SCMs a bit more closely, we realisticalexpect all individual differences to be significant as the SCMs differ such that each individual conditional is affected by the shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of bootstrapped differences for each tested conditional probability\n",
    "fig, axes = plt.subplots(2,4 , figsize=(20,10))\n",
    "#Sample 1 vs Sample 3\n",
    "for i in range(8):\n",
    "    ax = axes[i//4,i%4]\n",
    "    ax.hist(bootstrapped_diff_sample_1_sample_3[i], bins=50, color='blue', alpha=0.5, label='Bootstrapped')\n",
    "    ax.axvline(y_conditional_difference_sample_1_sample_3['Difference'][i], color='red', label='Observed')\n",
    "    ax.set_title(f\"Shift: differences in {index[i]}\")\n",
    "    #Add p-value\n",
    "    ax.legend(title=f'p-value: {np.round(bh_p_values_sample_1_sample_3[i],4)}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As at least one difference seems to be significantly different, we would reject the overall null hypothesis that the conditional of Y is the same for both samples. However, with the ground-truth being that all conditionals are affected by the shift, the test was only able to reject $\\frac{3}{8}=37.5\\%$ of all false nulls. This hints at low power of the test to detect shifted individual conditionals. A possible reason is that p-value corrections could lead to more conservative decisions compared to the unadjusted case. But, the unadjusted p-values are given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The unadjusted p-values for sample 1 vs sample 3 are:\\n{np.round(p_values_sample_1_sample_3,2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which would make the same decisions. A possible reason for the low power could also be that sampling variation dominates the distribution shift between sample 1 and sample 3 such that their difference is not detectable using the bootstrap test. The magnitude of shift between SCMs can be quantified exactly in this case and is given by   \n",
    "\\begin{align*}\n",
    "d = P_{s_1}(Y=1|X,Z,W) - P_{s_3}(Y=1|X,Z,W) = 0.09\n",
    "\\end{align*}\n",
    "We now repeat the test of sample 1 vs sample 3, but we change the following parameter:\n",
    "- we change the SCMs such that we have a stronger shift of 0.5 and perform the test on the original sample size\n",
    "- we keep the original SCMs and perform the test with increasing sample sizes [10,000;25,000;50,000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw new sample 3 with d = 0.4\n",
    "_ , sample_3_high_d = hiring_data(1, d=0.5)\n",
    "#compute observed difference in conditionals between sample 1 and sample 3 with d = 0.4\n",
    "y_conditional_diff_sample_1_sample_3_high_d = sample_1.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values - sample_3_high_d.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values \n",
    "\n",
    "#Draw samples 1 and 2 with increasing sample sizes (n=1000, 10000, 25000, 50000)\n",
    "sample_1_10000, sample_3_10000 = hiring_data(1,n=10000)\n",
    "#compute observed difference in conditionals between sample 1 and sample 3 with n=10000\n",
    "y_conditional_diff_sample_1_sample_3_10000 = sample_1_10000.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values - sample_3_10000.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values\n",
    "sample_1_25000, sample_3_25000 = hiring_data(1,n=25000)\n",
    "#compute observed difference in conditionals between sample 1 and sample 3 with n=25000\n",
    "y_conditional_diff_sample_1_sample_3_25000 = sample_1_25000.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values - sample_3_25000.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values\n",
    "sample_1_50000, sample_3_50000 = hiring_data(1,n=50000)\n",
    "#compute observed difference in conditionals between sample 1 and sample 3 with n=50000\n",
    "y_conditional_diff_sample_1_sample_3_50000 = sample_1_50000.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values - sample_3_50000.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values\n",
    "\n",
    "#Perform bootstrap test for sample 1 vs sample 3 with d = 0.4\n",
    "bootstrapped_diff_sample_1_sample_3_high_d, p_values_sample_1_sample_3_high_d, bh_p_values_sample_1_sample_3_high_d = bootstrap_test(sample_1, sample_3_high_d)\n",
    "\n",
    "\n",
    "#Perform bootstrap test for sample 1 vs sample 2 with increasing sample sizes\n",
    "bootstrapped_diff_sample_1_sample_3_10000, p_values_sample_1_sample_3_10000, bh_p_values_sample_1_sample_3_10000 = bootstrap_test(sample_1_10000, sample_3_10000)\n",
    "bootstrapped_diff_sample_1_sample_3_25000, p_values_sample_1_sample_3_25000, bh_p_values_sample_1_sample_3_25000 = bootstrap_test(sample_1_25000, sample_3_25000)\n",
    "bootstrapped_diff_sample_1_sample_3_50000, p_values_sample_1_sample_3_50000, bh_p_values_sample_1_sample_3_50000 = bootstrap_test(sample_1_50000, sample_3_50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we visually inspect the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot histograms for each of the tests performed:\n",
    "## Sample 1 vs Sample 3 with d = 0.5\n",
    "#4 histograms per row\n",
    "fig, axs = plt.subplots(2,4, figsize=(20,10))\n",
    "\n",
    "for i in range(8):\n",
    "    ax = axs[i//4, i%4]\n",
    "    ax.hist(bootstrapped_diff_sample_1_sample_3_high_d[i], bins=50, alpha=0.3, label='Bootstrapped', color='blue')\n",
    "    ax.axvline(y_conditional_diff_sample_1_sample_3_high_d[i], color='red', label='Observed')\n",
    "    ax.set_title(f'd = 0.5: Differences in P(Y=1|X={y_conditional_difference_sample_1_sample_2[\"X\"].values[i]}, Z={y_conditional_difference_sample_1_sample_2[\"Z\"].values[i]}, W={y_conditional_difference_sample_1_sample_2[\"W\"].values[i]})')\n",
    "    ax.set_xlabel('Difference in conditional probability')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    # Create a legend without the p-value as a color-coded entry\n",
    "    handles = [plt.Line2D([0], [0], color='blue',alpha = 0.3, lw=2, label='Bootstrapped'),\n",
    "               plt.Line2D([0], [0], color='red', lw=2, label='Observed')]\n",
    "    \n",
    "    # Add the p-value as a plain text\n",
    "    ax.legend(handles=handles + [plt.Line2D([0], [0], color='none', label=f'p-value: {bh_p_values_sample_1_sample_3_high_d[i]:.3f}')])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "## Sample 1 vs Sample 2 with increasing sample sizes\n",
    "##sample size 10000\n",
    "#4 histograms per row\n",
    "fig, axs = plt.subplots(2,4, figsize=(20,10))\n",
    "\n",
    "for i in range(8):\n",
    "    ax = axs[i//4, i%4]\n",
    "    ax.hist(bootstrapped_diff_sample_1_sample_3_10000[i], bins=50, alpha=0.3, label='Bootstrapped', color='blue')\n",
    "    ax.axvline(y_conditional_diff_sample_1_sample_3_10000[i], color='red', label='Observed')\n",
    "    ax.set_title(f'n = 10,000: Differences in P(Y=1|X={y_conditional_difference_sample_1_sample_2[\"X\"].values[i]}, Z={y_conditional_difference_sample_1_sample_2[\"Z\"].values[i]}, W={y_conditional_difference_sample_1_sample_2[\"W\"].values[i]})')\n",
    "    ax.set_xlabel('Difference in conditional probability')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    # Create a legend without the p-value as a color-coded entry\n",
    "    handles = [plt.Line2D([0], [0], color='blue',alpha = 0.3, lw=2, label='Bootstrapped'),\n",
    "               plt.Line2D([0], [0], color='red', lw=2, label='Observed')]\n",
    "    \n",
    "    # Add the p-value as a plain text\n",
    "    ax.legend(handles=handles + [plt.Line2D([0], [0], color='none', label=f'p-value: {bh_p_values_sample_1_sample_3_10000[i]:.3f}')])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##sample size 25000\n",
    "#4 histograms per row\n",
    "fig, axs = plt.subplots(2,4, figsize=(20,10))\n",
    "\n",
    "for i in range(8):\n",
    "    ax = axs[i//4, i%4]\n",
    "    ax.hist(bootstrapped_diff_sample_1_sample_3_25000[i], bins=50, alpha=0.3, label='Bootstrapped', color='blue')\n",
    "    ax.axvline(y_conditional_diff_sample_1_sample_3_25000[i], color='red', label='Observed')\n",
    "    ax.set_title(f'n = 25,000: Differences in P(Y=1|X={y_conditional_difference_sample_1_sample_2[\"X\"].values[i]}, Z={y_conditional_difference_sample_1_sample_2[\"Z\"].values[i]}, W={y_conditional_difference_sample_1_sample_2[\"W\"].values[i]})')\n",
    "    ax.set_xlabel('Difference in conditional probability')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    # Create a legend without the p-value as a color-coded entry\n",
    "    handles = [plt.Line2D([0], [0], color='blue',alpha = 0.3, lw=2, label='Bootstrapped'),\n",
    "               plt.Line2D([0], [0], color='red', lw=2, label='Observed')]\n",
    "    \n",
    "    # Add the p-value as a plain text\n",
    "    ax.legend(handles=handles + [plt.Line2D([0], [0], color='none', label=f'p-value: {bh_p_values_sample_1_sample_3_25000[i]:.3f}')])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "##sample size 50000\n",
    "#4 histograms per row\n",
    "fig, axs = plt.subplots(2,4, figsize=(20,10))\n",
    "\n",
    "for i in range(8):\n",
    "    ax = axs[i//4, i%4]\n",
    "    ax.hist(bootstrapped_diff_sample_1_sample_3_50000[i], bins=50, alpha=0.3, label='Bootstrapped', color='blue')\n",
    "    ax.axvline(y_conditional_diff_sample_1_sample_3_50000[i], color='red', label='Observed')\n",
    "    ax.set_title(f'n = 50,000: Differences in P(Y=1|X={y_conditional_difference_sample_1_sample_2[\"X\"].values[i]}, Z={y_conditional_difference_sample_1_sample_2[\"Z\"].values[i]}, W={y_conditional_difference_sample_1_sample_2[\"W\"].values[i]})')\n",
    "    ax.set_xlabel('Difference in conditional probability')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    # Create a legend without the p-value as a color-coded entry\n",
    "    handles = [plt.Line2D([0], [0], color='blue',alpha = 0.3, lw=2, label='Bootstrapped'),\n",
    "               plt.Line2D([0], [0], color='red', lw=2, label='Observed')]\n",
    "    \n",
    "    # Add the p-value as a plain text\n",
    "    ax.legend(handles=handles + [plt.Line2D([0], [0], color='none', label=f'p-value: {bh_p_values_sample_1_sample_3_50000[i]:.3f}')])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a very strong shift in distribution (i.e. $P(Y=1|X,Z,W)$ having difference $d = 0.5$), the bootstrap is capable of detecting any of the 8 individual conditionals as shifted. Thus, it seems that for more pronounced shifts the bootstrap tests becomes more powerful in detecting more fine-grained shifts instead of only working on aggregate.\n",
    "\n",
    "As for the sample size, increasing it 10-fold from 1k to 10k observations already allows the bootstrap approach to detect all individual shifts in the conditionals. With larger size, the individual samples become a more accurate representation of the underlying distribution and the effect of sample variation decreases. \n",
    "\n",
    "These two observations are important considerations, when choosing an appropiate method to detect dataset shifts. While in our example, the bootstrap test did not commit any Type I error, it was not able to detect all individual significant shifts. As the unadjusted p-values showed, it was not due to more conservative decisions when correcting for multiple testing. Instead, lower sample sizes and less pronounced shifts seem to have a negative influence on its power. A more theoretical power analysis could shed more light into this, but is beyond the scope of this work. Also, from an aggregate perspective which is our rationale for rejecting the null, for $n=1,000$, the test was able to detect at least one shifted conditional. While not capable of discovering all fine-granularities of shifts, it did suceed in pointing us towards significant differences in $Y=1|X,Z,W$. Thus, in all examples, we would have concluded that there is a shift between sample 1 and sample 3.\n",
    "\n",
    "## 1.2 Chi-squared test\n",
    "\n",
    "Remembering the initial problem, we were interested whether the two samples coincide in the conditional of $Y$ on $X,Z,W$. We did this by restructuring the problem into testing whether the conditional expectation of Y differs between the samples as in this case:\n",
    "\\begin{align*}\n",
    "\\text{E}[Y|X,Z,W] = P(Y=1|X,Z,W)\n",
    "\\end{align*}\n",
    "\n",
    "We then bootstrapped the difference in expectation under the null. However, there are also methods that can go without resampling methods. For instance, the $\\Chi^2$-Test, which is recommended by [Rabanser, Günneman and Liption (2019)](https://proceedings.neurips.cc/paper/2019/hash/846c260d715e5b854ffad5f70a516c88-Abstract.html) for dataset shifts in case of categorical data.\n",
    "\n",
    "We can perform the test as follows:\n",
    "\\begin{align*}\n",
    "\\text{} \\quad & \\textbf{Chi-squared test} \\ \\\\\n",
    "\\text{} \\quad & \\textbf{Input:} \\ \\text{samples a and b and observational distributions} P_a(V) \\text{ and } P_b(V)\\\\\n",
    "\\text{} \\quad & \\textbf{Output:} \\ \\text{array of Chi-squared statistics, array of p-values}  \\\\\n",
    "\\text{} \\quad & \\\\\n",
    "\\text{1.} \\quad & \\text{for  all combinations of } X,Z,W \\text{ do} \\\\\n",
    "\\text{2.} \\quad & \\quad \\text{compute and store counts of } Y=0 \\text{ and } Y=1   \\\\\n",
    "\\text{3.} \\quad & \\text{Merge the contigency tables for both samples} \\\\\n",
    "\\text{4.} \\quad & \\text{for  all combinations of } X,Z,W \\text{ do}\\\\\n",
    "\\text{5.} \\quad & \\quad \\text{Perform Chi-Squared test and obtain statistic and p-value (i.e. using Scipy implementation)}\\\\\n",
    "\\end{align*} \n",
    "\n",
    "While we are overall interested in whether the conditional distribution of Y differs between samples, the $\\Chi^2$-Test frames the hypotheses as:\n",
    "\\begin{align*}\n",
    "H_0: \\text{Group (sample) membership is independent of Y for each (X,Z,W) \\quad vs\\quad} H_1: \\text{Group (sample) membership is not independent of Y for each (X,Z,W)}\n",
    "\\end{align*}\n",
    "The phrasing might sound confusing at first. But, the null essentially conveyes that if we compare the outcome of $Y$ between the two samples (groups), if their conditional on (X,Z,W) is the same, their distribution should not depend on whether the outcome comes from sample 1 or sample 2. If they are not the same, we would expect their distribution to differ depending on belonging to sample 1 or sample 2 which is formulated in the alternative.   \n",
    "\n",
    "Again, we have to break down the problem into performing 8 individual tests and adjust the p-values using Benjamini-Hochberg correction. For performing the actual $X^2$-test, we will use the Scipy implementation which only needs the contigency table as an input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi-squared test to compare the distribution of Y=1 given X,Z,W between samples\n",
    "\n",
    "def chi_squared_test(sample_a, sample_b):\n",
    "\n",
    "    '''\n",
    "    Function to perform chi-squared test for difference in distribution of Y=1 given X,Z,W for two samples\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    sample_a: pandas DataFrame containing sample A\n",
    "    sample_b: pandas DataFrame containing sample B\n",
    "    \n",
    "    Returns:\n",
    "\n",
    "    chi_squared_stats: numpy array containing chi-squared statistics for each tested conditional probability\n",
    "    p_values: numpy array for the individual hypothesis tests\n",
    "    adjusted_p_values: numpy array for the adjusted p-values using Benjamini-Hochberg\n",
    "    '''\n",
    "\n",
    "    #compute counts of Y=1 and Y=0 for each combination of X,Z,W for both samples\n",
    "    y_counts_sample_a = sample_a.groupby(['X','Z','W'])['Y'].value_counts().unstack().reset_index().fillna(0)\n",
    "    y_counts_sample_b = sample_b.groupby(['X','Z','W'])['Y'].value_counts().unstack().reset_index().fillna(0)\n",
    "    #merge the two contingency tables\n",
    "    merged_counts = pd.merge(y_counts_sample_a, y_counts_sample_b, on=['X', 'Z', 'W'], suffixes=('_s1', '_s2'))\n",
    "    #empty arrays to store chi-squared statistics and p-values\n",
    "    chi_squared_stats = np.empty(8)\n",
    "    p_values = np.zeros(8)\n",
    "    #loop through each combination of X,Z,W\n",
    "    for i, row in merged_counts.iterrows():\n",
    "        #construct contingency table\n",
    "        contingency_table = row[['0_s1', '1_s1', '0_s2', '1_s2']].values.reshape(2, 2)\n",
    "        #perform chi-squared test\n",
    "        chi2_stat, p_val, _, _ = chi2_contingency(contingency_table)\n",
    "        chi_squared_stats[i] = chi2_stat\n",
    "        p_values[i] = p_val\n",
    "\n",
    "    #Need to adjust for multiple testing\n",
    "    bh_p_values = multitest.multipletests(p_values, method='fdr_bh')[1]\n",
    "\n",
    "    #return chi-squared statistics and p-values\n",
    "    return chi_squared_stats, p_values, bh_p_values\n",
    "\n",
    "\n",
    "#perform chi-squared test for sample 1 vs sample 2 and sample 1 vs sample 3\n",
    "chi_squared_stats_sample_1_sample_2, p_values_chi_squared_sample_1_sample_2, bh_p_values_chi_squared_sample_1_sample_2 = chi_squared_test(sample_1, sample_2)\n",
    "chi_squared_stats_sample_1_sample_3, p_values_chi_squared_sample_1_sample_3, bh_p_values_chi_squared_sample_1_sample_3 = chi_squared_test(sample_1, sample_3)\n",
    "\n",
    "#Summarise results in a table\n",
    "sample_1_sample_2_chi_squared_results = pd.DataFrame({'Chi-squared statistics': chi_squared_stats_sample_1_sample_2, 'p-values': p_values_chi_squared_sample_1_sample_2, 'Adjusted p-values': bh_p_values_chi_squared_sample_1_sample_2})\n",
    "sample_1_sample_2_chi_squared_results.index = index\n",
    "\n",
    "sample_1_sample_3_chi_squared_results = pd.DataFrame({'Chi-squared statistics': chi_squared_stats_sample_1_sample_3, 'p-values': p_values_chi_squared_sample_1_sample_3, 'Adjusted p-values': bh_p_values_chi_squared_sample_1_sample_3})\n",
    "sample_1_sample_3_chi_squared_results.index = index\n",
    "print(f\"Chi-squared test results for sample 1 vs sample 2:\\n\\n{np.round(sample_1_sample_2_chi_squared_results,4)}\\n\")\n",
    "print(f\"Chi-squared test results for sample 1 vs sample 3:\\n\\n{np.round(sample_1_sample_3_chi_squared_results,4)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a similar picture to the bootstrap. That is, the test correctly finds that sample 1 and sample 2 seem to be drawn from the same distribution as there are no rejected nulls. Comparing sample 1 and sample 3, the test is only able to detect $\\frac{2}{8} = 12.5\\%$ significant differences. Most notably, it succeeds/fails at the same conditionals as the bootstrap before. The intuition behind that is that testing 8 individual hypotheses leads to smaller sample sizes used for each test. The original $N=1,000$ are seperated into 8 groups. The grouping reduces the sample size used for each individual test and allows for sampling variation to dominate in the sub-problem. A more thorough power analysis might shed some light on this issue. As before, this is beyond the scope of this work.\n",
    "\n",
    "We can repeat the test for increasing sample sizes. We will pay special attention to the individual sample sizes corresponding to each sub-problem. We begin by examining the individual sample sizes for the just performed test and then perform the test for increasing sample sizes again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Track count of observations for each individual combination of X,Z,W\n",
    "sample_1_counts = sample_1.groupby(['X','Z','W']).size().reset_index(name='Count')\n",
    "sample_2_counts = sample_2.groupby(['X','Z','W']).size().reset_index(name='Count')\n",
    "sample_3_counts = sample_3.groupby(['X','Z','W']).size().reset_index(name='Count')\n",
    "#Merge all counts\n",
    "merged_counts = sample_1_counts.merge(sample_2_counts, on=['X','Z','W'], suffixes=('_s1','_s2')).merge(sample_3_counts, on=['X','Z','W'])\n",
    "merged_counts.columns = ['X','Z','W','Count_s1','Count_s2','Count_s3']\n",
    "#Print\n",
    "print(f\"Counts of observations for each individual combination of X,Z,W for all samples:\\n\\n{merged_counts}\\n\")\n",
    "\n",
    "#Perform test again for increasing sample sizes (only for sample 1 vs sample 3)\n",
    "sample_1_n_10000, sample_3_n_10000 = hiring_data(1,n=10000)\n",
    "sample_1_n_10000_counts, sample_3_n_10000_counts = sample_1_n_10000.groupby(['X','Z','W']).size().reset_index(name='Count'), sample_3_n_10000.groupby(['X','Z','W']).size().reset_index(name='Count')\n",
    "merged_counts_n_10000 = sample_1_n_10000_counts.merge(sample_3_n_10000_counts, on=['X','Z','W'])\n",
    "merged_counts_n_10000.columns = ['X','Z','W','Count_s1','Count_s3']\n",
    "chi_squared_stats_sample_1_sample_3_n_10000, p_values_chi_squared_sample_1_sample_3_n_10000, bh_p_values_chi_squared_sample_1_sample_3_n_10000 = chi_squared_test(sample_1_n_10000, sample_3_n_10000)\n",
    "merged_counts_n_10000['Adj. p-values'] = np.round(bh_p_values_chi_squared_sample_1_sample_3_n_10000,4)\n",
    "print(f\"Results for sample 1 vs sample 3 with N=10,000:\\n\\n{merged_counts_n_10000}\\n\")\n",
    "\n",
    "sample_1_n_25000, sample_3_n_25000 = hiring_data(1,n=25000)\n",
    "sample_1_n_25000_counts, sample_3_n_25000_counts = sample_1_n_25000.groupby(['X','Z','W']).size().reset_index(name='Count'), sample_3_n_25000.groupby(['X','Z','W']).size().reset_index(name='Count')\n",
    "merged_counts_n_25000 = sample_1_n_25000_counts.merge(sample_3_n_25000_counts, on=['X','Z','W'])\n",
    "merged_counts_n_25000.columns = ['X','Z','W','Count_s1','Count_s3']\n",
    "chi_squared_stats_sample_1_sample_3_n_25000, p_values_chi_squared_sample_1_sample_3_n_25000, bh_p_values_chi_squared_sample_1_sample_3_n_25000 = chi_squared_test(sample_1_n_25000, sample_3_n_25000)\n",
    "merged_counts_n_25000['Adj. p-values'] = np.round(bh_p_values_chi_squared_sample_1_sample_3_n_25000,4)\n",
    "print(f\"Results for sample 1 vs sample 3 with N=25,000:\\n\\n{merged_counts_n_25000}\\n\")\n",
    "\n",
    "sample_1_n_50000, sample_3_n_50000 = hiring_data(1,n=50000)\n",
    "sample_1_n_50000_counts, sample_3_n_50000_counts = sample_1_n_50000.groupby(['X','Z','W']).size().reset_index(name='Count'), sample_3_n_50000.groupby(['X','Z','W']).size().reset_index(name='Count')\n",
    "merged_counts_n_50000 = sample_1_n_50000_counts.merge(sample_3_n_50000_counts, on=['X','Z','W'])\n",
    "merged_counts_n_50000.columns = ['X','Z','W','Count_s1','Count_s3']\n",
    "chi_squared_stats_sample_1_sample_3_n_50000, p_values_chi_squared_sample_1_sample_3_n_50000, bh_p_values_chi_squared_sample_1_sample_3_n_50000 = chi_squared_test(sample_1_n_50000, sample_3_n_50000)\n",
    "merged_counts_n_50000['Adj. p-values'] = np.round(bh_p_values_chi_squared_sample_1_sample_3_n_50000,4)\n",
    "print(f\"Results for sample 1 vs sample 3 with N=50,000:\\n\\n{merged_counts_n_50000}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that for the original example (i.e. $N=1,000$), the two individual conditionals which our previous tests correctly identified as different where also the ones where the corresponding subproblem had the largest and 2nd largest samples size. As we increase the sample size, we can again see that the tests is able to detect each individual difference.\n",
    "\n",
    "We can also run the test again with the same large shift of $d=0.50$ between sample 1 and sample 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform test again for sample 1 vs sample 3 with d = 0.5\n",
    "chi_squared_stats_sample_1_sample_3_high_d, p_values_chi_squared_sample_1_sample_3_high_d, bh_p_values_chi_squared_sample_1_sample_3_high_d = chi_squared_test(sample_1, sample_3_high_d)\n",
    "print(f\"Chi-squared test results for sample 1 vs sample 3 with d = 0.5:\\n\\n{np.round(p_values_chi_squared_sample_1_sample_3_high_d,4)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the bootstrap, the $X^2$ test is able to detect all individual shifts if there is a more pronounced difference in the conditional of $Y$ between samples.\n",
    "\n",
    "A preliminary summary so far could be that:\n",
    "- Bootstrap and $X^2$-test did not reject true nulls (no shifts).\n",
    "- Bootstrap and $X^2$-test are able to detect shifts between samples from a aggregate perspective (detect at least 1 significant shift in the conditionals)\n",
    "- For individual conditionals, we see less rejections as expected. For moderate sample sizes and smaller shifts this might be an issue.\n",
    "- Likely it is in part due to breaking the problem into 8 individual ones with possibly much smaller indvidual sample sizes.\n",
    "- For large enough samples or very strong shifts, this does not seem to be a problem anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw initial samples\n",
    "sample_1, sample_2 = hiring_data(0)\n",
    "#Draw new sample 3\n",
    "_ , sample_3 = hiring_data(1,d=0.05)\n",
    "#Pool samples\n",
    "pool_s1s2 = pd.concat([sample_1, sample_2])\n",
    "pool_s1s3 = pd.concat([sample_1, sample_3])\n",
    "#add indicator for sample\n",
    "pool_s1s2['Sample'] = ['Sample 1']*len(sample_1) + ['Sample 2']*len(sample_2)\n",
    "pool_s1s3['Sample'] = ['Sample 1']*len(sample_1) + ['Sample 3']*len(sample_3)\n",
    "\n",
    "#Perform Chi Squared Test of independence for Y independent of Sample conditional on X,Z,W\n",
    "\n",
    "#Create contingency table stored in dictionary\n",
    "contingency_table = {}\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            name = f'X={i},Z={j},W={k}'\n",
    "            contingency_table[name] = pool_s1s2[(pool_s1s2['X'] == i) & (pool_s1s2['Z'] == j) & (pool_s1s2['W'] == k)].groupby(['Sample','Y']).size().unstack().fillna(0).values\n",
    "\n",
    "#Perform chi-squared test for each combination of X,Z,W\n",
    "chi_squared_stats = {}\n",
    "p_values = {}\n",
    "for key in contingency_table.keys():\n",
    "    chi2_stat, p_val, _, _ = chi2_contingency(contingency_table[key])\n",
    "    chi_squared_stats[key] = chi2_stat\n",
    "    p_values[key] = p_val\n",
    "\n",
    "#adjust p-values\n",
    "bh_p_values = multitest.multipletests(list(p_values.values()), method='fdr_bh')[1]\n",
    "\n",
    "#Print results\n",
    "print(f\"Chi-squared test results for Y independent of Sample conditional on X,Z,W:\\n\")\n",
    "for i, key in enumerate(p_values.keys()):\n",
    "    print(f\"{key}: p-value = {p_values[key]:.4f}, Adjusted p-value = {bh_p_values[i]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Repeat for sample 1 vs sample 3\n",
    "contingency_table = {}\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            name = f'X={i},Z={j},W={k}'\n",
    "            contingency_table[name] = pool_s1s3[(pool_s1s3['X'] == i) & (pool_s1s3['Z'] == j) & (pool_s1s3['W'] == k)].groupby(['Sample','Y']).size().unstack().fillna(0).values\n",
    "\n",
    "#Perform chi-squared test for each combination of X,Z,W\n",
    "chi_squared_stats = {}\n",
    "p_values = {}\n",
    "for key in contingency_table.keys():\n",
    "    chi2_stat, p_val, _, _ = chi2_contingency(contingency_table[key])\n",
    "    chi_squared_stats[key] = chi2_stat\n",
    "    p_values[key] = p_val\n",
    "\n",
    "#adjust p-values\n",
    "bh_p_values = multitest.multipletests(list(p_values.values()), method='fdr_bh')[1]\n",
    "\n",
    "#Print results\n",
    "print(f\"Chi-squared test results for Y independent of Sample conditional on X,Z,W:\\n\")\n",
    "for i, key in enumerate(p_values.keys()):\n",
    "    print(f\"{key}: p-value = {p_values[key]:.4f}, Adjusted p-value = {bh_p_values[i]:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue by trying out 2 additional tests, before shifting our attention towards a more \"non-traditional\" approach. The 2 additional tests are:\n",
    "- $Z$-test for proportions\n",
    "- testing weighted outcomes as described in [Schrouff et al (2022)](https://proceedings.neurips.cc/paper_files/paper/2022/hash/7a969c30dc7e74d4e891c8ffb217cf79-Abstract-Conference.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Z-test for proportions\n",
    "\n",
    "We begin with the $Z$-test. First, we notice that even without knowing the underlying data-generating mechanism one could assume that each observation in a sample corresponds to the outcome of a Bernoulli-Trial. The sample then corresponds to a sequence of $N$ individual Bernoulli-trials, which can be modelled with a binomial distribution $Binomial(n,p)$, where n corresponds to the number of samples and p the probability of success.  \n",
    "\n",
    "For this example, we could assume the following for each individual observation and the overall sample:\n",
    "\\begin{align*}\n",
    "Y_i \\leftarrow Ber(p(X,W,Z))\\\\\n",
    "Y \\leftarrow Bin(n, p(X,W,Z))\n",
    "\\end{align*}\n",
    "\n",
    "where, we assume that for both the individual observation and the overall sample the probability/number of successes for $Y$ depends on $X,W,Z$. For the probability of success, we can simply compute the empirical probability of success for all specific combinations of $(X,W,Z)$, while taking the number of samples available for that combination as our $n$. Thus, we model the conditional of $Y$ as:\n",
    "\\begin{align*}\n",
    "Y=1 | X=x, W=w, Z=z \\leftarrow \\text{Bin}(n_{xwz}, \\hat{p}(x,w,z))\n",
    "\\end{align*}\n",
    "\n",
    "Further, given we have enough samples in the individual conditionals, it could allow for a Normal approxmiation. Let's begin by first computing the individual $\\hat{p}(x,w,z)$ and $n_{xwz}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bernoulli Trial\n",
    "##compute p of success conditionally on X,Z,W for all samples\n",
    "p_conditional_on_xzw_sample_1 = sample_1.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values\n",
    "p_conditional_on_xzw_sample_2 = sample_2.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values\n",
    "p_conditional_on_xzw_sample_3 = sample_3.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values\n",
    "\n",
    "#Binomial Distribution\n",
    "##compute number of samples for each combination of X,Z,W for all samples\n",
    "n_sample_1 = sample_1.groupby(['X','Z','W']).size().reset_index(name='Count')['Count'].values\n",
    "n_sample_2 = sample_2.groupby(['X','Z','W']).size().reset_index(name='Count')['Count'].values\n",
    "n_sample_3 = sample_3.groupby(['X','Z','W']).size().reset_index(name='Count')['Count'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using that we can perform a Z-test for\n",
    "\\begin{align*}\n",
    "H_{0}: P_{s_1}(Y|X,Z,W) &= P_{s_2}(Y|X,Z,W)\\quad \\text{vs} \\quad H_{1}: P_{s_1}(Y|X,Z,W) \\neq P_{s_2}(Y|X,Z,W)\\\\\n",
    "H_{0}: P_{s_1}(Y|X,Z,W) &= P_{s_3}(Y|X,Z,W)\\quad \\text{vs} \\quad H_{1}: P_{s_1}(Y|X,Z,W) \\neq P_{s_3}(Y|X,Z,W)\\\\\n",
    "\\end{align*}\n",
    "by asessing the following hypotheses:\n",
    "\\begin{align*}\n",
    "H_{0}: \\hat{p}_{s_1}(x,w,z) &- \\hat{p}_{s_2}(x,w,z) = 0\\quad \\text{vs} \\quad H_{1}: \\hat{p}_{s_1}(x,w,z) - \\hat{p}_{s_2}(x,w,z) \\neq 0\\\\\n",
    "H_{0}: \\hat{p}_{s_1}(x,w,z) &- \\hat{p}_{s_3}(x,w,z) = 0\\quad \\text{vs} \\quad H_{1}: \\hat{p}_{s_1}(x,w,z) - \\hat{p}_{s_3}(x,w,z) \\neq 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "In case of no shift, the difference between the sample proportions should not be significantly different from each other while the opposite should hold in presence of distribution shifts. We will perform two slightly different versions of the test. The first is the \"standard\" approach and is given by:\n",
    "\\begin{align*}\n",
    "\\text{} \\quad & \\textbf{Z-test for proportions} \\ \\\\\n",
    "\\text{} \\quad & \\textbf{Input:} \\ \\text{samples a and b and observational distributions} P_a(V) \\text{ and } P_b(V)\\\\\n",
    "\\text{} \\quad & \\textbf{Output:} \\ \\text{array of p-values}  \\\\\n",
    "\\text{} \\quad & \\\\\n",
    "\\text{1.} \\quad & \\text{for both samples: }\\text{ do} \\\\\n",
    "\\text{2.} \\quad & \\quad \\text{Compute and store: } \\hat{p}(Y=1|x,z,w) \\text{ and }n_{xwz} \\\\\n",
    "\\text{3.} \\quad & \\text{Compute } d=\\hat{p}_a(Y=1|x,z,w) - \\hat{p}_b(Y=1|x,z,w) \\\\\n",
    "\\text{4.} \\quad & \\text{Compute } p^{pool} = \\frac{n_{a_{xwz}}*\\hat{p}_a(Y=1|x,z,w) + n_{b_{xwz}}*\\hat{p}_b(Y=1|x,z,w)}{n_{a_{xwz}} + n_{b_{xwz}}} \\\\\n",
    "\\text{5.} \\quad & \\text{Compute the test statistic } z = \\frac{d}{\\sqrt{p^{pool}(1-p^{pool})(\\frac{1}{n_{a_{xwz}}}+\\frac{1}{n_{b_{xwz}}})}} \\\\\n",
    "\\text{6.} \\quad & \\text{Obtain p-values by } p = 2(1 − Φ(|z|), \\text{where Φ(|z|) is the CDF of the standard Normal distribution.}\\\\\n",
    "\\end{align*} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform the z-test for proportions for sample 1 vs sample 2 and sample 1 vs sample 3\n",
    "\n",
    "def z_test(sample_a, sample_b):\n",
    "    \n",
    "    '''\n",
    "    Function to perform z-test for difference in conditional probabilities of Y=1 given X,Z,W for two samples\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    sample_a: pandas DataFrame containing sample A\n",
    "    sample_b: pandas DataFrame containing sample B\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    z: numpy array containing z-statistics for each tested conditional probability\n",
    "    p_values: numpy array for the individual hypothesis tests\n",
    "    adjusted_p_values: numpy array for the adjusted p-values using Benjamini-Hochberg\n",
    "    '''\n",
    "\n",
    "    #compute empirircal probabilities of success and number of samples for each combination of X,Z,W for both samples\n",
    "    p_sample_a = sample_a.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values\n",
    "    n_sample_a = sample_a.groupby(['X','Z','W']).size().reset_index(name='Count')['Count'].values\n",
    "    p_sample_b = sample_b.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values\n",
    "    n_sample_b = sample_b.groupby(['X','Z','W']).size().reset_index(name='Count')['Count'].values\n",
    "    #compute pooled p\n",
    "    p_pooled = (p_sample_a*n_sample_a + p_sample_b*n_sample_b)/(n_sample_a + n_sample_b)\n",
    "    #compute test statistic z\n",
    "    z = (p_sample_a - p_sample_b)/np.sqrt(p_pooled*(1-p_pooled)*(1/n_sample_a + 1/n_sample_b))\n",
    "    #compute p-values \n",
    "    p_values = 2*(1-norm.cdf(np.abs(z)))\n",
    "    #Need to adjust for multiple testing\n",
    "    bh_p_values = multitest.multipletests(p_values, method='fdr_bh')[1]\n",
    "\n",
    "    #return z-statistics and p-values\n",
    "    return z, p_values, bh_p_values\n",
    "\n",
    "\n",
    "#perform z-test for sample 1 vs sample 2 and sample 1 vs sample 3\n",
    "z_stats_sample_1_sample_2, p_values_z_sample_1_sample_2, bh_p_values_z_sample_1_sample_2 = z_test(sample_1, sample_2)\n",
    "z_stats_sample_1_sample_3, p_values_z_sample_1_sample_3, bh_p_values_z_sample_1_sample_3 = z_test(sample_1, sample_3)\n",
    "\n",
    "#Summarise results in a table\n",
    "sample_1_sample_2_z_results = pd.DataFrame({'Z statistics': z_stats_sample_1_sample_2, 'p-values': p_values_z_sample_1_sample_2, 'Adjusted p-values': bh_p_values_z_sample_1_sample_2})\n",
    "sample_1_sample_2_z_results.index = index\n",
    "sample_1_sample_3_z_results = pd.DataFrame({'Z statistics': z_stats_sample_1_sample_3, 'p-values': p_values_z_sample_1_sample_3, 'Adjusted p-values': bh_p_values_z_sample_1_sample_3})\n",
    "sample_1_sample_3_z_results.index = index\n",
    "\n",
    "#Print results\n",
    "print(f\"Z-test results for sample 1 vs sample 2:\\n\\n{np.round(sample_1_sample_2_z_results,4)}\\n\")\n",
    "print(f\"Z-test results for sample 1 vs sample 3:\\n\\n{np.round(sample_1_sample_3_z_results,4)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the performance is in line with all previously tests. If there is no shift, the test does not reject the nulls. For a shift with magnitude $d=0.09$ it is only able to detect $\\frac{3}{8}=37.5%$ of all individual shifts. As in real-life, we would only observe $P(X,W,Z,Y)$ without knowing the underlying SCM, we would correctly indicate that there is a shift from an aggregate perspective. For more pronounced shifts ($d=0.50$) and larger sample sizes, we would expect the test's power with respect to the individual conditionals to improve as it did with the previous methods. Thus, we will in this case not run the test again with larger shifts and samples (included in future appendix). Also, note that the test rejected the same nulls in presence of a shift as the previous tests. These, were again the conditionals with the largest individual sample sizes.\n",
    "\n",
    "The z-test is parametric in the sense that assumes that the sampling distribution for the difference in proportions is approximately normal. Instead of only relying on our two available samples, we could also try to bootstrap the whole procedure by:\n",
    "\\begin{align*}\n",
    "\\text{} \\quad & \\textbf{Bootstrapped Z-test for proportions} \\ \\\\\n",
    "\\text{} \\quad & \\textbf{Input:} \\ \\text{samples a and b and observational distributions} P_a(V) \\text{ and } P_b(V)\\\\\n",
    "\\text{} \\quad & \\textbf{Output:} \\ \\text{array of (bootstrapped) Z statistics, array of p-values}  \\\\\n",
    "\\text{} \\quad & \\\\\n",
    "\\text{1.} \\quad & \\text{for both samples }\\text{ do:} \\\\\n",
    "\\text{2.} \\quad & \\quad \\text{Compute and store: } \\hat{p}(Y=1|x,z,w) \\text{ and }n_{xwz} \\\\\n",
    "\\text{3.} \\quad & \\text{Compute } d=\\hat{p}_a(Y=1|x,z,w) - \\hat{p}_b(Y=1|x,z,w) \\\\\n",
    "\\text{4.} \\quad & \\text{Compute } p^{pool} = \\frac{n_{a_{xwz}}*\\hat{p}_a(Y=1|x,z,w) + n_{b_{xwz}}*\\hat{p}_b(Y=1|x,z,w)}{n_{a_{xwz}} + n_{b_{xwz}}} \\\\\n",
    "\\text{5.} \\quad & \\text{Compute the test statistic } z = \\frac{d}{\\sqrt{p^{pool}(1-p^{pool})(\\frac{1}{n_{a_{xwz}}}+\\frac{1}{n_{b_{xwz}}})}} \\\\\n",
    "\\text{6.} \\quad & \\text{For } i:n_{boot}\\text{\\quad do:} \\\\\n",
    "\\text{7.} \\quad & \\quad \\text{Pool samples a and b}\\\\\n",
    "\\text{8.} \\quad & \\quad \\text{draw samples } a^{boot} \\text{ and } b^{boot} \\text{ from pooled sample with replacement}\\\\\n",
    "\\text{9.} \\quad & \\quad \\text{for } a^{boot}, b^{boot}\\text{ do:} \\\\\n",
    "\\text{10.} \\quad & \\quad \\quad \\text{repeat steps 2-5} \\\\\n",
    "\\text{11.} \\quad &  \\text{Compute p-values } p=\\frac{\\#\\{|z^{boot} \\geq z|\\}}{n_{boot}} \\\\\n",
    "\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again assume exchangeability between samples under the null. Thus, we pool the samples again. The $z$-statistic is then bootstrapped under the null. We then compute the proportion of $z^{boot}$ which is at least as extreme as our observed $z$. The rationale is to compare how likely it is to obtain a $z$-statistic in the magnitude of the observed one assuming the null is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Number of bootstrap iterations\n",
    "B = 10000\n",
    "#empty arrays to store bootstrapped z-statistics\n",
    "bootstrap_z_stats_sample_1_sample_2 = np.empty((8,B))\n",
    "bootstrap_z_stats_sample_1_sample_3 = np.empty((8,B))\n",
    "\n",
    "#loop through B iterations\n",
    "for i in tqdm.tqdm(range(B), desc='Bootstrapping'):\n",
    "\n",
    "    #sample 1 vs sample 2\n",
    "    ##pool the samples\n",
    "    pooled_bootstrap_sample = pd.concat([sample_1, sample_2])\n",
    "    #shuffle pooled data\n",
    "    bootstrap_sample = pooled_bootstrap_sample.sample(n=len(pooled_bootstrap_sample), replace=True)\n",
    "    #create bootstrap samples\n",
    "    bootstrap_sample_1 = bootstrap_sample.iloc[:len(sample_1)]\n",
    "    bootstrap_sample_2 = bootstrap_sample.iloc[len(sample_1):]\n",
    "    #perform z-test\n",
    "    bootstrap_z_stats_sample_1_sample_2[:,i],_,_ = z_test(bootstrap_sample_1, bootstrap_sample_2)\n",
    "\n",
    "    #sample 1 vs sample 3\n",
    "    ##pool the samples\n",
    "    pooled_bootstrap_sample = pd.concat([sample_1, sample_3])\n",
    "    #shuffle pooled data\n",
    "    bootstrap_sample = pooled_bootstrap_sample.sample(n=len(pooled_bootstrap_sample), replace=True)\n",
    "    #create bootstrap samples\n",
    "    bootstrap_sample_1 = bootstrap_sample.iloc[:len(sample_1)]\n",
    "    bootstrap_sample_3 = bootstrap_sample.iloc[len(sample_1):]\n",
    "    #perform z-test\n",
    "    bootstrap_z_stats_sample_1_sample_3[:,i],_,_ = z_test(bootstrap_sample_1, bootstrap_sample_3)\n",
    "\n",
    "#compute p-vals\n",
    "bootstrap_p_values_sample_1_sample_2 = np.zeros(8)\n",
    "bootstrap_p_values_sample_1_sample_3 = np.zeros(8)\n",
    "\n",
    "for i in range(8):\n",
    "    bootstrap_p_values_sample_1_sample_2[i] = np.sum(np.abs(bootstrap_z_stats_sample_1_sample_2[i,:]) >= np.abs(z_stats_sample_1_sample_2[i]))/B\n",
    "    bootstrap_p_values_sample_1_sample_3[i] = np.sum(np.abs(bootstrap_z_stats_sample_1_sample_3[i,:]) >= np.abs(z_stats_sample_1_sample_3[i]))/B\n",
    "\n",
    "#adjust for multiple testing\n",
    "bh_bootstrap_p_values_sample_1_sample_2 = multitest.multipletests(bootstrap_p_values_sample_1_sample_2, method='fdr_bh')[1]\n",
    "bh_bootstrap_p_values_sample_1_sample_3 = multitest.multipletests(bootstrap_p_values_sample_1_sample_3, method='fdr_bh')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the results and p-values visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample 1 vs Sample 2\n",
    "fig, axs = plt.subplots(2,4, figsize=(20,10))\n",
    "for i in range(8):\n",
    "    ax = axs[i//4, i%4]\n",
    "    ax.hist(bootstrap_z_stats_sample_1_sample_2[i], bins=50, alpha=0.3, label='Bootstrapped', color='blue')\n",
    "    ax.axvline(z_stats_sample_1_sample_2[i], color='red', label='Observed')\n",
    "    ax.set_title(f'No Shift: Differences in P(Y=1|X={y_conditional_difference_sample_1_sample_2[\"X\"].values[i]}, Z={y_conditional_difference_sample_1_sample_2[\"Z\"].values[i]}, W={y_conditional_difference_sample_1_sample_2[\"W\"].values[i]})')\n",
    "    ax.set_xlabel('Z-statistic')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    # Create a legend without the p-value as a color-coded entry\n",
    "    handles = [plt.Line2D([0], [0], color='blue',alpha = 0.3, lw=2, label='Bootstrapped'),\n",
    "               plt.Line2D([0], [0], color='red', lw=2, label='Observed')]\n",
    "    \n",
    "    # Add the p-value as a plain text\n",
    "    ax.legend(handles=handles + [plt.Line2D([0], [0], color='none', label=f'p-value: {bh_bootstrap_p_values_sample_1_sample_2[i]:.3f}')])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Sample 1 vs Sample 3\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "for i in range(8):\n",
    "    ax = axs[i//4, i%4]\n",
    "    ax.hist(bootstrap_z_stats_sample_1_sample_3[i], bins=50, alpha=0.3, label='Bootstrapped', color='blue')\n",
    "    ax.axvline(z_stats_sample_1_sample_3[i], color='red', label='Observed')\n",
    "    ax.set_title(f'Shift: Differences in P(Y=1|X={y_conditional_difference_sample_1_sample_2[\"X\"].values[i]}, Z={y_conditional_difference_sample_1_sample_2[\"Z\"].values[i]}, W={y_conditional_difference_sample_1_sample_2[\"W\"].values[i]})')\n",
    "    ax.set_xlabel('Z-statistic')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    # Create a legend without the p-value as a color-coded entry\n",
    "    handles = [plt.Line2D([0], [0], color='blue',alpha = 0.3, lw=2, label='Bootstrapped'),\n",
    "               plt.Line2D([0], [0], color='red', lw=2, label='Observed')]\n",
    "    \n",
    "    # Add the p-value as a plain text\n",
    "    ax.legend(handles=handles + [plt.Line2D([0], [0], color='none', label=f'p-value: {bh_bootstrap_p_values_sample_1_sample_3[i]:.3f}')])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It  yields the same (non-)rejections as the base $Z$-test and previous tests before. For no shifts, no null is rejected. For a shift of $d=0.09$, the same significantly different proportions are detected in the $X^2$ and Bootstrap test. Again, for larger sample sizes or stronger shifts, we would expect the results to be closer to the ground-truth (in future appendix).\n",
    "\n",
    "\n",
    "## 1.4 Conditional Independence Testing via weighted outcomes\n",
    "\n",
    "We will now test an approach laid out by [Schrouff et al (2022)](https://proceedings.neurips.cc/paper_files/paper/2022/hash/7a969c30dc7e74d4e891c8ffb217cf79-Abstract-Conference.html), where the problem is reduced to testing:\n",
    "\\begin{align*}\n",
    "& \\text{H}_0: \\text{E}[\\hat{w}_aY|S=a] - \\text{E}[\\hat{w}_bY|S=b] = 0 \\quad \\text{vs} \\quad \\text{H}_1:\\text{E}[\\hat{w}_aY|S=a] - \\text{E}[\\hat{w}_bY|S=b] \\neq 0\n",
    "\\end{align*}\n",
    "\n",
    "where S=a or S=b in our example indicates whether a sample is coming from location a or location b. Essentially, the problem of testing conditional differences is reduced to testing whether weighted expected outcomes are different. The authors describe that the weights can be learned from the data as $\\hat{w}_a = P(S=a|X,W,Z)^{-1}$ and similarly for $\\hat{w}_b$. Their corresponding 'population' versions are $w_a = \\frac{1}{P(X,W,Z|S=a)}$ and $w_b = \\frac{1}{P(X,W,Z|S=b)}$ respectively.\n",
    "\n",
    " A formal derivation/description of the intuition behind this procedure will be in the write-up. The main idea is that given two samples, if we are weighting the outcomes $Y$ by the inverse probability of $pa(Y)$ for the corresponding sample, any difference in this weighted outcomes will be due to differences in $\\sum_{x,w,z}E[Y|X=x,W=w,Z=z,S=s]$, where S=s indicates which sample (i.e. sample 1) we are referring to. The authors additionally use (iterated) expectations and the total expectation theorem to show that starting from $P(Y|X,W,Z,S=a) = P(Y|X,W,Z,S=b)$ the problem can be rewritten to comparing weighted outcomes of Y for both samples. \n",
    "\n",
    " The testing algorithm as included in the paper is given by:\n",
    "\n",
    "<img src=\"../images/CondIndepTesting_Schrouff.png\" alt=\"Conditional Indep. Testing Schrouff et al\" width=\"600\"/>\n",
    "\n",
    "A high-level description is that we split each sample into two groups:  \n",
    "1. $D_w$: subsample for estimating the weights\n",
    "2. $D_t$: subsample to apply the weights for and used for testing\n",
    "\n",
    "Then, we resample the testing sets seperately using bootstrap. Assuming the variable we are testing (i.e.$Y$) has parents, we resample the weighting sets seperately using bootstrap. Then, we pool the bootstrapped weighting sets together and build a predictor $p(S|pa(Y))$ to seperate, for instance, whether an observation is coming from sample a or b. For comparing sample a and b we would have the following relationship:\n",
    "\\begin{align*}\n",
    "P(S=a|pa(Y)) = 1 - P(S=b|pa(Y)) \n",
    "\\end{align*} \n",
    "The inverse probabilities applied to the observations from the  bootstrapped testing sets are then used as our weights. Using the weights, we then compute the t-statistics for differences in the weighted means. While the algorithm does not explicitly mention how the p-values are obtained, the supplemental material explains that the authors use a two-sided $Z$-test for testing whether the resulting bootstraps t-statistics distribution's mean is significantly different from zero. The intuition is that if two samples are drawn from the same distribution, the bootstrapped t-statitics should center around 0.\n",
    "\n",
    "We could compute the weights with different approaches. For instance:\n",
    "\n",
    "1. Empirical weights: based of the pooled sample we could simply compute empirical versions of $w_a$ and $w_b$.\n",
    "2. Logistic regression: fit a logistic regression with the sample indicator being the outcome and $X,W,Z$ being the predictors\n",
    "\n",
    "Note, that is also possible to use more complex models (tree-based, neural-nets) to estimate the weights depending on the expected complexity of the relationship.\n",
    "\n",
    "The former is non-parametric and data-driven. However, the resulting weights would be discrete in the sense that we have 8 different weights for each sample (i.e. one for each ($X,W,Z$) combination). \n",
    "As we've done that before, we will now try to work with a model-based approach. Thus, we will try logistic regression at first. Let's start by just fitting a logistic regression on the overall pooled samples to see how well it works in distinguishing between samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Assume we use a split of 0.75 for estimating the weights using logistic regression\n",
    "split = 0.75\n",
    "##Split the samples\n",
    "sample_1_weights = sample_1.sample(frac=split)\n",
    "sample_2_weights = sample_2.sample(frac=split)\n",
    "sample_3_weights = sample_3.sample(frac=split)\n",
    "\n",
    "#Fit logistic regression model to estimate weights\n",
    "#Sample 1 vs Sample 2\n",
    "##Pool the samples\n",
    "pooled_weights = pd.concat([sample_1_weights, sample_2_weights])\n",
    "##Add sample indicator\n",
    "pooled_weights['Sample'] = ['Sample 1']*len(sample_1_weights) + ['Sample 2']*len(sample_2_weights)\n",
    "##Feature matrix X with intercept\n",
    "X = sm.add_constant(pooled_weights[['X','Z','W']])\n",
    "##Target vector Y with sample 1 as reference\n",
    "Y = (pooled_weights['Sample'] == 'Sample 2').astype(int)\n",
    "##Fit logistic regression model\n",
    "logit_model_s1s2 = sm.Logit(Y, X)\n",
    "print(f\"Logistic regression model for sample 1 vs sample 2:\\n\\n{logit_model_s1s2.fit().summary()}\\n\")\n",
    "\n",
    "#Sample 1 vs Sample 3\n",
    "##Pool the samples\n",
    "pooled_weights = pd.concat([sample_1_weights, sample_3_weights])\n",
    "##Add sample indicator\n",
    "pooled_weights['Sample'] = ['Sample 1']*len(sample_1_weights) + ['Sample 3']*len(sample_3_weights)\n",
    "##Feature matrix X with intercept\n",
    "X = sm.add_constant(pooled_weights[['X','Z','W']])\n",
    "##Target vector Y with sample 1 as reference\n",
    "Y = (pooled_weights['Sample'] == 'Sample 3').astype(int)\n",
    "##Fit logistic regression model\n",
    "logit_model_s1s3 = sm.Logit(Y, X)\n",
    "print(f\"Logistic regression model for sample 1 vs sample 3:\\n\\n{logit_model_s1s3.fit().summary()}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, we see for both cases (Sample 1 vs Sample 2 and Sample 1 vs Sample 3) that the specified log. regression is not better than the null model. And this makes sense. Given that the weights are supposed to estimate $P(X,W,Z|S=s)$, and across all samples $P(X,W,Z)$ does not differ, the logistic regression should not really be able to distinguish between different samples. Hence, we should expect weights to be similar between samples in cases where $pa(Y)$ are not affected by shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract weights for sample 1 vs sample 2\n",
    "##get remaining samples for testing\n",
    "sample_1_remaining = sample_1[~sample_1.index.isin(sample_1_weights.index)]\n",
    "X = sm.add_constant(sample_1_remaining[['X','Z','W']])\n",
    "##extract probabilities\n",
    "p_s1s2 = logit_model_s1s2.fit(disp=0).predict(X)\n",
    "p_s1 = 1-p_s1s2\n",
    "p_s2 = p_s1s2\n",
    "w_s1 = 1/p_s1\n",
    "w_s2 = 1/p_s2\n",
    "print(f'Average weight for sample 1: {np.mean(w_s1):.4f}\\nAverage weight for sample 2: {np.mean(w_s2):.4f}\\n')\n",
    "\n",
    "#Extract weights for sample 1 vs sample 3\n",
    "##get remaining samples for testing\n",
    "sample_1_remaining = sample_1[~sample_1.index.isin(sample_1_weights.index)]\n",
    "X = sm.add_constant(sample_1_remaining[['X','Z','W']])\n",
    "##extract probabilities\n",
    "p_s1s3 = logit_model_s1s3.fit(disp=0).predict(X)\n",
    "p_s1 = 1-p_s1s3\n",
    "p_s3 = p_s1s3\n",
    "w_s1 = 1/p_s1\n",
    "w_s3 = 1/p_s3\n",
    "print(f'Average weight for sample 1: {np.mean(w_s1):.4f}\\nAverage weight for sample 3: {np.mean(w_s3):.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the weights are very similar between samples. Thus, any significant difference in $E[\\hat{w}Y]$ will be attributable to differences in $Y$ between samples. Also, the paper does not really mention whether or not the weights are normalized or not. The supplemental material hints at that, but does not explicitly state it. However to avoid excessively large t-values we will normalize the weights as:\n",
    "\\begin{align*}\n",
    "\\hat{w}^{norm}_i &=\\frac{\\hat{w}_i}{\\sum_i\\hat{w}_i}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now, we are ready to perform the test as described in the paper. We will begin with a split of $0.75$ (i.e. $75\\%$ of observations used for weight estimation) between weight and testing set. Besides the bootstrapped t-statistics, we will also inspect the probabilities assigned to observations being from a specific sample, and the weighted outcomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define function to perform weighted bootstrap test\n",
    "def weighted_outcomes_bootstrap_test_schrouff(sample_a,sample_b, causal_parents_source, B=10000, split=0.75):\n",
    "\n",
    "    '''\n",
    "    Function to perform weighted bootstrap test for difference in conditional probabilities of Y=1 given X,Z,W for two samples\n",
    "    as described in Schrouff et al. (2022)\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    sample_a: pandas DataFrame containing sample A\n",
    "    sample_b: pandas DataFrame containing sample B\n",
    "    causal_parents_source: list of strings containing causal paraents of variable of interest (i.e. PA(Y) = {X,Z,W})\n",
    "    B: int, number of bootstrap iterations\n",
    "    split: float, proportion of samples to use for estimating the weights using logistic regression\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    bootstrapped_t_stats: numpy array containing bootstrapped t-statistics for each tested conditional probability\n",
    "    bootstrapped_mean_probabilities: numpy array containing the mean bootstrapped probabilities for each tested conditional probability\n",
    "    bootstrapped_weights_mean: numpy array containing the mean bootstrapped weights for each sample\n",
    "    bootstrapped_weighted_means: numpy array containing the bootstrapped weighted means for each sample\n",
    "    bootstrapped_pooled_se: numpy array containing the bootstrapped pooled standard errors for each tested conditional probability\n",
    "    p_values: float containing the p-value obtained from a two-sided z-test of the bootstrapped t-statistics distribution against 0\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "    #Step 1: Split initial samples into weighting and testing sets\n",
    "    ##Split the samples\n",
    "    sample_a_weights = sample_a.sample(frac=split)\n",
    "    sample_b_weights = sample_b.sample(frac=split)\n",
    "    sample_a_testing = sample_a[~sample_a.index.isin(sample_a_weights.index)]\n",
    "    sample_b_testing = sample_b[~sample_b.index.isin(sample_b_weights.index)]\n",
    "    ##initialize empty arrays to store bootstrapped t-statistics, bootstrapped probabilities, mean bootstrapped weights and bootstrapped weighted means\n",
    "    bootstrapped_t_stats = np.empty(B)\n",
    "    bootstrapped_mean_probabilities = np.empty(B)\n",
    "    bootstrapped_weights_mean = np.empty((2,B))\n",
    "    bootstrapped_weighted_outcome = np.empty((2,B))\n",
    "    bootstrapped_pooled_se = np.empty(B)\n",
    "\n",
    "    #Step2: perform bootstrap\n",
    "    for i in tqdm.tqdm(range(B), desc='Bootstrapping'):\n",
    "        #Test, for each bootstrap iteration have a different set for weights and testing\n",
    "        #Step 2.1: Sample with replacement from the weights and testing\n",
    "        bootstrap_sample_a_weights = sample_a_weights.sample(n=len(sample_a_weights), replace=True)\n",
    "        bootstrap_sample_b_weights = sample_b_weights.sample(n=len(sample_b_weights), replace=True)\n",
    "        bootstrap_sample_a_testing = sample_a_testing.sample(n=len(sample_a_testing), replace=True)\n",
    "        bootstrap_sample_b_testing = sample_b_testing.sample(n=len(sample_b_testing), replace=True)\n",
    "\n",
    "        #Step 2.2: Fit logistic regression model to estimate weights\n",
    "        ##Pool the samples\n",
    "        pooled_weights = pd.concat([bootstrap_sample_a_weights, bootstrap_sample_b_weights])\n",
    "        ##Add sample indicator\n",
    "        pooled_weights['Sample'] = ['Sample A']*len(bootstrap_sample_a_weights) + ['Sample B']*len(bootstrap_sample_b_weights)\n",
    "        ##Feature matrix X with intercept\n",
    "        X = sm.add_constant(pooled_weights[causal_parents_source])\n",
    "        ##Target vector Y with sample 1 as reference\n",
    "        Y = (pooled_weights['Sample'] == 'Sample B').astype(int)\n",
    "        ##Fit logistic regression model\n",
    "        logit_model = sm.Logit(Y, X).fit(disp=0)\n",
    "        ##extract weights on testing set\n",
    "        X_a = sm.add_constant(bootstrap_sample_a_testing[causal_parents_source])\n",
    "        X_b = sm.add_constant(bootstrap_sample_b_testing[causal_parents_source])\n",
    "        p_a = logit_model.predict(X_a)\n",
    "        p_a = 1-p_a #need to compute 1-pa, because the logit model predicts the probability of being in class b\n",
    "        p_b = logit_model.predict(X_b)\n",
    "        #Store the mean probability for p_a\n",
    "        bootstrapped_mean_probabilities[i] = np.mean(p_a)\n",
    "        if np.any(p_a == 0) or np.any(p_b == 0):\n",
    "            print('Warning: Zero values in p_a or p_b')\n",
    "        if np.any(p_a < 1e-10) or np.any(p_b < 1e-10):  # Threshold can vary based on the scale of your data\n",
    "            print('Warning: Near-zero values in p_a or p_b')\n",
    "        #Compute the weights\n",
    "        w_a = 1/p_a\n",
    "        w_b = 1/p_b\n",
    "        #Normalize the weights\n",
    "        w_a_norm = w_a/np.sum(w_a)\n",
    "        w_b_norm = w_b/np.sum(w_b)\n",
    "        #Check for nans and infs\n",
    "        if np.any(np.isnan(w_a)) or np.any(np.isnan(w_b)):\n",
    "            print('Warning: NaN values in w_a or w_b')\n",
    "        if np.any(np.isinf(w_a)) or np.any(np.isinf(w_b)):\n",
    "            print('Warning: Inf values in w_a or w_b')\n",
    "        #Check for very small values\n",
    "        if np.any(w_a < 1e-10) or np.any(w_b < 1e-10):\n",
    "            print('Warning: Near-zero values in w_a or w_b')\n",
    "        \n",
    "        #Compute hadamard product\n",
    "        Y_a = bootstrap_sample_a_testing['Y'].values\n",
    "        Y_b = bootstrap_sample_b_testing['Y'].values\n",
    "        product_a = w_a_norm.values*Y_a\n",
    "        product_b = w_b_norm.values*Y_b\n",
    "        together = np.concatenate([product_a,-product_b])\n",
    "        bootstrapped_t_stats[i] = ttest_1samp(together, 0)[0]\n",
    "        #Compute and store weighted means\n",
    "        bootstrapped_weighted_outcome[0,i] = np.sum(product_a)\n",
    "        bootstrapped_weighted_outcome[1,i] = np.sum(product_b)\n",
    "        #Compute and store mean weights\n",
    "        bootstrapped_weights_mean[0,i] = np.mean(w_a)\n",
    "        bootstrapped_weights_mean[1,i] = np.mean(w_b)\n",
    "\n",
    "    #Step 3: Compute p-values\n",
    "    mean = np.mean(bootstrapped_t_stats)\n",
    "    std = np.std(bootstrapped_t_stats)\n",
    "    p_value = (2*norm.cdf(-abs(mean/std)))\n",
    "\n",
    "    return bootstrapped_t_stats, p_value,bootstrapped_weighted_outcome,bootstrapped_mean_probabilities, bootstrapped_weights_mean\n",
    "\n",
    "\n",
    "#Perform the test for sample 1 vs sample 2 and sample 1 vs sample 3\n",
    "#Draw the initial samples again\n",
    "sample_1, sample_2 = hiring_data(0,n=1000)\n",
    "_, sample_3 = hiring_data(1)\n",
    "\n",
    "causal_parents_source = ['X','Z','W']\n",
    "bootstrapped_t_stats_s1s2, p_value_s1s2, bootstrapped_weighted_outcomes_s1s2,mean_bootstrapped_probabilities_s1s2,bootstrapped_weights_mean_s1s2 = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_2, causal_parents_source,split=0.75,B=20000)\n",
    "bootstrapped_t_stats_s1s3, p_value_s1s3, bootstrapped_weighted_outcomes_s1s3,mean_bootstrapped_probabilities_s1s3,bootstrapped_weights_mean_s1s3 = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_3, causal_parents_source,split=0.5,B=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograms of bootstrapped t-statistics\n",
    "fig, axs = plt.subplots(1,2, figsize=(20,5))\n",
    "axs[0].hist(bootstrapped_t_stats_s1s2, bins=50, alpha=0.3, label='Bootstrapped', color='blue')\n",
    "axs[0].axvline(np.mean(bootstrapped_t_stats_s1s2), color='red', label='Mean')\n",
    "axs[0].set_title('Sample 1 vs Sample 2')\n",
    "axs[0].set_xlabel('T-statistic')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "# Create a legend without the p-value as a color-coded entry\n",
    "handles = [plt.Line2D([0], [0], color='blue',alpha = 0.3, lw=2, label='Bootstrapped'),\n",
    "              plt.Line2D([0], [0], color='red', lw=2, label='Mean')]\n",
    "axs[0].legend(handles=handles + [plt.Line2D([0], [0], color='none', label=f'p-value: {p_value_s1s2:.3f}')])\n",
    "\n",
    "# Add the p-value as a plain text\n",
    "\n",
    "axs[1].hist(bootstrapped_t_stats_s1s3, bins=50, alpha=0.3, label='Bootstrapped', color='blue')\n",
    "axs[1].axvline(np.mean(bootstrapped_t_stats_s1s3), color='red', label='Mean')\n",
    "axs[1].set_title('Sample 1 vs Sample 3')\n",
    "axs[1].set_xlabel('T-statistic')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "# Create a legend without the p-value as a color-coded entry\n",
    "handles = [plt.Line2D([0], [0], color='blue',alpha = 0.3, lw=2, label='Bootstrapped'),\n",
    "              plt.Line2D([0], [0], color='red', lw=2, label='Mean')]\n",
    "axs[1].legend(handles=handles + [plt.Line2D([0], [0], color='none', label=f'p-value: {p_value_s1s3:.3f}')])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Histograms of bootstrapped probabilities\n",
    "fig, axs = plt.subplots(1,2, figsize=(20,5))\n",
    "\n",
    "\n",
    "axs[0].hist(mean_bootstrapped_probabilities_s1s2, bins=50, alpha=0.3, label='Bootstrapped', color='blue')\n",
    "axs[0].axvline(np.mean(mean_bootstrapped_probabilities_s1s2), color='red', label='Mean')\n",
    "axs[0].set_title('P(Sample 1)')\n",
    "axs[0].set_xlabel('Probability')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "\n",
    "axs[1].hist(mean_bootstrapped_probabilities_s1s3, bins=50, alpha=0.3, label='Bootstrapped', color='blue')\n",
    "axs[1].axvline(np.mean(mean_bootstrapped_probabilities_s1s3), color='red', label='Mean')\n",
    "axs[1].set_title('P(Sample 1)')\n",
    "axs[1].set_xlabel('Probability')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Histograms of weighted outcomes\n",
    "fig, axs = plt.subplots(1,2, figsize=(20,5))\n",
    "\n",
    "axs[0].hist(bootstrapped_weighted_outcomes_s1s2[0], bins=50, alpha=0.3, label='Sample 1', color='blue')\n",
    "axs[0].hist(bootstrapped_weighted_outcomes_s1s2[1], bins=50, alpha=0.3, label='Sample 2', color='red')\n",
    "#Add vertical lines for means\n",
    "axs[0].axvline(np.mean(bootstrapped_weighted_outcomes_s1s2[0]), color='blue', label='Mean Sample 1')\n",
    "axs[0].axvline(np.mean(bootstrapped_weighted_outcomes_s1s2[1]), color='red', label='Mean Sample 2')\n",
    "axs[0].set_title('Weighted Means')\n",
    "axs[0].set_xlabel('Weighted Mean')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "#Add means \n",
    "axs[0].legend([f'Mean Sample 1: {np.mean(bootstrapped_weighted_outcomes_s1s2[0]):.3f}', f'Mean Sample 2: {np.mean(bootstrapped_weighted_outcomes_s1s2[1]):.3f}'])\n",
    "\n",
    "\n",
    "\n",
    "axs[1].hist(bootstrapped_weighted_outcomes_s1s3[0], bins=50, alpha=0.3, label='Sample 1', color='blue')\n",
    "axs[1].hist(bootstrapped_weighted_outcomes_s1s3[1], bins=50, alpha=0.3, label='Sample 3', color='red')\n",
    "#Add vertical lines for means\n",
    "axs[1].axvline(np.mean(bootstrapped_weighted_outcomes_s1s3[0]), color='blue', label='Mean Sample 1')\n",
    "axs[1].axvline(np.mean(bootstrapped_weighted_outcomes_s1s3[1]), color='red', label='Mean Sample 2')\n",
    "axs[1].set_title('Weighted Means')\n",
    "axs[1].set_xlabel('Weighted Mean')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "#Add means\n",
    "axs[1].legend([f'Mean Sample 1: {np.mean(bootstrapped_weighted_outcomes_s1s3[0]):.3f}', f'Mean Sample 3: {np.mean(bootstrapped_weighted_outcomes_s1s3[1]):.3f}'])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the test is not able to detect a significant shift between samples 1 and sample 3, while it does not commit the error of rejecting the null between sample 1 and sample 2\n",
    "\n",
    "Furthermore, as expected the probabilities of the logistic regression for an observation being from sample 1 is around $50\\%$ for both comparisons. This is not surprising as the probabilities and weights depend on $P(X,W,Z)$. In this example, this joint probability is not affected by any shifts. Thus, the logistic regression and thus the weights should not really be able to distinguish between both samples.\n",
    "\n",
    "Another interesting observation is that the procedure quantifies the difference in weighted means between sample 1 and sample 2 as roughly $0.012$, which given they are generated using the same SCM is quite good. \n",
    "\n",
    "However, the problem still remains that the significant shift was not detected.\n",
    "\n",
    "The split between data used for weights estimation and testing followed a recommendation of the authors to allocate as many samples as possible for weight estimation. However, in all examples featured in the paper/supplemental, the sample size was comfortably $>10,000$. Here, we assumed $n=1,000$. Thus, one might have some skepticism with either very small splits (i.e. $s \\leq 0.2$) or very high splits (i.e. $s \\geq 0.8$). The skepticism is rooted in the intuition that if we only use a very small or very large fraction for weight estimation and testing there is a chance that one of the two sets will not be representative enough of the actual sample.\n",
    "\n",
    "For instance, assume that the weighting set is relatively small (i.e. $s=0.2$). In this case, we woud use 200 observations for each sample. Now assume, out of these only 20 observations are attributable to the conditional ($X=1,Z=0,W=1$) even though there are a total of 150 observations (dummy example) of it in sample a. If there are $> 20$ observations corresponding to that conditional in the weight set for sample b, the weighting procedure might conclude that these observations are more likely to occur in sample b than in sample a, even though the overall occurences for ($X=1,Z=0,W=1$) might be lower in sample b compared to sample a. This can lead to inaccurate weights, which has repercussions for the computed t-statistics and test decision. Similarly, for large splits, it might be the case that the observations in the testing set are not well represented during weighting. This also can lead to inaccurate weights with negative consequences for the computed statistics and test decision made.\n",
    "\n",
    "Thus, we will perform the test again for a different number of splits starting from $s=0.1$ with increments of $+0.1$ until we end with $s=0.9$. We will pay a special attention to whether the splits might have an influence on the tests power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of splits\n",
    "s = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "B = 10000\n",
    "#empty arrays to store results\n",
    "bootstrapped_t_stats_s1s3_splits = np.empty((len(s),B)) \n",
    "p_values_s1s3_splits = np.empty(len(s))\n",
    "bootstrapped_weighted_means_s1s3_splits = np.empty((len(s),2,B))    \n",
    "causal_parents_source = ['X','Z','W']\n",
    "p_values_s1s3_splits = np.empty(len(s))\n",
    "#loop through splits\n",
    "for i in range(len(s)):\n",
    "    \n",
    "    #Obtain bootstrapped t-statistics, bootstrapped probabilities, mean bootstrapped weights and bootstrapped weighted means\n",
    "    bootstrapped_t_stats_s1s3_splits[i], p_values_s1s3_splits[i], bootstrapped_weighted_means_s1s3_splits[i],_,_   = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_3, causal_parents_source,split=s[i])\n",
    "    #print(f\" z-statistic for sample 1 vs sample 3 with split = {s[i]}: {p_values_s1s3_splits[i]:.4f}\\n p-value for sample 1 vs sample 3 with split = {s[i]}: {p_values_s1s3_splits[i]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of bootstrapped t-statistics for different splits\n",
    "fig, axs = plt.subplots(3,3, figsize=(20,10))\n",
    "\n",
    "for i in range(len(s)):\n",
    "\n",
    "    axs[i//3,i%3].hist(bootstrapped_t_stats_s1s3_splits[i], bins=50, alpha=0.3, label='Bootstrapped', color='blue')\n",
    "    #Add mean t-statistic\n",
    "    axs[i//3,i%3].axvline(np.mean(bootstrapped_t_stats_s1s3_splits[i]), color='green', label='Mean')\n",
    "    axs[i//3,i%3].set_title(f'Shift: Bootstrapped t-statistics with split = {s[i]}')\n",
    "    axs[i//3,i%3].set_xlabel('Bootstrapped t-statistic')\n",
    "    axs[i//3,i%3].set_ylabel('Frequency')\n",
    "    #Add p-value as a plain text\n",
    "    axs[i//3,i%3].legend([f'p-value: {p_values_s1s3_splits[i]:.4f}'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Histogram of weighted means for different splits\n",
    "fig, axs = plt.subplots(3,3, figsize=(20,10))\n",
    "\n",
    "for i in range(len(s)):\n",
    "    axs[i//3,i%3].hist(bootstrapped_weighted_means_s1s3_splits[i][0], bins=50, alpha=0.3, label='Sample 1', color='blue')\n",
    "    axs[i//3,i%3].hist(bootstrapped_weighted_means_s1s3_splits[i][1], bins=50, alpha=0.3, label='Sample 3', color='red')\n",
    "    #Add mean t-statistic\n",
    "    axs[i//3,i%3].axvline(np.mean(bootstrapped_weighted_means_s1s3_splits[i][0]), color='green', label='Mean Weights Sample 1')\n",
    "    axs[i//3,i%3].axvline(np.mean(bootstrapped_weighted_means_s1s3_splits[i][1]), color='purple', label='Mean Weights Sample 3')\n",
    "    axs[i//3,i%3].set_title(f'Shift: Bootstrapped weighted means with split = {s[i]}')\n",
    "    axs[i//3,i%3].set_xlabel('Bootstrapped mean weight')\n",
    "    axs[i//3,i%3].set_ylabel('Frequency')\n",
    "    axs[i//3,i%3].legend([f'Mean Weights Sample 1: {np.mean(bootstrapped_weighted_means_s1s3_splits[i][0]):.4f}', f'Mean Weights Sample 3: {np.mean(bootstrapped_weighted_means_s1s3_splits[i][1]):.4f}'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we see for $s=0.8$ that the test is not able to detect a significant shift. This might support the intuition that higher/lower splits lead to decreased power if the original sample sizes are small. However, it is important to note that for the more extreme splits of $s=0.1$ and $s=0.9$, the test detected significant shifts. This gives an inconclusive result as to which split to choose for our example. \n",
    "\n",
    "We will repeat the procedure for the different splits for the case of no shifts (Sample 1 and Sample 2) to see if specific splits might lead to type I errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of splits\n",
    "s = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "B = 10000\n",
    "#empty arrays to store results\n",
    "bootstrapped_t_stats_s1s2_splits = np.empty((len(s),B)) \n",
    "p_values_s1s2_splits = np.empty(len(s))\n",
    "bootstrapped_weighted_means_s1s2_splits = np.empty((len(s),2,B))  \n",
    "p_values_s1s2_splits = np.empty(len(s))  \n",
    "causal_parents_source = ['X','Z','W']\n",
    "#loop through splits\n",
    "for i in range(len(s)):\n",
    "    \n",
    "    #Obtain bootstrapped t-statistics, bootstrapped probabilities, mean bootstrapped weights and bootstrapped weighted means\n",
    "    bootstrapped_t_stats_s1s2_splits[i], p_values_s1s2_splits[i], bootstrapped_weighted_means_s1s2_splits[i],_,_ = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_2, causal_parents_source,split=s[i])\n",
    "    #print(f\" z-statistic for sample 1 vs sample 2 with split = {s[i]}: {p_values_s1s2_splits[i]:.4f}\\n p-value for sample 1 vs sample 3 with split = {s[i]}: {p_values_s1s2_splits[i]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of bootstrapped t-statistics for different splits\n",
    "fig, axs = plt.subplots(3,3, figsize=(20,10))\n",
    "\n",
    "for i in range(len(s)):\n",
    "\n",
    "    axs[i//3,i%3].hist(bootstrapped_t_stats_s1s2_splits[i], bins=50, alpha=0.3, label='Bootstrapped', color='blue')\n",
    "    #Add mean t-statistic\n",
    "    axs[i//3,i%3].axvline(np.mean(bootstrapped_t_stats_s1s2_splits[i]), color='green', label='Mean')\n",
    "    axs[i//3,i%3].set_title(f'No shift: Bootstrapped t-statistics with split = {s[i]}')\n",
    "    axs[i//3,i%3].set_xlabel('Bootstrapped t-statistic')\n",
    "    axs[i//3,i%3].set_ylabel('Frequency')\n",
    "    #Add p-value as a plain text\n",
    "    axs[i//3,i%3].legend([f'p-value: {p_values_s1s2_splits[i]:.4f}'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Histogram of mean bootstrapped weights for different splits\n",
    "fig, axs = plt.subplots(3,3, figsize=(20,10))\n",
    "\n",
    "for i in range(len(s)):\n",
    "    axs[i//3,i%3].hist(bootstrapped_weighted_means_s1s2_splits[i][0], bins=50, alpha=0.3, label='Sample 1', color='blue')\n",
    "    axs[i//3,i%3].hist(bootstrapped_weighted_means_s1s2_splits[i][1], bins=50, alpha=0.3, label='Sample 2', color='red')\n",
    "    #Add mean t-statistic\n",
    "    axs[i//3,i%3].axvline(np.mean(bootstrapped_weighted_means_s1s2_splits[i][0]), color='green', label='Mean Weights Sample 1')\n",
    "    axs[i//3,i%3].axvline(np.mean(bootstrapped_weighted_means_s1s2_splits[i][1]), color='purple', label='Mean Weights Sample 2')\n",
    "    axs[i//3,i%3].set_title(f'No shift: Bootstrapped weighted means with split = {s[i]}')\n",
    "    axs[i//3,i%3].set_xlabel('Bootstrapped mean weight')\n",
    "    axs[i//3,i%3].set_ylabel('Frequency')\n",
    "    axs[i//3,i%3].legend([f'Mean Weights Sample 1: {np.mean(bootstrapped_weighted_means_s1s2_splits[i][0]):.4f}', f'Mean Weights Sample 2: {np.mean(bootstrapped_weighted_means_s1s2_splits[i][1]):.4f}'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small simulation did not indicate that the test might be at risk of commiting Type I erros for extreme splits. For all splits the corresponding p-values are significantly larger than $>0.05$. So it seems that the splits only have a impact on the power in our brief simulation.\n",
    "\n",
    "Going forward, we will continue with $s=0.4$ based on the intuition that the generated weighted means are somewhat reflective of the ground-truth. This is slightly biased due to us knowing the ground-truth. It is still not very clear which split is ideal in case of samples sizes of $n\\leq1,000$. Both weighting and testing are equally important and should contain data reflective of the sample. Thus, a good heuristic could be to balance both sets somewhat evenly by setting $0.4\\leq s\\leq0.6$.\n",
    "\n",
    "We now are interested for which shift magnitudes the test is able to detect shifts. We saw that it was able to detect shifts with magnitude $d=0.09$ given specific splits. The assumption is that it most likely will be able to detect shifts larger than that. Hence, we focus on very small shifts. We begin with $d=0.01$ using increments of $+0.01$ until we end with $d=0.08$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform the test now for different shifts\n",
    "d = [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08]\n",
    "\n",
    "#empty arrays to store results\n",
    "bootstrapped_t_stats_s1s3_shifts = np.empty((len(d),B))\n",
    "p_values_s1s3_shifts = np.empty(len(d))\n",
    "bootstrapped_weighted_means_s1s3_shifts = np.empty((len(d),2,B))\n",
    "#loop through shifts\n",
    "for i in range(len(d)):\n",
    "    \n",
    "    #Obtain bootstrapped t-statistics, bootstrapped probabilities, mean bootstrapped weights and bootstrapped weighted means\n",
    "    sample_1, sample_3 = hiring_data(1,d=d[i])\n",
    "    bootstrapped_t_stats_s1s3_shifts[i], p_values_s1s3_shifts[i],bootstrapped_weighted_means_s1s3_splits[i],_,_  = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_3, causal_parents_source,split=0.4)\n",
    "    #print(f\" z-statistic for sample 1 vs sample 3 with shift = {d[i]}: {p_values_s1s3_shifts[i]:.4f}\\n p-value for sample 1 vs sample 3 with shift = {d[i]}: {p_values_s1s3_shifts[i]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of bootstrapped t-statistics for different shifts\n",
    "fig, axs = plt.subplots(2,4, figsize=(20,10))\n",
    "\n",
    "for i in range(len(d)):\n",
    "    axs[i//4,i%4].hist(bootstrapped_t_stats_s1s3_shifts[i], bins=50, alpha=0.3, label='Bootstrapped', color='blue')\n",
    "    #Add mean t-statistic\n",
    "    axs[i//4,i%4].axvline(np.mean(bootstrapped_t_stats_s1s3_shifts[i]), color='green', label='Mean')\n",
    "    axs[i//4,i%4].set_title(f'Shift: Bootstrapped t-statistics with d = {d[i]}')\n",
    "    axs[i//4,i%4].set_xlabel('Bootstrapped t-statistic')\n",
    "    axs[i//4,i%4].set_ylabel('Frequency')\n",
    "    #Add p-value without color coding\n",
    "    handles = [plt.Line2D([0], [0], color='blue',alpha = 0.3, lw=2, label='Bootstrapped'),\n",
    "                plt.Line2D([0], [0], color='green', lw=2, label='Mean')]\n",
    "    axs[i//4,i%4].legend(handles=handles + [plt.Line2D([0], [0], color='none', label=f'p-value: {p_values_s1s3_shifts[i]:.3f}')])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Histogram of mean bootstrapped weights for different shifts\n",
    "fig, axs = plt.subplots(2,4, figsize=(20,10))\n",
    "\n",
    "for i in range(len(d)):\n",
    "    axs[i//4,i%4].hist(bootstrapped_weighted_means_s1s3_splits[i][0], bins=50, alpha=0.3, label='Sample 1', color='blue')\n",
    "    axs[i//4,i%4].hist(bootstrapped_weighted_means_s1s3_splits[i][1], bins=50, alpha=0.3, label='Sample 3', color='red')\n",
    "    #Add mean t-statistic\n",
    "    axs[i//4,i%4].axvline(np.mean(bootstrapped_weighted_means_s1s3_splits[i][0]), color='green', label='Mean Sample 1')\n",
    "    axs[i//4,i%4].axvline(np.mean(bootstrapped_weighted_means_s1s3_splits[i][1]), color='purple', label='MeanSample 3')\n",
    "    axs[i//4,i%4].set_title(f'Shift: weighted means with d = {d[i]}')\n",
    "    axs[i//4,i%4].set_xlabel('Bootstrapped mean weight')\n",
    "    axs[i//4,i%4].set_ylabel('Frequency')\n",
    "    axs[i//4,i%4].legend([f'Mean Sample 1: {np.mean(bootstrapped_weighted_means_s1s3_splits[i][0]):.4f}', f'Mean Sample 3: {np.mean(bootstrapped_weighted_means_s1s3_splits[i][1]):.4f}'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When simulating the example with smaller shifts (i.e. range from 0.01 to 0.08), we see that for shifts $d\\leq0.3$ and $d\\geq 0.7$, the test detected significant differences. The remaining three are not rejected. This, hints that for $n=1,000$, the test might not be reliable for smaller shifts. It seems a bit odd that shifts towards the lower and upper end were detected, but shifts in the mid-range were not. Likely it's due to some natural variation when sampling data with these different shifts.\n",
    "\n",
    "However, we will run the test again with $n= 5000$ to see if we see a hint that increasing sample sizes will be able to detect all smaller shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform the test now for different shifts\n",
    "d = [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08]\n",
    "\n",
    "#empty arrays to store results\n",
    "bootstrapped_t_stats_s1s3_shifts = np.empty((len(d),B))\n",
    "p_values_s1s3_shifts = np.empty(len(d))\n",
    "bootstrapped_weighted_means_s1s3_shifts = np.empty((len(d),2,B))\n",
    "#loop through shifts\n",
    "for i in range(len(d)):\n",
    "    \n",
    "    #Obtain bootstrapped t-statistics, bootstrapped probabilities, mean bootstrapped weights and bootstrapped weighted means\n",
    "    sample_1, sample_3 = hiring_data(1,d=d[i], n=5000)\n",
    "    bootstrapped_t_stats_s1s3_shifts[i], p_values_s1s3_shifts[i],bootstrapped_weighted_means_s1s3_splits[i],_,_ = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_3, causal_parents_source,split=0.4)\n",
    "    #print(f\" z-statistic for sample 1 vs sample 3 with shift = {d[i]}: {p_values_s1s3_shifts[i]:.4f}\\n p-value for sample 1 vs sample 3 with shift = {d[i]}: {p_values_s1s3_shifts[i]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of bootstrapped t-statistics for different shifts\n",
    "fig, axs = plt.subplots(2,4, figsize=(20,10))\n",
    "\n",
    "for i in range(len(d)):\n",
    "    axs[i//4,i%4].hist(bootstrapped_t_stats_s1s3_shifts[i], bins=50, alpha=0.3, label='Bootstrapped', color='blue')\n",
    "    #Add mean t-statistic\n",
    "    axs[i//4,i%4].axvline(np.mean(bootstrapped_t_stats_s1s3_shifts[i]), color='green', label='Mean')\n",
    "    axs[i//4,i%4].set_title(f'Shift: Bootstrapped t-statistics with = d={d[i]} and n=5,000')\n",
    "    axs[i//4,i%4].set_xlabel('Bootstrapped t-statistic')\n",
    "    axs[i//4,i%4].set_ylabel('Frequency')\n",
    "    #Add p-value without color coding\n",
    "    handles = [plt.Line2D([0], [0], color='blue',alpha = 0.3, lw=2, label='Bootstrapped'),\n",
    "                plt.Line2D([0], [0], color='green', lw=2, label='Mean')]\n",
    "    axs[i//4,i%4].legend(handles=handles + [plt.Line2D([0], [0], color='none', label=f'p-value: {p_values_s1s3_shifts[i]:.3f}')])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With increasing the sample size to $n= 5000$, we now see that apart from $d=0.01$ on we were able to detect all significant shifts. This hints at sample sensitivity of small shifts as with previous tests. Interestingly enough, the test shows some sort of erratic behaviour. With smaller sample size it was able to detect shifts at the lower and upper end of the simulated range while failing in the middle range. With increased sample size, the test, however, is not rejecting a shift of magnitude $d=0.01$ anymore.\n",
    "\n",
    "Nonetheless, it followed the overall intuition that larger sample sizes allow for better representation of the underlying data-generating mechanism. Thus, allowing for subtler shifts to be detected.\n",
    "\n",
    "Let's test if a sample size of $n=10,000$ allows for shifts of magnitude $d=0.01$ to be detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample 1 and Sample 3 with n = 10,000\n",
    "sample_1, sample_3 = hiring_data(1,n=10000, d=0.01)\n",
    "#Perform test\n",
    "bootstrapped_t_stats_s1s3, p_value_s1s3,_,_,_ = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_3, causal_parents_source,split=0.4,B=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "So far, we have assumed that the shift in $Y$ is a constant $d$ added to $P(Y=1|X,Z,W)$ compared to the case of no shifts. In this case, the weights between samples will be somewhat similar as $pa(Y)$ is unaffected by the shift.  \n",
    "\n",
    "We now want perform the test for more different shifts, including compound and single shifts in $pa(Y)$\n",
    "\n",
    "- A shift only in $pa(Y)$\n",
    "- A compound shift both in $Y$ and $pa(Y)$\n",
    "- A shift in $Y$ of the form of changed set $pa(Y)$ (i.e. $W$ not being a parent of $Y$ anymore)\n",
    "- A shift in $Y$ such that $P(Y|X,W,Z)$ becomes a constant. \n",
    "\n",
    "Specifically, we generated samples according to:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\text{Shift only in W: }W \\leftarrow Bernoulli(0.3)\\\\\n",
    "&\\text{Shift in W and Y: } W \\leftarrow Bernoulli(0.3) \\quad \\text{and} \\quad Y \\leftarrow Bernoulli(0.2(X+Z-2XZ)+ 0.25W + 0.09)\\\\\n",
    "&\\text{Y not depending on W: } Y \\leftarrow Bernoulli(0.2(X+Z-2XZ)+  0.02)\\\\\n",
    "&\\text{P(Y) becoming a constant: } Y \\leftarrow Bernoulli(0.5)\n",
    "\\end{align*}\n",
    "\n",
    "For these, we will perform the test using a split $s=0.5$. Given the knowledge of the underlying SCMs we would expect rejections in all but the first shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform the test now for different shifts\n",
    "##change the scm\n",
    "n = 1000\n",
    "U = np.random.normal(0, 1, n)\n",
    "X = np.random.binomial(1,np.exp(U)/(1+np.exp(U)),n)\n",
    "Z = np.random.binomial(1,np.exp(U)/(1+np.exp(U)),n)\n",
    "W = np.random.binomial(1,0.3,n)\n",
    "Y = np.random.binomial(1,(1/5)*(X+Z-2*X*Z)+(1/4)*W+0.02,n)\n",
    "sample_3_shift_only_w = pd.DataFrame({'X':X,'Z':Z,'W':W,'Y':Y})\n",
    "U = np.random.normal(0, 1, n)\n",
    "X = np.random.binomial(1,np.exp(U)/(1+np.exp(U)),n)\n",
    "Z = np.random.binomial(1,np.exp(U)/(1+np.exp(U)),n)\n",
    "W = np.random.binomial(1,0.3,n)\n",
    "Y = np.random.binomial(1,(1/5)*(X+Z-2*X*Z)+(1/4)*W+0.02+0.09,n)\n",
    "sample_3_shift_w_and_y = pd.DataFrame({'X':X,'Z':Z,'W':W,'Y':Y})\n",
    "U = np.random.normal(0, 1, n)\n",
    "X = np.random.binomial(1,np.exp(U)/(1+np.exp(U)),n)\n",
    "Z = np.random.binomial(1,np.exp(U)/(1+np.exp(U)),n)\n",
    "W = np.random.binomial(1,0.3*(np.abs(X-Z)) + 0.1,n)\n",
    "Y = np.random.binomial(1,(1/5)*(X+Z-2*X*Z)+0.02,n)\n",
    "sample_3_shift_y_not_depending_on_w = pd.DataFrame({'X':X,'Z':Z,'W':W,'Y':Y})\n",
    "U = np.random.normal(0, 1, n)\n",
    "X = np.random.binomial(1,np.exp(U)/(1+np.exp(U)),n)\n",
    "Z = np.random.binomial(1,np.exp(U)/(1+np.exp(U)),n)\n",
    "W = np.random.binomial(1,0.3*(np.abs(X-Z)) + 0.1,n)\n",
    "Y = np.random.binomial(1,(1/2),n)\n",
    "sample_3_shift_y_constant = pd.DataFrame({'X':X,'Z':Z,'W':W,'Y':Y})\n",
    "\n",
    "#perform the test for the different sample 3's now. We use a split of 0.5\n",
    "B = 10000\n",
    "sample_3_shifts = [sample_3_shift_only_w,sample_3_shift_w_and_y,sample_3_shift_y_not_depending_on_w,sample_3_shift_y_constant]\n",
    "p_values_sample_3_shifts = np.empty(len(sample_3_shifts))\n",
    "bootstrapped_t_stats_different_sample_3 = np.empty((len(sample_3_shifts),B))\n",
    "bootstrapped_weighted_means_s1s3_splits = np.empty((len(sample_3_shifts),2,B))   \n",
    "\n",
    "\n",
    "'''\n",
    "for i in range(len(sample_3_shifts)):\n",
    "\n",
    "    #Obtain bootstrapped t-statistics, bootstrapped probabilities, mean bootstrapped weights and bootstrapped weighted means\n",
    "    bootstrapped_t_stats_s1s3_shifts[i], p_values_s1s3_shifts[i],bootstrapped_weighted_means_s1s3_splits[i],_,_  = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_3_shifts[i], causal_parents_source,split=0.4)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of bootstrapped t-statistics for different shifts\n",
    "fig, axs = plt.subplots(2,2, figsize=(20,10))\n",
    "shifts = ['only in W','in W and Y','Y not depending on W','Y being constant']\n",
    "\n",
    "for i in range(len(sample_3_shifts)):\n",
    "    axs[i//2,i%2].hist(bootstrapped_t_stats_s1s3_shifts[i], bins=50, alpha=0.3, label='Bootstrapped', color='blue')\n",
    "    #Add mean t-statistic\n",
    "    axs[i//2,i%2].axvline(np.mean(bootstrapped_t_stats_s1s3_shifts[i]), color='green', label='Mean')\n",
    "    axs[i//2,i%2].set_title(f'Shift: Bootstrapped t-statistics with shift: {shifts[i]}')\n",
    "    axs[i//2,i%2].set_xlabel('Bootstrapped t-statistic')\n",
    "    axs[i//2,i%2].set_ylabel('Frequency')\n",
    "    #Add p-value as a plain text\n",
    "    axs[i//2,i%2].legend([f'p-value: {p_values_s1s3_shifts[i]:.4f}'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Histogram of mean bootstrapped weights for different shifts\n",
    "fig, axs = plt.subplots(2,2, figsize=(20,10))\n",
    "\n",
    "for i in range(len(sample_3_shifts)):\n",
    "    axs[i//2,i%2].hist(bootstrapped_weighted_means_s1s3_splits[i][0], bins=50, alpha=0.3, label='Sample 1', color='blue')\n",
    "    axs[i//2,i%2].hist(bootstrapped_weighted_means_s1s3_splits[i][1], bins=50, alpha=0.3, label='Sample 3', color='red')\n",
    "    #Add mean t-statistic\n",
    "    axs[i//2,i%2].axvline(np.mean(bootstrapped_weighted_means_s1s3_splits[i][0]), color='green', label='Mean Weights Sample 1')\n",
    "    axs[i//2,i%2].axvline(np.mean(bootstrapped_weighted_means_s1s3_splits[i][1]), color='purple', label='Mean Weights Sample 3')\n",
    "    axs[i//2,i%2].set_title(f'Shift: weighted means with shift: {shifts[i]}')\n",
    "    axs[i//2,i%2].set_xlabel('Bootstrapped mean weight')\n",
    "    axs[i//2,i%2].set_ylabel('Frequency')\n",
    "    axs[i//2,i%2].legend([f'Mean Weights Sample 1: {np.mean(bootstrapped_weighted_means_s1s3_splits[i][0]):.4f}', f'Mean Weights Sample 3: {np.mean(bootstrapped_weighted_means_s1s3_splits[i][1]):.4f}'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all cases the test correctly did not reject or rejected the null. This is a promising first result as it shows that this method seems to work on different types of shifts in one variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Divergence based testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last approach for detecting distribution shift is to use divergences such as the Kullback-Leibler (KL) divergence and has gathered increasing attention by ML researchers and  practicioners. Works such as [Polo et al (2023)](https://www.sciencedirect.com/science/article/pii/S0020025523011970) have looked into using them to decide when a target distribution might be different from a source distribution and used test statistics based on the KL divergence for hypothesis testing.\n",
    "\n",
    "A simple intuition for why using divergence might be useful, is that if two probability distributions $P$ and $Q$ are the same, the corresponding divergence will be 0. Vice versa for the case where $P\\neq Q$. Similarly, if the available samples are a good representation of the overall population and underlying distribution, the sample divergence should be close to 0 if the underyling distributions are the same and vice verca significantly different from $0$ in case of different distributions.\n",
    "\n",
    "The KL-divergence is probably the most popular one and is given (for the discrete case) as:\n",
    "\n",
    "\\begin{align*}\n",
    "KL(Q||P) = \\sum_xP(x)log(\\frac{P(x)}{Q(x)})\n",
    "\\end{align*}\n",
    "\n",
    "It is not a formal distance as it does not fulfill the triangle-equality and symmetry as well. While also lower bounded at 0, it can converge to +$infinity$. Thus, it might not be the best metric for our case.\n",
    "\n",
    "Additionally, we could consider the Jensen-Shannon Divergence. It is given for the discrete case by:\n",
    "\n",
    "\\begin{align*}\n",
    "JS(Q||P) &= \\frac{1}{2}KL(P||M) + \\frac{1}{2}KL(Q||M)\\\\\n",
    "\\text{where } M &= \\frac{1}{2}(P+M)\n",
    "\\end{align*}\n",
    "\n",
    "Compared to the KL divergence, the JS divergence is symmetric and finite. And, if we take the square root of it, it becomes a proper metric and is also sometimes known as Jennsen-Shannon distance. Thus, it might seem more appropiate and stable to use.\n",
    "\n",
    "We can begin by computing the Kullback-Leibler and Jensen-Shannon Divergence for our hiring example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drawing the samples again\n",
    "sample_1, sample_2 = hiring_data(0)\n",
    "_ , sample_3 = hiring_data(1)\n",
    "\n",
    "#Compute conditional Y=1 given X,Z,W\n",
    "p_y_xzw_s1 = sample_1.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values\n",
    "p_y_xzw_s2 = sample_2.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values\n",
    "p_y_xzw_s3 = sample_3.groupby(['X','Z','W'])['Y'].mean().reset_index()['Y'].values\n",
    "#Compute Divergences\n",
    "##KL Divergence between sample 1 and sample 2\n",
    "kl_s1_s2 = entropy(p_y_xzw_s1,p_y_xzw_s2)\n",
    "##KL Divergence between sample 1 and sample 3\n",
    "kl_s1_s3 = entropy(p_y_xzw_s1,p_y_xzw_s3)\n",
    "##JS Divergence between sample 1 and sample 2\n",
    "js_s1_s2 = distance.jensenshannon(p_y_xzw_s1,p_y_xzw_s2)\n",
    "##JS Divergence between sample 1 and sample 3\n",
    "js_s1_s3 = distance.jensenshannon(p_y_xzw_s1,p_y_xzw_s3)\n",
    "\n",
    "print(f\"Divergences between sample 1 and sample 2:\\n KL Divergence: {kl_s1_s2:.4f}\\n JS Divergence: {js_s1_s2:.4f}\\n\")\n",
    "print(f\"Divergences between sample 1 and sample 3:\\n KL Divergence: {kl_s1_s3:.4f}\\n JS Divergence: {js_s1_s3:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the beginning, when we looked at the differences in conditional $Y=1|X,Z,W$ across samples, we see that the divergences for sample 1 and sample 2 are not exactly zero even though they were drawn from the same distributions. Again, this is due to sampling variation. For sample 1 and sample 3 we see that the divergences seem larger than for the first pair of samples.\n",
    "\n",
    "We can now use a test similar to our very first bootstrap test. Using the assumption of exhangeability, we will repeatly pool our data, create bootstrap samples and compute the divergences for these. Then, we will obtain p-values by computing the fraction of bootstrapped divergences which are at least as extreme as the observed one.\n",
    "\n",
    "The test procedure will be as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{} \\quad & \\textbf{Bootstrapped divergences test} \\ \\\\\n",
    "\\text{} \\quad & \\textbf{Input:} \\ \\text{samples a and b and observational distributions} P_a(V) \\text{ and } P_b(V)\\\\\n",
    "\\text{} \\quad & \\textbf{Output:} \\ \\text{array of p-values, array of bootstrapped divergences}  \\\\\n",
    "\\text{} \\quad & \\\\\n",
    "\\text{1.} \\quad & \\text{for  all combinations of } X,Z,W \\text{ do} \\\\\n",
    "\\text{2.} \\quad & \\quad \\text{compute and store } P(Y=1|x,z,w) \\text{ for both samples}   \\\\\n",
    "\\text{3.} \\quad & \\quad \\text{compute and store  divergence } d \\text{ between both samples}   \\\\\n",
    "\\text{3.} \\quad & \\text{for } 1:n_{bootstrap} \\text{ do} \\\\\n",
    "\\text{4.} \\quad & \\quad  \\text{Create pooled sample } \\text{ from } a \\text{ and } b\\\\\n",
    "\\text{5.} \\quad & \\quad \\text{With replacement draw bootstrap samples } a^{boot},b^{boot} \\text{ from pooled sample}\\\\\n",
    "\\text{6.} \\quad & \\quad \\text{for  all combinations of } X,Z,W \\text{ do} \\\\\n",
    "\\text{7.} \\quad & \\quad \\quad \\text{compute and store } P(Y=1|x,z,w) \\text{ for } a^{boot},b^{boot}  \\\\\n",
    "\\text{7.} \\quad & \\quad \\quad \\text{compute and store } d^{boot} \\text{ between the bootstrapped samples}  \\\\\n",
    "\n",
    "\\text{8.} \\quad & \\text{for  all combinations of } X,Z,W \\text{ do} \\\\\n",
    "\\text{9.} \\quad & \\quad \\text{compute and store } p=\\frac{\\#\\{|d^{boot}| \\geq |d|\\}}{n_{bootstrap}}  \\\\\n",
    "\\end{align*} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_divergences(sample_a, sample_b, B=20000, pooling=True):\n",
    "    # Compute conditionals of Y on X, Z, W\n",
    "    p_y_xwz_s1 = sample_a.groupby(['X', 'W', 'Z'])['Y'].mean().reset_index()['Y'].values\n",
    "    p_y_xwz_s2 = sample_b.groupby(['X', 'W', 'Z'])['Y'].mean().reset_index()['Y'].values\n",
    "\n",
    "    # Compute observed divergences\n",
    "    kl = entropy(p_y_xwz_s1, p_y_xwz_s2)\n",
    "    js = distance.jensenshannon(p_y_xwz_s1, p_y_xwz_s2)\n",
    "\n",
    "    # Create arrays to store bootstrap divergences\n",
    "    kl_boot = np.zeros(B)\n",
    "    js_boot = np.zeros(B)\n",
    "\n",
    "    for i in tqdm.tqdm(range(B), desc='Bootstrapping'):\n",
    "\n",
    "        if pooling:\n",
    "            # Pool samples\n",
    "            pooled = pd.concat([sample_a, sample_b])\n",
    "            \n",
    "            # Create bootstrap samples\n",
    "            bootstrap_sample_a = pooled.sample(len(sample_a), replace=True)\n",
    "            bootstrap_sample_b = pooled.sample(len(sample_b), replace=True)\n",
    "\n",
    "        else:\n",
    "            bootstrap_sample_a = sample_a.sample(len(sample_a), replace=True)\n",
    "            bootstrap_sample_b = sample_b.sample(len(sample_b), replace=True)\n",
    "\n",
    "        # Compute conditionals of Y on X, Z, W\n",
    "        p_y_xwz_s1 = bootstrap_sample_a.groupby(['X', 'W', 'Z'])['Y'].mean().reset_index()['Y'].values\n",
    "        p_y_xwz_s2 = bootstrap_sample_b.groupby(['X', 'W', 'Z'])['Y'].mean().reset_index()['Y'].values\n",
    "        \n",
    "        # Compute divergences\n",
    "        kl_boot[i] = entropy(p_y_xwz_s1, p_y_xwz_s2) \n",
    "        js_boot[i] = distance.jensenshannon(p_y_xwz_s1, p_y_xwz_s2) \n",
    "\n",
    "    \n",
    "    #Compute a CI for the divergences\n",
    "    if not pooling:\n",
    "        lower_bound_kl = 2*kl-np.quantile(kl_boot,0.975)\n",
    "        upper_bound_kl = 2*kl-np.quantile(kl_boot,0.025)\n",
    "        lower_bound_js = 2*js-np.quantile(js_boot,0.975)\n",
    "        upper_bound_js = 2*js-np.quantile(js_boot,0.025)\n",
    "    \n",
    "        ci_kl = [lower_bound_kl, upper_bound_kl]\n",
    "        ci_js = [lower_bound_js, upper_bound_js]\n",
    "\n",
    "        return kl, js, kl_boot, js_boot, ci_kl, ci_js\n",
    "\n",
    "    else:\n",
    "        # Compute p-values: fraction of absolute bootstrap divergences that are greater or equal than the absolute observed divergence\n",
    "        p_kl = np.mean(np.abs(kl_boot) >= np.abs(kl))\n",
    "        p_js = np.mean(np.abs(js_boot) >= np.abs(js))\n",
    "        return kl, js, kl_boot, js_boot, p_kl, p_js\n",
    "    \n",
    "#kl_s1s2, js_s1s2, kl_boot_s1s2, js_boot_s1s2, p_kl_s1s2, p_js_s1s2,ci_kl_s1s2, ci_js_s1s2 = bootstrap_divergences(sample_1, sample_2)\n",
    "#kl_s1s3, js_s1s3, kl_boot_s1s3, js_boot_s1s3, p_kl_s1s3, p_js_s1s3, ci_kl_s1s3, ci_js_s1s3 = bootstrap_divergences(sample_1, sample_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of bootstrap divergences\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#need to remove inf values in kl_boot_s1s2\n",
    "kl_boot_s1s2 = kl_boot_s1s2[~np.isinf(kl_boot_s1s2)]\n",
    "axs[0].hist(kl_boot_s1s2, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0].axvline(kl_s1s2, color='red', linestyle='dashed', linewidth=2)\n",
    "axs[0].set_title('KL divergence Sample 1 vs Sample 2')\n",
    "axs[0].set_xlabel('KL divergence')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "#add pvalue \n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_kl_s1s2}')])\n",
    "\n",
    "axs[1].hist(js_boot_s1s2, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1].axvline(js_s1s2, color='red', linestyle='dashed', linewidth=2)\n",
    "axs[1].set_title('JS divergence Sample 1 vs Sample 2')\n",
    "axs[1].set_xlabel('JS divergence')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[1].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_js_s1s2}')])\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "#Histogram of bootstrap divergences\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#need to remove inf values in kl_boot_s1s3\n",
    "kl_boot_s1s3 = kl_boot_s1s3[~np.isinf(kl_boot_s1s3)]\n",
    "axs[0].hist(kl_boot_s1s3, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0].axvline(kl_s1s3, color='red', linestyle='dashed', linewidth=2)\n",
    "axs[0].set_title('KL divergence Sample 1 vs Sample 3')\n",
    "axs[0].set_xlabel('KL divergence')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_kl_s1s3}')])\n",
    "\n",
    "axs[1].hist(js_boot_s1s3, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1].axvline(js_s1s3, color='red', linestyle='dashed', linewidth=2)\n",
    "axs[1].set_title('JS divergence Sample 1 vs Sample 3')\n",
    "axs[1].set_xlabel('JS divergence')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[1].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_js_s1s3}')])\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with previous bootstrap based tests, the divergence based approach does not commit any false rejections. However, compared to the previous methods, we would falsely conclude that we cannot reject the null for the case between sample 1 and sample 3. With previous bootstrapped methods, while not able to detect shifts at the level of each individual conditional, we were able to conclude a shift from an aggregate point of view. The divergence based approach is an aggregate one and seems to lack power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see similar to the previous methods that the test struggles to detect significant shifts with $n=1,000$. A possible reason, as before, is sampling variation. We can empirically test the assumption that the sample KL/JS divergences become a better representation of the population divergences with increasing samples sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1, sample_2 = hiring_data(0,n=10000)\n",
    "_ , sample_3 = hiring_data(1,n=10000)\n",
    "kl_s1s2_10k, js_s1s2_10k, kl_boot_s1s2_10k, js_boot_s1s2_10k, p_kl_s1s2_10k, p_js_s1s2_10k,ci_kl_s1s2_10k, ci_js_s1s2_10k = bootstrap_divergences(sample_1, sample_2)\n",
    "kl_s1s3_10k, js_s1s3_10k, kl_boot_s1s3_10k, js_boot_s1s3_10k, p_kl_s1s3_10k, p_js_s1s3_10k, ci_kl_s1s3_10k, ci_js_s1s3_10k = bootstrap_divergences(sample_1, sample_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of bootstrap divergences\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 8))\n",
    "#need to remove inf values in kl_boot_s1s2\n",
    "kl_boot_s1s2_10k = kl_boot_s1s2_10k[~np.isinf(kl_boot_s1s2_10k)]\n",
    "axs[0].hist(kl_boot_s1s2_10k, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0].axvline(kl_s1s2_10k, color='red', linestyle='dashed', linewidth=2)\n",
    "axs[0].set_title('KL divergence Sample 1 vs Sample 2 with n=10,000')\n",
    "axs[0].set_xlabel('KL divergence')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_kl_s1s2_10k}')])\n",
    "\n",
    "axs[1].hist(js_boot_s1s2_10k, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1].axvline(js_s1s2_10k, color='red', linestyle='dashed', linewidth=2)\n",
    "axs[1].set_title('JS Sample 1 vs Sample 2 with n=10,000')\n",
    "axs[1].set_xlabel('JS divergence')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[1].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_js_s1s2_10k}')])\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "#Histogram of bootstrap divergences\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 8))\n",
    "#need to remove inf values in kl_boot_s1s3\n",
    "kl_boot_s1s3_10k = kl_boot_s1s3_10k[~np.isinf(kl_boot_s1s3_10k)]\n",
    "axs[0].hist(kl_boot_s1s3_10k, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0].axvline(kl_s1s3_10k, color='red', linestyle='dashed', linewidth=2)\n",
    "axs[0].set_title('KL Sample 1 vs Sample 3 with n=10,000')\n",
    "axs[0].set_xlabel('KL divergence')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_kl_s1s3_10k}')])\n",
    "\n",
    "axs[1].hist(js_boot_s1s3_10k, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1].axvline(js_s1s3_10k, color='red', linestyle='dashed', linewidth=2)\n",
    "axs[1].set_title('JS Sample 1 vs Sample 3 with n=10,000')\n",
    "axs[1].set_xlabel('JS divergence')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[1].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_js_s1s3_10k}')])\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, with a larger sample size, the samples become a better representation of their underlying distributions. Thus, the divergence based procedure correctly detects a significant shift between sample 1 and sample 3, while not rejecting the null between sample 1 and sample 2.\n",
    "\n",
    "So far, we've only looked at p-values as the basis of our decisions. We could however, also create confidence intervals to guide our decisions. These, will give us a range within the true effect might be.\n",
    "\n",
    "To this end, let's compute the CI's for the bootstrapped divergences for both the case of $n=1,000$ and $n=10,000$. Note, that in this case we will not pool the data as we are interested in a possible CI for the true underlying divergences. Bootstrapping under the null, would give us a bootstrap confidence interval for the divergences under the null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bootstrap divergences without pooling\n",
    "kl_s1s2_no_pool, js_s1s2_no_pool, kl_boot_s1s2_no_pool, js_boot_s1s2_no_pool, p_kl_s1s2_no_pool, p_js_s1s2_no_pool, ci_kl_s1s2_no_pool, ci_js_s1s2_no_pool = bootstrap_divergences(sample_1, sample_2, pooling=False)\n",
    "kl_s1s3_no_pool, js_s1s3_no_pool, kl_boot_s1s3_no_pool, js_boot_s1s3_no_pool, p_kl_s1s3_no_pool, p_js_s1s3_no_pool, ci_kl_s1s3_no_pool, ci_js_s1s3_no_pool = bootstrap_divergences(sample_1, sample_3, pooling=False)\n",
    "#sample data with n=10,000\n",
    "sample_1, sample_2 = hiring_data(0,n=10000)\n",
    "_ , sample_3 = hiring_data(1,n=10000)\n",
    "kl_s1s2_no_pool_10k, js_s1s2_no_pool_10k, kl_boot_s1s2_no_pool_10k, js_boot_s1s2_no_pool_10k, p_kl_s1s2_no_pool_10k, p_js_s1s2_no_pool_10k, ci_kl_s1s2_no_pool_10k, ci_js_s1s2_no_pool_10k = bootstrap_divergences(sample_1, sample_2, pooling=False)\n",
    "kl_s1s3_no_pool_10k, js_s1s3_no_pool_10k, kl_boot_s1s3_no_pool_10k, js_boot_s1s3_no_pool_10k, p_kl_s1s3_no_pool_10k, p_js_s1s3_no_pool_10k, ci_kl_s1s3_no_pool_10k, ci_js_s1s3_no_pool_10k = bootstrap_divergences(sample_1, sample_3, pooling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of bootstrapped divergences with confidence intervals\n",
    "fig, axs = plt.subplots(2,2 , figsize=(20, 10))\n",
    "#N=1000\n",
    "axs[0,0].hist(kl_boot_s1s2_no_pool, bins=50, alpha=0.3, color='blue', edgecolor='black', label='Bootstrap')\n",
    "axs[0,0].axvline(kl_s1s2_no_pool, color='red', linestyle='dashed', linewidth=2, label = 'Observed')\n",
    "axs[0,0].set_title('KL divergence Sample 1 vs Sample 2 (n=1,000)')\n",
    "axs[0,0].set_xlabel('KL divergence')\n",
    "axs[0,0].set_ylabel('Frequency')\n",
    "#Add CI as two lines in the histogram\n",
    "axs[0,0].axvline(0 if ci_kl_s1s2_no_pool[0] < 0 else ci_kl_s1s2_no_pool[0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[0,0].axvline(ci_kl_s1s2_no_pool[1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[0,0].legend()\n",
    "\n",
    "axs[0,1].hist(js_boot_s1s2_no_pool, bins=50, alpha=0.3, color='blue', edgecolor='black', label='Bootstrap')\n",
    "axs[0,1].axvline(js_s1s2_no_pool, color='red', linestyle='dashed', linewidth=2, label = 'Observed')\n",
    "axs[0,1].set_title('JS divergence Sample 1 vs Sample 2 (n=1,000)')\n",
    "axs[0,1].set_xlabel('JS divergence')\n",
    "axs[0,1].set_ylabel('Frequency')\n",
    "#Add CI as two lines in the histogram\n",
    "axs[0,1].axvline(0 if ci_js_s1s2_no_pool[0] < 0 else ci_js_s1s2_no_pool[0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[0,1].axvline(ci_js_s1s2_no_pool[1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[0,1].legend()\n",
    "\n",
    "#N=10,000\n",
    "axs[1,0].hist(kl_boot_s1s2_no_pool_10k, bins=50, alpha=0.3, color='blue', edgecolor='black', label='Bootstrap')\n",
    "axs[1,0].axvline(kl_s1s2_no_pool_10k, color='red', linestyle='dashed', linewidth=2, label = 'Observed')\n",
    "axs[1,0].set_title('KL divergence Sample 1 vs Sample 2 (n=10,000)')\n",
    "axs[1,0].set_xlabel('KL divergence')\n",
    "axs[1,0].set_ylabel('Frequency')\n",
    "#Add CI as two lines in the histogram\n",
    "axs[1,0].axvline(0 if ci_kl_s1s2_no_pool_10k[0] < 0 else ci_kl_s1s2_no_pool_10k[0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[1,0].axvline(ci_kl_s1s2_no_pool_10k[1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[1,0].legend()\n",
    "\n",
    "axs[1,1].hist(js_boot_s1s2_no_pool_10k, bins=50, alpha=0.3, color='blue', edgecolor='black', label='Bootstrap')\n",
    "axs[1,1].axvline(js_s1s2_no_pool_10k, color='red', linestyle='dashed', linewidth=2, label = 'Observed')\n",
    "axs[1,1].set_title('JS divergence Sample 1 vs Sample 2 (n=10,000)')\n",
    "axs[1,1].set_xlabel('JS divergence')\n",
    "axs[1,1].set_ylabel('Frequency')\n",
    "#Add CI as two lines in the histogram\n",
    "axs[1,1].axvline(0 if ci_js_s1s2_no_pool_10k[0] < 0 else ci_js_s1s2_no_pool_10k[0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[1,1].axvline(ci_js_s1s2_no_pool_10k[1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[1,1].legend()\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "#Histogram of bootstrapped divergences with confidence intervals\n",
    "fig, axs = plt.subplots(2,2 , figsize=(20, 10))\n",
    "#N=1000\n",
    "axs[0,0].hist(kl_boot_s1s3_no_pool, bins=50, alpha=0.3, color='blue', edgecolor='black', label='Bootstrap')\n",
    "axs[0,0].axvline(kl_s1s3_no_pool, color='red', linestyle='dashed', linewidth=2, label = 'Observed')\n",
    "axs[0,0].set_title('KL divergence Sample 1 vs Sample 3 (n=1,000)')\n",
    "axs[0,0].set_xlabel('KL divergence')\n",
    "axs[0,0].set_ylabel('Frequency')\n",
    "#Add CI as two lines in the histogram\n",
    "axs[0,0].axvline(0 if ci_kl_s1s3_no_pool[0] < 0 else ci_kl_s1s3_no_pool[0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[0,0].axvline(ci_kl_s1s3_no_pool[1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[0,0].legend()\n",
    "\n",
    "axs[0,1].hist(js_boot_s1s3_no_pool, bins=50, alpha=0.3, color='blue', edgecolor='black', label='Bootstrap')\n",
    "axs[0,1].axvline(js_s1s3_no_pool, color='red', linestyle='dashed', linewidth=2, label = 'Observed')\n",
    "axs[0,1].set_title('JS divergence Sample 1 vs Sample 3 (n=1,000)')\n",
    "axs[0,1].set_xlabel('JS divergence')\n",
    "axs[0,1].set_ylabel('Frequency')\n",
    "\n",
    "#Add CI as two lines in the histogram\n",
    "axs[0,1].axvline(0 if ci_js_s1s3_no_pool[0] < 0 else ci_js_s1s3_no_pool[0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[0,1].axvline(ci_js_s1s3_no_pool[1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[0,1].legend()\n",
    "\n",
    "#N=10,000\n",
    "axs[1,0].hist(kl_boot_s1s3_no_pool_10k, bins=50, alpha=0.3, color='blue', edgecolor='black', label='Bootstrap')\n",
    "axs[1,0].axvline(kl_s1s3_no_pool_10k, color='red', linestyle='dashed', linewidth=2, label = 'Observed')\n",
    "axs[1,0].set_title('KL divergence Sample 1 vs Sample 3 (n=10,000)')\n",
    "axs[1,0].set_xlabel('KL divergence')\n",
    "axs[1,0].set_ylabel('Frequency')\n",
    "#Add CI as two lines in the histogram\n",
    "axs[1,0].axvline(0 if ci_kl_s1s3_no_pool_10k[0] < 0 else ci_kl_s1s3_no_pool_10k[0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[1,0].axvline(ci_kl_s1s3_no_pool_10k[1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[1,0].legend()\n",
    "\n",
    "axs[1,1].hist(js_boot_s1s3_no_pool_10k, bins=50, alpha=0.3, color='blue', edgecolor='black', label='Bootstrap')\n",
    "axs[1,1].axvline(js_s1s3_no_pool_10k, color='red', linestyle='dashed', linewidth=2, label = 'Observed')\n",
    "axs[1,1].set_title('JS divergence Sample 1 vs Sample 3 (n=10,000)')\n",
    "axs[1,1].set_xlabel('JS divergence')\n",
    "axs[1,1].set_ylabel('Frequency')\n",
    "#Add CI as two lines in the histogram\n",
    "axs[1,1].axvline(0 if ci_js_s1s3_no_pool_10k[0] < 0 else ci_js_s1s3_no_pool_10k[0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[1,1].axvline(ci_js_s1s3_no_pool_10k[1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[1,1].legend()\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidence interval give a interesting picture. In the case of no distribution shifts, the CIs for both divergences and sample sizes contain the 0 and the observed divergences. Thus, we would not reject the null of no differences.\n",
    "\n",
    "For the case of shifts, the CIs for both divergences and sample sizes contain the observed divergences but do not contain 0. Thus, using the CIs as our heuristic, we could reject the null of no differences and would conclude that sample 1 and sample 3 come from different distributions. \n",
    "\n",
    "To close the chapter on testing for distribution shifts, let's run the divergence based approach for a more pronounced shift (i.e. $d=0.5$), smaller shifts (i.e.$d=[0.01,...,0.08]$) and the different type of shifts (i.e. only shifts in W,...) as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "#Sample the corresponding datasets\n",
    "_ , sample_3_d_05 = hiring_data(1, d=0.5)\n",
    "####d = 0.5##################################################################################\n",
    "#Compute the bootstrap divergence test for sample 1 vs sample 3_d_05 (with pooling)\n",
    "kl_s1s3_d_05, js_s1s3_d_05, kl_boot_s1s3_d_05, js_boot_s1s3_d_05, p_kl_s1s3_d_05, p_js_s1s3_d_05  = bootstrap_divergences(sample_1, sample_3_d_05)\n",
    "#Compute the bootstrap divergence test for sample 1 vs sample 3_d_05 (with pooling, for CIs)\n",
    "kl_s1s3_d_05_no_pool, js_s1s3_d_05_no_pool, kl_boot_s1s3_d_05_no_pool, js_boot_s1s3_d_05_no_pool, ci_kl_s1s3_d_05, ci_js_s1s3_d_05 = bootstrap_divergences(sample_1, sample_3_d_05, pooling=False)\n",
    "##############################################################################################\n",
    "#Sample datasets with different d values [0.01,...0,.08]\n",
    "d_values = [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08]\n",
    "#empty arrays to store the divergences and p-values\n",
    "B = 20000\n",
    "kl_s1s3_d = np.empty(len(d_values))\n",
    "js_s1s3_d = np.empty(len(d_values))\n",
    "kl_s1s3_d_no_pool = np.empty(len(d_values))\n",
    "js_s1s3_d_no_pool = np.empty(len(d_values))\n",
    "kl_boot_s1s3_d = np.empty((len(d_values),B))\n",
    "js_boot_s1s3_d = np.empty((len(d_values),B))\n",
    "p_kl_s1s3_d = np.empty(len(d_values))\n",
    "p_js_s1s3_d = np.empty(len(d_values))\n",
    "kl_boot_s1s3_no_pool_d = np.empty((len(d_values),B))\n",
    "js_boot_s1s3_no_pool_d = np.empty((len(d_values),B))\n",
    "ci_kl_s1s3_d = np.empty((len(d_values),2))\n",
    "ci_js_s1s3_d = np.empty((len(d_values),2))\n",
    "\n",
    "for i in range(len(d_values)):\n",
    "    _ , sample_3_d = hiring_data(1, d=d_values[i])\n",
    "    kl_s1s3_d[i], js_s1s3_d[i], kl_boot_s1s3_d[i], js_boot_s1s3_d[i], p_kl_s1s3_d[i], p_js_s1s3_d[i] = bootstrap_divergences(sample_1, sample_3_d)\n",
    "    kl_s1s3_d_no_pool[i], js_s1s3_d_no_pool[i], kl_boot_s1s3_no_pool_d[i], js_boot_s1s3_no_pool_d[i], ci_kl_s1s3_d[i], ci_js_s1s3_d[i] = bootstrap_divergences(sample_1, sample_3_d, pooling=False)\n",
    "'''\n",
    "\n",
    "#Use the different shiftsfrom 1.4 \n",
    "B=20000\n",
    "sample_3_shifts = [sample_3_shift_only_w,sample_3_shift_w_and_y,sample_3_shift_y_not_depending_on_w,sample_3_shift_y_constant]\n",
    "bootstrapped_t_stats_different_sample_3 = np.empty((len(sample_3_shifts),B))\n",
    "kl_boot_s1s3_shifts = np.empty((len(sample_3_shifts),B))\n",
    "js_boot_s1s3_shifts = np.empty((len(sample_3_shifts),B))\n",
    "kl_boot_s1s3_no_pool_shifts = np.empty((len(sample_3_shifts),B))\n",
    "js_boot_s1s3_no_pool_shifts = np.empty((len(sample_3_shifts),B))\n",
    "kl_s1s3_shifts = np.empty(len(sample_3_shifts))\n",
    "js_s1s3_shifts = np.empty(len(sample_3_shifts))\n",
    "kl_s1s3_shifts_no_pool = np.empty(len(sample_3_shifts))\n",
    "js_s1s3_shifts_no_pool = np.empty(len(sample_3_shifts))\n",
    "p_kl_s1s3_shifts = np.empty(len(sample_3_shifts))\n",
    "p_js_s1s3_shifts = np.empty(len(sample_3_shifts))\n",
    "ci_kl_s1s3_shifts = np.empty((len(sample_3_shifts),2))\n",
    "ci_js_s1s3_shifts = np.empty((len(sample_3_shifts),2))\n",
    "\n",
    "for i in range(len(sample_3_shifts)):\n",
    "    #Compute the bootstrap divergence test for sample 1 vs sample 3_shifts[i] (with pooling)\n",
    "    kl_s1s3_shifts[i], js_s1s3_shifts[i], kl_boot_s1s3_shifts[i], js_boot_s1s3_shifts[i], p_kl_s1s3_shifts[i], p_js_s1s3_shifts[i] = bootstrap_divergences(sample_1, sample_3_shifts[i])\n",
    "    #Compute the bootstrap divergence test for sample 1 vs sample 3_shifts[i] (without pooling, for CIs)\n",
    "    kl_s1s3_shifts_no_pool[i], js_s1s3_shifts_no_pool[i], kl_boot_s1s3_no_pool_shifts[i], js_boot_s1s3_no_pool_shifts[i], ci_kl_s1s3_shifts[i], ci_js_s1s3_shifts[i] = bootstrap_divergences(sample_1, sample_3_shifts[i], pooling=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first look at the results for $d = 0.5$. With all previous methods it was able to detect shifts of that magnitude. Thus, we would expect to see the same here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##For the bootstrap test with pooling:\n",
    "##Histogram with bootstrapped divergences and p-value\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "#need to remove inf values in kl_boot_s1s3_d_05\n",
    "kl_boot_s1s3_d_05 = kl_boot_s1s3_d_05[~np.isinf(kl_boot_s1s3_d_05)]\n",
    "axs[0].hist(kl_boot_s1s3_d_05, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0].axvline(kl_s1s3_d_05, color='red', linestyle='dashed', linewidth=2)\n",
    "axs[0].set_title('KL Sample 1 vs Sample 3 with d=0.5')\n",
    "axs[0].set_xlabel('KL divergence')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_kl_s1s3_d_05}')])\n",
    "\n",
    "axs[1].hist(js_boot_s1s3_d_05, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1].axvline(js_s1s3_d_05, color='red', linestyle='dashed', linewidth=2)\n",
    "axs[1].set_title('JS Sample 1 vs Sample 3 with d=0.5')\n",
    "axs[1].set_xlabel('JS divergence')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[1].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_js_s1s3_d_05}')])\n",
    "plt.show()  \n",
    "plt.tight_layout()\n",
    "\n",
    "##For the bootstrap test without pooling:\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "#need to remove inf values in kl_boot_s1s3_d_05_no_pool\n",
    "kl_boot_s1s3_d_05_no_pool = kl_boot_s1s3_d_05_no_pool[~np.isinf(kl_boot_s1s3_d_05_no_pool)]\n",
    "axs[0].hist(kl_boot_s1s3_d_05_no_pool, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0].axvline(kl_s1s3_d_05_no_pool, color='red', linestyle='dashed', linewidth=2)\n",
    "axs[0].set_title('KL Sample 1 vs Sample 3 with d=0.5 (no pooling)')\n",
    "axs[0].set_xlabel('KL divergence')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "#add CI as vertical lines\n",
    "axs[0].axvline(0 if ci_kl_s1s3_d_05[0] < 0 else ci_kl_s1s3_d_05[0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[0].axvline(ci_kl_s1s3_d_05[1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].hist(js_boot_s1s3_d_05_no_pool, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1].axvline(js_s1s3_d_05_no_pool, color='red', linestyle='dashed', linewidth=2)\n",
    "axs[1].set_title('JS Sample 1 vs Sample 3 with d=0.5 (no pooling)')\n",
    "axs[1].set_xlabel('JS divergence')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "#add CI as vertical lines\n",
    "axs[1].axvline(0 if ci_js_s1s3_d_05[0] < 0 else ci_js_s1s3_d_05[0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "#Add lower bound of CI as text \n",
    "axs[1].axvline(ci_js_s1s3_d_05[1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[1].legend()\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with a stronger shift, that using both the bootstrapped test and CI-heuristic, we would arrive at the conclusion of a significant shift between the samples. The p-values for bootstrapping under the null are significant and the CIs for the bootstrapped sample divergences do not contain the 0. \n",
    "\n",
    "Next, let's inspect the \"smaller\" shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the divergences for the different shifts\n",
    "#For no pooling:\n",
    "##Start with KL divergence\n",
    "fig, axs = plt.subplots(2,4, figsize=(20, 10))\n",
    "\n",
    "for i in range(len(d_values)):\n",
    "\n",
    "    #need to remove inf values in kl_boot_s1s3_d[i]\n",
    "    filtered_kl_boot = kl_boot_s1s3_d[i][~np.isinf(kl_boot_s1s3_d[i])]\n",
    "    axs[i//4,i%4].hist(filtered_kl_boot, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "    axs[i//4,i%4].axvline(kl_s1s3_d[i], color='red', linestyle='dashed', linewidth=2)\n",
    "    axs[i//4,i%4].set_title(f'KL Sample 1 vs Sample 3 with d={d_values[i]}')\n",
    "    axs[i//4,i%4].set_xlabel('KL divergence')\n",
    "    axs[i//4,i%4].set_ylabel('Frequency')\n",
    "    #Add p-value\n",
    "    handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "    #legend with smaller font size\n",
    "    axs[i//4,i%4].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_kl_s1s3_d[i]}')])\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "##JS divergence\n",
    "fig, axs = plt.subplots(2,4, figsize=(20, 10))\n",
    "\n",
    "for i in range(len(d_values)):\n",
    "    axs[i//4,i%4].hist(js_boot_s1s3_d[i], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "    axs[i//4,i%4].axvline(js_s1s3_d[i], color='red', linestyle='dashed', linewidth=2)\n",
    "    axs[i//4,i%4].set_title(f'JS Sample 1 vs Sample 3 with d={d_values[i]}')\n",
    "    axs[i//4,i%4].set_xlabel('JS divergence')\n",
    "    axs[i//4,i%4].set_ylabel('Frequency')\n",
    "    #Add p-value\n",
    "    handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "    #legend with smaller font size\n",
    "    axs[i//4,i%4].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_js_s1s3_d[i]}')])\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "#For without pooling:\n",
    "##Start with KL divergence\n",
    "fig, axs = plt.subplots(2,4, figsize=(20, 10))\n",
    "\n",
    "for i in range(len(d_values)):\n",
    "\n",
    "    #need to remove inf values in kl_boot_s1s3_d[i]\n",
    "    filtered_kl_boot = kl_boot_s1s3_no_pool_d[i][~np.isinf(kl_boot_s1s3_no_pool_d[i])]\n",
    "    axs[i//4,i%4].hist(filtered_kl_boot, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "    axs[i//4,i%4].axvline(kl_s1s3_d_no_pool[i], color='red', linestyle='dashed', linewidth=2)\n",
    "    axs[i//4,i%4].set_title(f'KL Sample 1 vs Sample 3 with d={d_values[i]}')\n",
    "    axs[i//4,i%4].set_xlabel('KL divergence')\n",
    "    axs[i//4,i%4].set_ylabel('Frequency')\n",
    "    #Add CI as vertical lines\n",
    "    axs[i//4,i%4].axvline(0 if ci_kl_s1s3_d[i][0] < 0 else ci_kl_s1s3_d[i][0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "    axs[i//4,i%4].axvline(ci_kl_s1s3_d[i][1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "    axs[i//4,i%4].legend()\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "##JS divergence\n",
    "fig, axs = plt.subplots(2,4, figsize=(20, 10))\n",
    "\n",
    "for i in range(len(d_values)):\n",
    "\n",
    "    axs[i//4,i%4].hist(js_boot_s1s3_no_pool_d[i], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "    axs[i//4,i%4].axvline(js_s1s3_d_no_pool[i], color='red', linestyle='dashed', linewidth=2)\n",
    "    axs[i//4,i%4].set_title(f'JS Sample 1 vs Sample 3 with d={d_values[i]}')\n",
    "    axs[i//4,i%4].set_xlabel('JS divergence')\n",
    "    axs[i//4,i%4].set_ylabel('Frequency')\n",
    "    #Add CI as vertical lines\n",
    "    axs[i//4,i%4].axvline(0 if ci_js_s1s3_d[i][0] < 0 else ci_js_s1s3_d[i][0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "    axs[i//4,i%4].axvline(ci_js_s1s3_d[i][1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "    axs[i//4,i%4].legend()  \n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we follow the procedure of bootstrapping under the null no shift was detected to be significant between both samples for both divergences. However, if we instead bootstrap the sample divergences to compute a 95% CI, we see that from $d=0.05$ on, the CIs do not contain $0$ anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's inspect the shifts of different structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the divergences for the different shifts\n",
    "#Shift only in W\n",
    "##For pooling:\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "#need to remove inf values in kl_boot_s1s3_shifts[0]\n",
    "filtered = kl_boot_s1s3_shifts[0][~np.isinf(kl_boot_s1s3_shifts[0])]\n",
    "axs[0,0].hist(filtered, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0,0].axvline(kl_s1s3_shifts[0], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[0,0].set_title('KL Sample 1 vs Sample 3 with shift only in W')\n",
    "axs[0,0].set_xlabel('KL divergence')\n",
    "axs[0,0].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[0,0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_kl_s1s3_shifts[0]}')])\n",
    "#JS divergence\n",
    "axs[0,1].hist(js_boot_s1s3_shifts[0], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0,1].axvline(js_s1s3_shifts[0], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[0,1].set_title('JS Sample 1 vs Sample 3 with shift only in W')\n",
    "axs[0,1].set_xlabel('JS divergence')\n",
    "axs[0,1].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[0,1].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_js_s1s3_shifts[0]}')])\n",
    "\n",
    "#For no pooling:\n",
    "filtered = kl_boot_s1s3_no_pool_shifts[0][~np.isinf(kl_boot_s1s3_no_pool_shifts[0])]\n",
    "axs[1,0].hist(filtered, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1,0].axvline(kl_s1s3_shifts_no_pool[0], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[1,0].set_title('KL Sample 1 vs Sample 3 with shift only in W (no pooling)')\n",
    "axs[1,0].set_xlabel('KL divergence')\n",
    "axs[1,0].set_ylabel('Frequency')\n",
    "#add CI as vertical lines\n",
    "axs[1,0].axvline(0 if ci_kl_s1s3_shifts[0][0] < 0 else ci_kl_s1s3_shifts[0][0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[1,0].axvline(ci_kl_s1s3_shifts[0][1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[1,0].legend()\n",
    "\n",
    "axs[1,1].hist(js_boot_s1s3_no_pool_shifts[0], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1,1].axvline(js_s1s3_shifts_no_pool[0], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[1,1].set_title('JS Sample 1 vs Sample 3 with shift only in W (no pooling)')\n",
    "axs[1,1].set_xlabel('JS divergence')\n",
    "axs[1,1].set_ylabel('Frequency')\n",
    "#add CI as vertical lines\n",
    "axs[1,1].axvline(0 if ci_js_s1s3_shifts[0][0] < 0 else ci_js_s1s3_shifts[0][0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[1,1].axvline(ci_js_s1s3_shifts[0][1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[1,1].legend()\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "##Shift in W and Y\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "#need to remove inf values in kl_boot_s1s3_shifts[1]\n",
    "filtered = kl_boot_s1s3_shifts[1][~np.isinf(kl_boot_s1s3_shifts[1])]\n",
    "axs[0,0].hist(filtered, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0,0].axvline(kl_s1s3_shifts[1], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[0,0].set_title('KL Sample 1 vs Sample 3 with shift in W and Y')\n",
    "axs[0,0].set_xlabel('KL divergence')\n",
    "axs[0,0].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[0,0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_kl_s1s3_shifts[1]}')])\n",
    "#JS divergence\n",
    "axs[0,1].hist(js_boot_s1s3_shifts[1], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0,1].axvline(js_s1s3_shifts[1], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[0,1].set_title('JS Sample 1 vs Sample 3 with shift in W and Y')\n",
    "axs[0,1].set_xlabel('JS divergence')\n",
    "axs[0,1].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[0,1].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_js_s1s3_shifts[1]}')])\n",
    "#For no pooling:\n",
    "filtered = kl_boot_s1s3_no_pool_shifts[1][~np.isinf(kl_boot_s1s3_no_pool_shifts[1])]\n",
    "axs[1,0].hist(filtered, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1,0].axvline(kl_s1s3_shifts_no_pool[1], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[1,0].set_title('KL Sample 1 vs Sample 3 with shift in W and Y (no pooling)')\n",
    "axs[1,0].set_xlabel('KL divergence')\n",
    "axs[1,0].set_ylabel('Frequency')\n",
    "#add CI as vertical lines\n",
    "axs[1,0].axvline(0 if ci_kl_s1s3_shifts[1][0] < 0 else ci_kl_s1s3_shifts[1][0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[1,0].axvline(ci_kl_s1s3_shifts[1][1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[1,0].legend()\n",
    "\n",
    "axs[1,1].hist(js_boot_s1s3_no_pool_shifts[1], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1,1].axvline(js_s1s3_shifts_no_pool[1], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[1,1].set_title('JS Sample 1 vs Sample 3 with shift in W and Y (no pooling)')\n",
    "axs[1,1].set_xlabel('JS divergence')\n",
    "axs[1,1].set_ylabel('Frequency')\n",
    "#add CI as vertical lines\n",
    "axs[1,1].axvline(0 if ci_js_s1s3_shifts[1][0] < 0 else ci_js_s1s3_shifts[1][0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[1,1].axvline(ci_js_s1s3_shifts[1][1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[1,1].legend()\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "##Shift in Y not depending on W\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "#need to remove inf values in kl_boot_s1s3_shifts[2]\n",
    "filtered = kl_boot_s1s3_shifts[2][~np.isinf(kl_boot_s1s3_shifts[2])]\n",
    "axs[0,0].hist(filtered, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0,0].axvline(kl_s1s3_shifts[2], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[0,0].set_title('KL Sample 1 vs Sample 3 with shift in Y not depending on W')\n",
    "axs[0,0].set_xlabel('KL divergence')\n",
    "axs[0,0].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[0,0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_kl_s1s3_shifts[2]}')])\n",
    "#JS divergence\n",
    "axs[0,1].hist(js_boot_s1s3_shifts[2], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0,1].axvline(js_s1s3_shifts[2], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[0,1].set_title('JS Sample 1 vs Sample 3 with shift in Y not depending on W')\n",
    "axs[0,1].set_xlabel('JS divergence')\n",
    "axs[0,1].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[0,1].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_js_s1s3_shifts[2]}')])\n",
    "#For no pooling:\n",
    "filtered = kl_boot_s1s3_no_pool_shifts[2][~np.isinf(kl_boot_s1s3_no_pool_shifts[2])]\n",
    "axs[1,0].hist(filtered, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1,0].axvline(kl_s1s3_shifts_no_pool[2], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[1,0].set_title('KL Sample 1 vs Sample 3 with shift in Y not depending on W (no pooling)')\n",
    "axs[1,0].set_xlabel('KL divergence')\n",
    "axs[1,0].set_ylabel('Frequency')\n",
    "#add CI as vertical lines\n",
    "axs[1,0].axvline(0 if ci_kl_s1s3_shifts[2][0] < 0 else ci_kl_s1s3_shifts[2][0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[1,0].axvline(ci_kl_s1s3_shifts[2][1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[1,0].legend()\n",
    "\n",
    "axs[1,1].hist(js_boot_s1s3_no_pool_shifts[2], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1,1].axvline(js_s1s3_shifts_no_pool[2], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[1,1].set_title('JS Sample 1 vs Sample 3 with shift in Y not depending on W (no pooling)')\n",
    "axs[1,1].set_xlabel('JS divergence')\n",
    "axs[1,1].set_ylabel('Frequency')\n",
    "#add CI as vertical lines\n",
    "axs[1,1].axvline(0 if ci_js_s1s3_shifts[2][0] < 0 else ci_js_s1s3_shifts[2][0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[1,1].axvline(ci_js_s1s3_shifts[2][1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[1,1].legend()\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "##Shift in Y constant\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "#need to remove inf values in kl_boot_s1s3_shifts[3]\n",
    "filtered = kl_boot_s1s3_shifts[3][~np.isinf(kl_boot_s1s3_shifts[3])]\n",
    "axs[0,0].hist(filtered, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0,0].axvline(kl_s1s3_shifts[3], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[0,0].set_title('KL Sample 1 vs Sample 3 with shift in Y constant')\n",
    "axs[0,0].set_xlabel('KL divergence')\n",
    "axs[0,0].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[0,0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_kl_s1s3_shifts[3]}')])\n",
    "#JS divergence\n",
    "\n",
    "axs[0,1].hist(js_boot_s1s3_shifts[3], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0,1].axvline(js_s1s3_shifts[3], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[0,1].set_title('JS Sample 1 vs Sample 3 with shift in Y constant')\n",
    "axs[0,1].set_xlabel('JS divergence')\n",
    "axs[0,1].set_ylabel('Frequency')\n",
    "#add pvalue\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Observed')]\n",
    "axs[0,1].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {p_js_s1s3_shifts[3]}')])\n",
    "#For no pooling:\n",
    "filtered = kl_boot_s1s3_no_pool_shifts[3][~np.isinf(kl_boot_s1s3_no_pool_shifts[3])]\n",
    "axs[1,0].hist(filtered, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1,0].axvline(kl_s1s3_shifts_no_pool[3], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[1,0].set_title('KL Sample 1 vs Sample 3 with shift in Y constant (no pooling)')\n",
    "axs[1,0].set_xlabel('KL divergence')\n",
    "axs[1,0].set_ylabel('Frequency')\n",
    "#add CI as vertical lines\n",
    "axs[1,0].axvline(0 if ci_kl_s1s3_shifts[3][0] < 0 else ci_kl_s1s3_shifts[3][0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[1,0].axvline(ci_kl_s1s3_shifts[3][1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[1,0].legend()\n",
    "\n",
    "axs[1,1].hist(js_boot_s1s3_no_pool_shifts[3], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1,1].axvline(js_s1s3_shifts_no_pool[3], color='red', linestyle='dashed', linewidth=2)\n",
    "axs[1,1].set_title('JS Sample 1 vs Sample 3 with shift in Y constant (no pooling)')\n",
    "axs[1,1].set_xlabel('JS divergence')\n",
    "axs[1,1].set_ylabel('Frequency')\n",
    "#add CI as vertical lines\n",
    "axs[1,1].axvline(0 if ci_js_s1s3_shifts[3][0] < 0 else ci_js_s1s3_shifts[3][0], color='green', linestyle='dashed', linewidth=2, label = 'CI lower bound')\n",
    "axs[1,1].axvline(ci_js_s1s3_shifts[3][1], color='green', linestyle='dashed', linewidth=2, label = 'CI upper bound')\n",
    "axs[1,1].legend()\n",
    "plt.show()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through the different shifts one-by-one.  \n",
    "\n",
    "For only a shift in $W$, the test correctly indicates no significant differences between samples. Both heuristics (p-value/CIs) support that. \n",
    "\n",
    "For shifts in both $W $and $Y$, only the JS based approach detects a significant shifts both using p-values and CIs. The KL-based approach struggles to do so.\n",
    "\n",
    "When changing the parents of $Y$ to not contain $W$ anymore, only the JS based approach gives a consistent answer. It correctly detects the shift between samples. For the KL-Divergence we see the pitfalls of using it for testing. As mentioned, it is nout upper-bounded and can get infinitely high values. This is exactly what happens when computed for the CI-based approach. Before creating the histograms, the $\\inf$ values get removed. The resulting histograms then looks erratic and not informative.  \n",
    "\n",
    "When shifting $Y$ to be constant, we see that both divergences and their respective approaches pick up on the shift between samples.\n",
    "\n",
    "To close the quick iteration on divergence-based approaches, it is important to note that one should use it more as a finite sample heuristic or rule-of-thumb, instead of a fully-fledged approach for detecting shifts. The intuition is, that it is a statistic which can be computed reasonably well from data and, if two distributions agree, it should be 0 and different from 0 for different distributions. Thus, one could use this to argue that if samples are a good representation of their distributions, then the corresponding sample divergence should be close to zero as well.\n",
    "\n",
    "When following this approach, it is probably better to not use the KL-Divergence and opt for the JS-Divergence instead. As laid out and could be seen, the KL divergence has properties which might cause some issues such as non-symmetry and taking infinitely high values. Overall, the JS-Divergence gave a more consistent picture. As before, the larger the samples the better does the heuristic perform. \n",
    "\n",
    "Finally, one also has to be cautious with respect to bootstrapping under the null. In the very beginning we were able to center the bootstrap distribution around $0$ by pooling the data. This is not really possible in the scenario where we bootstrap KL/JS divergences, due to their lower bound at $0$. Thus, the resulting bootstrap distributions are mostly skewed and not centered around 0 which hurts the credibility of the bootstrapping under the null for divergence based testing. \n",
    "\n",
    "However, estimating a CI for the divergences using bootstrapped sample divergences generally worked quite well and seems to be the advised method when wanting to quickly check whether there is evidence of distribution shifts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Summary\n",
    "\n",
    "Two observations were very prevalent in our review of different methods to test for distribution shifts.\n",
    "\n",
    "1. **Sample size matters**:  \n",
    " We started out with sample sizes of $n=1,000$. For this size, that there was no test which performed without any issues. For instance, when investigating each individual conditional in our example, we could see that while working on aggregate no test was able to detect all significant shifts. The same somewhat erratic behaviour continues for the aggregate tests we tried. For instance, for the method proposed by Schrouff (2022), we would see somewhat unexplainable behaviour depending on the split used between weighting and testing set. The divergences were not conclusive enough to reject the null of no shift in case of shifts. As a silver-lining, no tests commited type I errors. Thus, even with smaller samples, it seems that we will only conclue that there is a shift if there really is one. Also, we started with an initial shift of $d=0.09$, which might be considered low to moderate in terms of magnitude. The more stronger a shift, the more the tests were able to pick up on that. Additionally, with increasing sample sizes, the problems vanished.\n",
    "\n",
    "2. **Simple Methods work**:   \n",
    "In terms of methods reviewed, here we picked 5 different ones to present. Starting with a rather simple approach via bootstrapping, continuing with Chi-squared and Z-tests before coming to more unconvential approaches as laid out by Schrouff et al or rely on divergence based heuristics, all methods worked up to a certain degree.  That is, there was no method that was not able to pick up on shifts. With sample size $n->\\inf$, one could even argue that our review indicated that there are no substantial differences in methods. This brings us to the main point, which is that the method is only as good as the sample is a good representation of the overall population mechanisms.  \n",
    "There are still some differences in methods though and for detecting smaller shifts our simulations (some might be in the appendix) showed that testing weighted outcomes as laid out in Schrouff (2022), seemed to work quite well with finite data and more subtle shifts.\n",
    "\n",
    "Another point to add is the following,\n",
    "\n",
    "3. **No one size fits all**: \n",
    "While it is possible to test and detect distribution shifts, there is no one single method which one can apply to each sort of problems. It is highly task dependent. For instance, in our example we were working with discrete data. Thus, methods such as K-S testing were not directly applicable. However, it allowed us to directly work and compute empirical probabilities where in continous cases, we would need to estimate densities. Hence, in these types of problems, the practicioner needs to choose methods which are suitable for their respective needs. Later, we will use the knowledge gained in this section to test for distribution shifts and equality within real-life US census data and see that we will need to adapt our approach to that.   \n",
    "\n",
    "Now, we will shift our attention to ask the opposite question. When can we conclude two samples to come from the same distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Testing for equality (No Distribution Shifts)\n",
    "\n",
    "The previous section spent a lot of time exploring methods to detect distribution shifts between samples. This is very helpful, specifically from a \"Fairness\" POV as this tells us why fairness transfers might fail when applying statistical models over different environments. However, it has a very large caveat. It does not tell us when we can assume that distributions likely do not differ between samples. In the previous section we had the example of hiring data from two locations (a and b). Knowing the ground-truth, we rightfully did not see any significant shifts between the two samples drawn from the SCM for location a. Does the non-rejections mean that we can assume that $P(Y|X,W,Z)$ is equal between samples?\n",
    "\n",
    "No. Take the example of smaller shifts or some individual conditionals that we've seen before. Even though, the samples were drawn from different SCMs, we did not see rejections of the null. Assuming both samples are drawn from the same distribution would be wrong.\n",
    "\n",
    "Thus, using only observational data, we cannot take two conditional distributions to be the same across samples in case of non-rejections. Having established that we need to expand on testing for distribution shifts to establish some sort of equality, one can realize that this topic does not experience a lot of attention.\n",
    "\n",
    "One reason could be simply the fact that it's statistically not possible to show that a statistic is exactly zero. Taken this as a given fact, it explains why the incentives to conduct research in that direction does not seem senseful.\n",
    "\n",
    "However, there is a a field in science that is particularly interested in questions of equality. That is, clinical/pharmaceutical trials which established a subfield in hypothesis testing called Equivlance Testing.  The motivation can be illustrated with a short example. Assume a newly developed drug A just entered the market. As is often the case, competitors might be interested in creating generikas which carry the same substances/effects. The generika will only be brought to the market if it has the same effect as the original drug. Thus, the researchers need to show that the generika has the same effects as the original.\n",
    "\n",
    "## 2.1 Two-one-sided tests (TOST)\n",
    "\n",
    "At first, researchers performed so called non-inferiority trials, where the goal was to establish that the generika's effect is at minimum almost so effective as the original defined by some inferiority margin $-\\Delta$. In more recent times, so called two-one-sided tests (TOST) gained more attention. This method first gathered increased attention through the work of [Schuirmann (1987)](https://link.springer.com/article/10.1007/bf01068419#Abs1). The idea is to specify a lower and upper equivalence bound based on the smallest effect size of interest. For instance, instead of directly testing for a difference to be $0$, the problem gets relaxed to testing for a difference to be in the interval $[-\\Delta=-0.05;+\\Delta=+0.05]$. The equivalence bounds need to tailored to the task at hand. The rationale is to choose a bound small enough such that rejecting the null concludes that any effect is likely too small to be deemed worthwile. This provides a feasible alternative to the 'impossible' null of an effect being exactly 0.\n",
    "\n",
    "In our case, we would be interested in testing the composite hypothesis:\n",
    "\\begin{align*}\n",
    "H_{0,1}: P_a(Y|X,W,Z) - P_a(Y|X,W,Z) \\leq -\\Delta \\quad &\\text{vs} \\quad H_{1,1}: P_a(Y|X,W,Z) - P_a(Y|X,W,Z) > -\\Delta  \\\\\n",
    "H_{0,2}: P_a(Y|X,W,Z) - P_a(Y|X,W,Z) \\geq +\\Delta \\quad &\\text{vs} \\quad H_{1,2}: P_a(Y|X,W,Z) - P_a(Y|X,W,Z) < +\\Delta  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Rejecting both nulls, we would conclude the difference in the conditional of $Y$ is likely to be within $[-\\Delta;+\\Delta]$. The motivation behind choosing the bounds in clinal trials is to test whether any differences are likely not large enough to have relevant repercussions for patients. However, in this case any established equivalence bounds will have implications for working on fairness within the Standard-Fairness-Model framework. \n",
    "\n",
    "For instance, if we want to estimate causal fairness measures in a source domain and investigate whether we can apply it to some target domain without reestimating some parts. The previous section has outlined various methods which tell us what we need to re-estimate. This section is describing to us, which parts might be very similar across environments. If we are able to show a very tight significant interval (i.e.$+ and -\\Delta = 0.01$), we might get away with assuming any possible non-zero effect to be somewhat irrelevant. However, if the interval is wider, for instance $[-\\Delta=-0.05;+\\Delta=+0.05]$, any non-zero difference is likely to have a relevant influence on the causal fairness measures. The silver-lining, in any cases it will bound differences between environments at some significane level $\\alpha$, which we can use for further analysis. \n",
    "\n",
    "A very high-level algorithm can be described by:\n",
    "\\begin{align*}\n",
    "\\text{} \\quad & \\textbf{Two-one-sided-tests} \\ \\\\\n",
    "\\text{} \\quad & \\textbf{Input:} \\ \\text{equivalence margin } \\Delta \\text{,graph G, samples a and b and obs. distr. } P_a(V) \\text{ and } P_b(V)\\\\\n",
    "\\text{} \\quad & \\textbf{Output:} \\ \\text{array of p-values}  \\\\\n",
    "\\text{} \\quad & \\\\\n",
    "\\text{1.} \\quad & \\text{Choose a test allowing for one-sided testing} \\\\\n",
    "\\text{2.} \\quad & \\text{For the chosen test, test hypotheses: }  \\\\\n",
    "\\text{3.} \\quad & \\quad  H_{0,1}: P_a(Y|X,W,Z) - P_a(Y|X,W,Z) \\leq -\\Delta \\quad \\text{vs} \\quad H_{1,1}: P_a(Y|X,W,Z) - P_a(Y|X,W,Z) > -\\Delta  \\\\\n",
    "\\text{4.} \\quad & \\quad H_{0,2}: P_a(Y|X,W,Z) - P_a(Y|X,W,Z) \\geq +\\Delta \\quad \\text{vs} \\quad H_{1,2}: P_a(Y|X,W,Z) - P_a(Y|X,W,Z) < +\\Delta \\\\\n",
    "\\text{5.} \\quad & \\text{Obtain corresponding p-values}  \\\\\n",
    "\\end{align*} \n",
    "\n",
    "Reviewing the previous methods,we will adapt the cond. independence testing of weighted outcomes [Schrouff et al (2022)](https://proceedings.neurips.cc/paper_files/paper/2022/hash/7a969c30dc7e74d4e891c8ffb217cf79-Abstract-Conference.html) to fit within this regime. While very specific to our task, another reason to choose this method is that we would not have to deal with 8 subproblems.     \n",
    "\n",
    "Additionally, we can easily modify the procedure to be performing two-one-sided tests whether the differences in weighted outcomes is significantly larger or smaller than $[-\\Delta,+\\Delta]$.\n",
    "Also, the focus will be primarily on testing sample 1 vs sample 2. As they are drawn from the same distribution, we would expect a powerful test to pick up on that and be able to conclude equivalence with a tight margin $\\Delta$. However, we will also investigate sample 3 and the different shifts performed before to see whether we will see some wrong conclusions.\n",
    "\n",
    "Let's first focus on our initial samples and approach this problem iteratively to get a feeling for this tests power. We set the initial margin at $\\Delta=0.5$. This means, test whether the difference between the conditional of Y lies between $[-0.5,0.5]$. Both comparisons should yield significant p-values as the difference between sample 1 and sample 3 is $0.09$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Define function to perform weighted bootstrap test\n",
    "def weighted_outcomes_bootstrap_test_schrouff(sample_a,sample_b, causal_parents_source, B=10000, split=0.75,delta=0.05):\n",
    "\n",
    "    np.random.seed(36)\n",
    "    '''\n",
    "    Function to perform weighted bootstrap test for difference in conditional probabilities of Y=1 given X,Z,W for two samples\n",
    "    as described in Schrouff et al. (2022)\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    sample_a: pandas DataFrame containing sample A\n",
    "    sample_b: pandas DataFrame containing sample B\n",
    "    causal_parents_source: list of strings containing causal paraents of variable of interest (i.e. PA(Y) = {X,Z,W})\n",
    "    B: int, number of bootstrap iterations\n",
    "    split: float, proportion of samples to use for estimating the weights using logistic regression\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    bootstrapped_t_stats: numpy array containing bootstrapped t-statistics for each tested conditional probability\n",
    "    bootstrapped_mean_probabilities: numpy array containing the mean bootstrapped probabilities for each tested conditional probability\n",
    "    bootstrapped_weights_mean: numpy array containing the mean bootstrapped weights for each sample\n",
    "    bootstrapped_weighted_means: numpy array containing the bootstrapped weighted means for each sample\n",
    "    bootstrapped_pooled_se: numpy array containing the bootstrapped pooled standard errors for each tested conditional probability\n",
    "    p_values: float containing the p-value obtained from a two-sided z-test of the bootstrapped t-statistics distribution against 0\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "    #Step 1: Split initial samples into weighting and testing sets\n",
    "    ##Split the samples\n",
    "    sample_a_weights = sample_a.sample(frac=split)\n",
    "    sample_b_weights = sample_b.sample(frac=split)\n",
    "    sample_a_testing = sample_a[~sample_a.index.isin(sample_a_weights.index)]\n",
    "    sample_b_testing = sample_b[~sample_b.index.isin(sample_b_weights.index)]\n",
    "    ##initialize empty arrays to store bootstrapped t-statistics, bootstrapped probabilities, mean bootstrapped weights and bootstrapped weighted means\n",
    "    bootstrapped_t_stats = np.empty((2,B))\n",
    "    bootstrapped_mean_probabilities = np.empty(B)\n",
    "    bootstrapped_weights_mean = np.empty((2,B))\n",
    "    bootstrapped_weighted_outcome = np.empty((2,B))\n",
    "    bootstrapped_pooled_se = np.empty(B)\n",
    "    differences = np.empty(B)\n",
    "    #Step2: perform bootstrap\n",
    "    for i in tqdm.tqdm(range(B), desc='Bootstrapping'):\n",
    "        #Test, for each bootstrap iteration have a different set for weights and testing\n",
    "        #Step 2.1: Sample with replacement from the weights and testing\n",
    "        bootstrap_sample_a_weights = sample_a_weights.sample(n=len(sample_a_weights), replace=True)\n",
    "        bootstrap_sample_b_weights = sample_b_weights.sample(n=len(sample_b_weights), replace=True)\n",
    "        bootstrap_sample_a_testing = sample_a_testing.sample(n=len(sample_a_testing), replace=True)\n",
    "        bootstrap_sample_b_testing = sample_b_testing.sample(n=len(sample_b_testing), replace=True)\n",
    "\n",
    "        #Step 2.2: Fit logistic regression model to estimate weights\n",
    "        ##Pool the samples\n",
    "        pooled_weights = pd.concat([bootstrap_sample_a_weights, bootstrap_sample_b_weights])\n",
    "        ##Add sample indicator\n",
    "        pooled_weights['Sample'] = ['Sample A']*len(bootstrap_sample_a_weights) + ['Sample B']*len(bootstrap_sample_b_weights)\n",
    "        ##Feature matrix X with intercept\n",
    "        X = sm.add_constant(pooled_weights[causal_parents_source])\n",
    "        ##Target vector Y with sample 1 as reference\n",
    "        Y = (pooled_weights['Sample'] == 'Sample B').astype(int)\n",
    "        ##Fit logistic regression model\n",
    "        logit_model = sm.Logit(Y, X).fit(disp=0)\n",
    "        ##extract weights on testing set\n",
    "        X_a = sm.add_constant(bootstrap_sample_a_testing[causal_parents_source])\n",
    "        X_b = sm.add_constant(bootstrap_sample_b_testing[causal_parents_source])\n",
    "        p_a = logit_model.predict(X_a)\n",
    "        p_a = 1-p_a #need to compute 1-pa, because the logit model predicts the probability of being in class b\n",
    "        p_b = logit_model.predict(X_b)\n",
    "        #Store the mean probability for p_a\n",
    "        bootstrapped_mean_probabilities[i] = np.mean(p_a)\n",
    "        if np.any(p_a == 0) or np.any(p_b == 0):\n",
    "            print('Warning: Zero values in p_a or p_b')\n",
    "        if np.any(p_a < 1e-10) or np.any(p_b < 1e-10):  # Threshold can vary based on the scale of your data\n",
    "            print('Warning: Near-zero values in p_a or p_b')\n",
    "        #Compute the weights\n",
    "        w_a = 1/p_a\n",
    "        w_b = 1/p_b\n",
    "        #Normalize the weights\n",
    "        w_a_norm = w_a/np.sum(w_a)\n",
    "        w_b_norm = w_b/np.sum(w_b)\n",
    "        #Check for nans and infs\n",
    "        if np.any(np.isnan(w_a)) or np.any(np.isnan(w_b)):\n",
    "            print('Warning: NaN values in w_a or w_b')\n",
    "        if np.any(np.isinf(w_a)) or np.any(np.isinf(w_b)):\n",
    "            print('Warning: Inf values in w_a or w_b')\n",
    "        #Check for very small values\n",
    "        if np.any(w_a < 1e-10) or np.any(w_b < 1e-10):\n",
    "            print('Warning: Near-zero values in w_a or w_b')\n",
    "        \n",
    "        #Compute hadamard product\n",
    "        Y_a = bootstrap_sample_a_testing['Y'].values\n",
    "        Y_b = bootstrap_sample_b_testing['Y'].values\n",
    "        product_a = w_a_norm.values*Y_a\n",
    "        product_b = w_b_norm.values*Y_b\n",
    "        sum_a = np.sum(product_a)\n",
    "        sum_b = np.sum(product_b)\n",
    "        difference = sum_a - sum_b\n",
    "        differences[i] = difference\n",
    "        #Compute t-statistic against -delta and delta\n",
    "        bootstrapped_t_stats[0,i] = ttest_1samp(difference, -delta, alternative = 'greater')[0]\n",
    "        bootstrapped_t_stats[1,i] = ttest_1samp(difference, delta, alternative = 'less')[0]\n",
    "        '''\n",
    "        together = np.concatenate([product_a,-product_b])\n",
    "        bootstrapped_t_stats[0,i] = ttest_1samp(together, -delta, alternative = 'greater')[0]\n",
    "        bootstrapped_t_stats[1,i] = ttest_1samp(together, delta, alternative = 'less')[0]\n",
    "        '''\n",
    "        #Compute and store weighted means\n",
    "        bootstrapped_weighted_outcome[0,i] = np.sum(product_a)\n",
    "        bootstrapped_weighted_outcome[1,i] = np.sum(product_b)\n",
    "        #Compute and store mean weights\n",
    "        bootstrapped_weights_mean[0,i] = np.mean(w_a)\n",
    "        bootstrapped_weights_mean[1,i] = np.mean(w_b)\n",
    "\n",
    "    #Compute t-statistic\n",
    "    mean_diff = np.mean(differences)\n",
    "    std_diff = np.std(differences)\n",
    "    t_stat_lower, p_value_lower = ttest_1samp(differences, -delta, alternative = 'greater')\n",
    "    t_stat_greater, p_value_greater = ttest_1samp(differences, delta, alternative = 'less')\n",
    "    \n",
    "    '''\n",
    "    #Step 3: Compute p-values\n",
    "    mean_greater = np.mean(bootstrapped_t_stats[0])\n",
    "    std_greater = np.std(bootstrapped_t_stats[0])\n",
    "    mean_less = np.mean(bootstrapped_t_stats[1])\n",
    "    std_less = np.std(bootstrapped_t_stats[1])\n",
    "    p_value_greater = (2*norm.cdf(-abs((mean_greater)/std_greater)))\n",
    "    p_value_less = (2*norm.cdf(-abs((mean_less)/std_less)))\n",
    "    '''\n",
    "    return bootstrapped_t_stats, p_value_greater, p_value_lower, bootstrapped_weighted_outcome, differences\n",
    "\n",
    "\n",
    "#Perform the test for sample 1 vs sample 2 and sample 1 vs sample 3\n",
    "#Draw the initial samples again\n",
    "sample_1, sample_2 = hiring_data(0,n=1000)\n",
    "_, sample_3 = hiring_data(1)\n",
    "\n",
    "causal_parents_source = ['X','Z','W']\n",
    "bootstrapped_t_stats_s1s2, p_value_s1s2_less_ub, p_value_s1s2_greater_lb, weighted_outcomes_s1s2,differences_s1s2 = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_2, causal_parents_source,split=0.4,B=20000,delta=0.5)\n",
    "bootstrapped_t_stats_s1s3, p_value_s1s3_less_ub, p_value_s1s3_greater_lb, weighted_outcomes_s1s3,differences_s1s3  = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_3, causal_parents_source,split=0.4,B=20000,delta=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the first iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograms of the differences in weighted outcomes and the weighted outcomes itself\n",
    "fig, axs = plt.subplots(2,2, figsize=(20, 10))\n",
    "axs[0,0].hist(differences_s1s2, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "#Add mean difference as vertical line\n",
    "axs[0,0].axvline(np.mean(differences_s1s2), color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "axs[0,0].set_title('Differences in weighted outcomes:Sample 1 vs Sample 2')\n",
    "axs[0,0].set_xlabel('Difference in weighted outcomes')\n",
    "axs[0,0].set_ylabel('Frequency')\n",
    "#Add mean values to legend and p-values\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Mean')]\n",
    "axs[0,0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value > lb = {p_value_s1s2_greater_lb}'), plt.Line2D([0], [0], color='none', lw=1, label=f'p-value < ub = {p_value_s1s2_less_ub}')])  \n",
    "\n",
    "axs[0,1].hist(weighted_outcomes_s1s2[0], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0,1].hist(weighted_outcomes_s1s2[1], bins=50, alpha=0.3, color='red', edgecolor='black')\n",
    "#Add mean values as vertical lines\n",
    "axs[0,1].axvline(np.mean(weighted_outcomes_s1s2[0]), color='blue', linestyle='dashed', linewidth=2, label='Mean Sample 1')\n",
    "axs[0,1].axvline(np.mean(weighted_outcomes_s1s2[1]), color='red', linestyle='dashed', linewidth=2, label='Mean Sample 2')\n",
    "axs[0,1].set_title('Weighted outcomes: Sample 1 vs Sample 2')\n",
    "axs[0,1].set_xlabel('Weighted outcomes')\n",
    "axs[0,1].set_ylabel('Frequency')\n",
    "#Add mean values to legend\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Sample 1'), plt.Line2D([0], [0], color='red', lw=1, label='Sample 2')]\n",
    "axs[0,1].legend(handles=handles)\n",
    "\n",
    "axs[1,0].hist(differences_s1s3, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "#Add mean difference as vertical line\n",
    "axs[1,0].axvline(np.mean(differences_s1s3), color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "axs[1,0].set_title('Differences in weighted outcomes:Sample 1 vs Sample 3')\n",
    "axs[1,0].set_xlabel('Difference in weighted outcomes')\n",
    "axs[1,0].set_ylabel('Frequency')\n",
    "#Add mean values to legend and p-values\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Mean')]\n",
    "axs[1,0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value > lb = {p_value_s1s3_greater_lb}'), plt.Line2D([0], [0], color='none', lw=1, label=f'p-value < ub = {p_value_s1s3_less_ub}')])\n",
    "\n",
    "axs[1,1].hist(weighted_outcomes_s1s3[0], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1,1].hist(weighted_outcomes_s1s3[1], bins=50, alpha=0.3, color='red', edgecolor='black')\n",
    "#Add mean values as vertical lines\n",
    "axs[1,1].axvline(np.mean(weighted_outcomes_s1s3[0]), color='blue', linestyle='dashed', linewidth=2, label='Mean Sample 1')\n",
    "axs[1,1].axvline(np.mean(weighted_outcomes_s1s3[1]), color='red', linestyle='dashed', linewidth=2, label='Mean Sample 3')\n",
    "axs[1,1].set_title('Weighted outcomes: Sample 1 vs Sample 3')\n",
    "axs[1,1].set_xlabel('Weighted outcomes')\n",
    "axs[1,1].set_ylabel('Frequency')\n",
    "#Add mean values to legend\n",
    "#Handles should include the rounded mean values for the weighted outcomes\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Sample 1'), plt.Line2D([0], [0], color='red', lw=1, label='Sample 3')]\n",
    "axs[1,1].legend(handles=handles)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the TOST-method we are essentially testing whether the difference in weighted outcomes fits within that region. Given the rather high margin of $0.5$, we see significant results for both comparisons which is as expected.\n",
    "\n",
    "Let's repeat the procedure with a halved equivalence bound (i.e. $\\Delta = 0.25$) and visualize the results. We should still see significant results for both comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, p_value_s1s2_less_ub, p_value_s1s2_greater_lb, weighted_outcomes_s1s2,differences_s1s2 = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_2, causal_parents_source,B=20000,split=0.5,delta=0.25)\n",
    "_, p_value_s1s3_less_ub, p_value_s1s3_greater_lb, weighted_outcomes_s1s3,differences_s1s3  = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_3, causal_parents_source,B=20000,split=0.5,delta=0.25)\n",
    "\n",
    "#Histograms of the differences in weighted outcomes and the weighted outcomes itself\n",
    "fig, axs = plt.subplots(2,2, figsize=(20, 10))\n",
    "axs[0,0].hist(differences_s1s2, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "#Add mean difference as vertical line\n",
    "axs[0,0].axvline(np.mean(differences_s1s2), color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "axs[0,0].set_title('Differences in weighted outcomes:Sample 1 vs Sample 2')\n",
    "axs[0,0].set_xlabel('Difference in weighted outcomes')\n",
    "axs[0,0].set_ylabel('Frequency')\n",
    "#Add mean values to legend and p-values\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Mean')]\n",
    "axs[0,0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value > lb = {p_value_s1s2_greater_lb}'), plt.Line2D([0], [0], color='none', lw=1, label=f'p-value < ub = {p_value_s1s2_less_ub}')])  \n",
    "\n",
    "axs[0,1].hist(weighted_outcomes_s1s2[0], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0,1].hist(weighted_outcomes_s1s2[1], bins=50, alpha=0.3, color='red', edgecolor='black')\n",
    "#Add mean values as vertical lines\n",
    "axs[0,1].axvline(np.mean(weighted_outcomes_s1s2[0]), color='blue', linestyle='dashed', linewidth=2, label='Mean Sample 1')\n",
    "axs[0,1].axvline(np.mean(weighted_outcomes_s1s2[1]), color='red', linestyle='dashed', linewidth=2, label='Mean Sample 2')\n",
    "axs[0,1].set_title('Weighted outcomes: Sample 1 vs Sample 2')\n",
    "axs[0,1].set_xlabel('Weighted outcomes')\n",
    "axs[0,1].set_ylabel('Frequency')\n",
    "#Add mean values to legend\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Sample 1'), plt.Line2D([0], [0], color='red', lw=1, label='Sample 2')]\n",
    "axs[0,1].legend(handles=handles)\n",
    "\n",
    "axs[1,0].hist(differences_s1s3, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "#Add mean difference as vertical line\n",
    "axs[1,0].axvline(np.mean(differences_s1s3), color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "axs[1,0].set_title('Differences in weighted outcomes:Sample 1 vs Sample 3')\n",
    "axs[1,0].set_xlabel('Difference in weighted outcomes')\n",
    "axs[1,0].set_ylabel('Frequency')\n",
    "#Add mean values to legend and p-values\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Mean')]\n",
    "axs[1,0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value > lb = {p_value_s1s3_greater_lb}'), plt.Line2D([0], [0], color='none', lw=1, label=f'p-value < ub = {p_value_s1s3_less_ub}')])\n",
    "\n",
    "axs[1,1].hist(weighted_outcomes_s1s3[0], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1,1].hist(weighted_outcomes_s1s3[1], bins=50, alpha=0.3, color='red', edgecolor='black')\n",
    "#Add mean values as vertical lines\n",
    "axs[1,1].axvline(np.mean(weighted_outcomes_s1s3[0]), color='blue', linestyle='dashed', linewidth=2, label='Mean Sample 1')\n",
    "axs[1,1].axvline(np.mean(weighted_outcomes_s1s3[1]), color='red', linestyle='dashed', linewidth=2, label='Mean Sample 3')\n",
    "axs[1,1].set_title('Weighted outcomes: Sample 1 vs Sample 3')\n",
    "axs[1,1].set_xlabel('Weighted outcomes')\n",
    "axs[1,1].set_ylabel('Frequency')\n",
    "#Add mean values to legend\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Sample 1'), plt.Line2D([0], [0], color='red', lw=1, label='Sample 3')]\n",
    "axs[1,1].legend(handles=handles)\n",
    "plt.show()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the test correctly concludes that any differences, if at all, are likely within the equivalence bound. Lets reduce it further and set $\\Delta = 0.05$. We should still see rejections for sample 1 vs sample 2. The interesting case is sample 1 vs sample 3, given that the true difference in $P(Y=1|X,Z,W)$ is $0.09$.  As the true difference is $P_{s_1}(Y=1|X,Z,W)-P_{s_2}(Y=1|X,Z,W)=-0.09$, we would expect to see a non-significant p-value for $P_{s_1}(Y=1|X,Z,W)-P_{s_2}(Y=1|X,Z,W) > -0.05$, but a significant p-value for the difference being less than the upper bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, p_value_s1s2_less_ub, p_value_s1s2_greater_lb, weighted_outcomes_s1s2,differences_s1s2 = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_2, causal_parents_source,B=20000,split=0.5,delta=0.05)\n",
    "_, p_value_s1s3_less_ub, p_value_s1s3_greater_lb, weighted_outcomes_s1s3,differences_s1s3  = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_3, causal_parents_source,B=20000,split=0.5,delta=0.05)\n",
    "\n",
    "#Histograms of the differences in weighted outcomes and the weighted outcomes itself\n",
    "fig, axs = plt.subplots(2,2, figsize=(20, 10))\n",
    "axs[0,0].hist(differences_s1s2, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "#Add mean difference as vertical line\n",
    "axs[0,0].axvline(np.mean(differences_s1s2), color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "axs[0,0].set_title('Differences in weighted outcomes:Sample 1 vs Sample 2')\n",
    "axs[0,0].set_xlabel('Difference in weighted outcomes')\n",
    "axs[0,0].set_ylabel('Frequency')\n",
    "#Add mean values to legend and p-values\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Mean')]\n",
    "axs[0,0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value > lb = {p_value_s1s2_greater_lb}'), plt.Line2D([0], [0], color='none', lw=1, label=f'p-value < ub = {p_value_s1s2_less_ub}')])  \n",
    "\n",
    "axs[0,1].hist(weighted_outcomes_s1s2[0], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0,1].hist(weighted_outcomes_s1s2[1], bins=50, alpha=0.3, color='red', edgecolor='black')\n",
    "#Add mean values as vertical lines\n",
    "axs[0,1].axvline(np.mean(weighted_outcomes_s1s2[0]), color='blue', linestyle='dashed', linewidth=2, label='Mean Sample 1')\n",
    "axs[0,1].axvline(np.mean(weighted_outcomes_s1s2[1]), color='red', linestyle='dashed', linewidth=2, label='Mean Sample 2')\n",
    "axs[0,1].set_title('Weighted outcomes: Sample 1 vs Sample 2')\n",
    "axs[0,1].set_xlabel('Weighted outcomes')\n",
    "axs[0,1].set_ylabel('Frequency')\n",
    "#Add mean values to legend\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Sample 1'), plt.Line2D([0], [0], color='red', lw=1, label='Sample 2')]\n",
    "axs[0,1].legend(handles=handles)\n",
    "\n",
    "axs[1,0].hist(differences_s1s3, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "#Add mean difference as vertical line\n",
    "axs[1,0].axvline(np.mean(differences_s1s3), color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "axs[1,0].set_title('Differences in weighted outcomes:Sample 1 vs Sample 3')\n",
    "axs[1,0].set_xlabel('Difference in weighted outcomes')\n",
    "axs[1,0].set_ylabel('Frequency')\n",
    "#Add mean values to legend and p-values\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Mean')]\n",
    "axs[1,0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value > lb = {p_value_s1s3_greater_lb}'), plt.Line2D([0], [0], color='none', lw=1, label=f'p-value < ub = {p_value_s1s3_less_ub}')])\n",
    "\n",
    "axs[1,1].hist(weighted_outcomes_s1s3[0], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1,1].hist(weighted_outcomes_s1s3[1], bins=50, alpha=0.3, color='red', edgecolor='black')\n",
    "#Add mean values as vertical lines\n",
    "axs[1,1].axvline(np.mean(weighted_outcomes_s1s3[0]), color='blue', linestyle='dashed', linewidth=2, label='Mean Sample 1')\n",
    "axs[1,1].axvline(np.mean(weighted_outcomes_s1s3[1]), color='red', linestyle='dashed', linewidth=2, label='Mean Sample 3')\n",
    "axs[1,1].set_title('Weighted outcomes: Sample 1 vs Sample 3')\n",
    "axs[1,1].set_xlabel('Weighted outcomes')\n",
    "axs[1,1].set_ylabel('Frequency')\n",
    "#Add mean values to legend\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Sample 1'), plt.Line2D([0], [0], color='red', lw=1, label='Sample 3')]\n",
    "axs[1,1].legend(handles=handles)\n",
    "plt.show()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sample 1 and sample 2, we do not see any rejections which is in line with the ground-truth. \n",
    "\n",
    "For sample 1 vs sample 3, we see a rejection for the one-sided test of whether the differences are significantly greater than $-0.05$. Thus, we would now conclude that sample 1 and sample 3 are not equivalent to each other with respect to the equivalence bound and there is not sufficient evidence for them being equivalent to each other with respect to $\\Delta$.\n",
    "\n",
    "Important is that by design, we cannot for sure say when two samples are drawn from the same underlying distributions. All we can do is try to find as tight a margin as we can.\n",
    "\n",
    "Let's investigate how small we can let the margin become before we will see rejections between sample 1 and sample 2. We start with $0.05$ and work ourselves down to $0.001$. If, we do not see any rejections for very small margins, one might get suspicious. As can be seen throughout the notebook, while we would expect differences between sample 1 and sample 2 to be close to zero, we would not expect them to be that close to zero that we would see a significant margin of $\\Delta = 0.001$ due to sampling variation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "margin = [0.05,0.04,0.03,0.02,0.01,0.005,0.0025,0.001] \n",
    "p_values_less_ub = np.empty(len(margin))\n",
    "p_values_greater_lb = np.empty(len(margin))\n",
    "differences_s1s2 = np.empty((len(margin),20000))\n",
    "weighted_outcomes_s1s2 = np.empty((len(margin),2,20000))\n",
    "\n",
    "#loop over margins\n",
    "for i in range(len(margin)):\n",
    "    _, p_value_s1s2_less_ub, p_value_s1s2_greater_lb, weighted_outcomes_s1s2[i],differences_s1s2[i] = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_2, causal_parents_source,B=20000,split=0.5,delta=margin[i])\n",
    "    p_values_less_ub[i] = p_value_s1s2_less_ub\n",
    "    p_values_greater_lb[i] = p_value_s1s2_greater_lb\n",
    "\n",
    "#Plot bootstrapped differences and weighted outcomes for different margins\n",
    "fig, axs = plt.subplots(8,2, figsize=(30, 30))\n",
    "\n",
    "for i in range(len(margin)):\n",
    "    axs[i,0].hist(differences_s1s2[i], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "    #Add mean difference as vertical line\n",
    "    axs[i,0].axvline(np.mean(differences_s1s2[i]), color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "    axs[i,0].set_title(f'Differences :Sample 1 vs Sample 2 with margin = {margin[i]}')\n",
    "    #axs[i,0].set_xlabel('Difference in weighted outcomes')\n",
    "    axs[i,0].set_ylabel('Frequency')\n",
    "    #Add mean values to legend and p-values\n",
    "    handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Mean')]\n",
    "    axs[i,0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value > lb = {np.round(p_values_greater_lb[i],4)}'), plt.Line2D([0], [0], color='none', lw=1, label=f'p-value < ub = {p_values_less_ub[i]}')])\n",
    "    axs[i,1].hist(weighted_outcomes_s1s2[i][0], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "    axs[i,1].hist(weighted_outcomes_s1s2[i][1], bins=50, alpha=0.3, color='red', edgecolor='black')\n",
    "    #Add mean values as vertical lines\n",
    "    axs[i,1].axvline(np.mean(weighted_outcomes_s1s2[i][0]), color='blue', linestyle='dashed', linewidth=2, label='Mean Sample 1')\n",
    "    axs[i,1].axvline(np.mean(weighted_outcomes_s1s2[i][1]), color='red', linestyle='dashed', linewidth=2, label='Mean Sample 2')\n",
    "    axs[i,1].set_title(f'Weighted outcomes: Sample 1 vs Sample 2 with margin = {margin[i]}')\n",
    "    handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Sample 1'), plt.Line2D([0], [0], color='red', lw=1, label='Sample 2')]\n",
    "    axs[i,1].legend(handles=handles)\n",
    "    #axs[i,1].set_xlabel('Weighted outcomes')\n",
    "    axs[i,1].set_ylabel('Frequency')\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the test finds very small significant equivalence margins. From $\\Delta = 0.01$, we start to see non-significant results. This is somewhat a good sign, as it would be unrealistic if the test would be able to find increasingly smaller margins even if samples are drawn from the same distributions. However, using this result, we would be only able to establish a equivalence bound of $[-0.02,+0.02] between samples 1 and 2.\n",
    "\n",
    "Let's run the test again for margins $\\Delta \\leq 0.01$ for a sample size of $n=5,000$ instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sample_2_n5000 = hiring_data(0,n=5000)\n",
    "margin = [0.01,0.005,0.0025,0.001]\n",
    "p_values_less_ub = np.empty(len(margin))\n",
    "p_values_greater_lb = np.empty(len(margin))\n",
    "differences_s1s2 = np.empty((len(margin),20000))\n",
    "weighted_outcomes_s1s2 = np.empty((len(margin),2,20000))\n",
    "\n",
    "#loop over margins\n",
    "for i in range(len(margin)):\n",
    "    _, p_value_s1s2_less_ub, p_value_s1s2_greater_lb, weighted_outcomes_s1s2[i],differences_s1s2[i] = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_2_n5000, causal_parents_source,B=20000,split=0.5,delta=margin[i])\n",
    "    p_values_less_ub[i] = p_value_s1s2_less_ub\n",
    "    p_values_greater_lb[i] = p_value_s1s2_greater_lb\n",
    "\n",
    "#Plot bootstrapped differences and weighted outcomes for different margins\n",
    "fig, axs = plt.subplots(4,2, figsize=(30, 30))\n",
    "\n",
    "for i in range(len(margin)):\n",
    "    axs[i,0].hist(differences_s1s2[i], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "    #Add mean difference as vertical line\n",
    "    axs[i,0].axvline(np.mean(differences_s1s2[i]), color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "    axs[i,0].set_title(f'Differences :Sample 1 vs Sample 2 with margin = {margin[i]}')\n",
    "    #axs[i,0].set_xlabel('Difference in weighted outcomes')\n",
    "    axs[i,0].set_ylabel('Frequency')\n",
    "    #Add mean values to legend and p-values\n",
    "    handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Mean')]\n",
    "    axs[i,0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value > lb = {np.round(p_values_greater_lb[i],4)}'), plt.Line2D([0], [0], color='none', lw=1, label=f'p-value < ub = {p_values_less_ub[i]}')])\n",
    "    axs[i,1].hist(weighted_outcomes_s1s2[i][0], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "    axs[i,1].hist(weighted_outcomes_s1s2[i][1], bins=50, alpha=0.3, color='red', edgecolor='black')\n",
    "    #Add mean values as vertical lines\n",
    "    axs[i,1].axvline(np.mean(weighted_outcomes_s1s2[i][0]), color='blue', linestyle='dashed', linewidth=2, label='Mean Sample 1')\n",
    "    axs[i,1].axvline(np.mean(weighted_outcomes_s1s2[i][1]), color='red', linestyle='dashed', linewidth=2, label='Mean Sample 2')\n",
    "    handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Sample 1'), plt.Line2D([0], [0], color='red', lw=1, label='Sample 2')]\n",
    "    axs[i,1].legend(handles=handles)\n",
    "    axs[i,1].set_title(f'Weighted outcomes: Sample 1 vs Sample 2 with margin = {margin[i]}')\n",
    "    #axs[i,1].set_xlabel('Weighted outcomes')\n",
    "    axs[i,1].set_ylabel('Frequency')\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for a higher sample size (i.e. $n=5,000$), we are able to find a significant equivalence bound up to a margin of $\\Delta=0.005$. This is quite impressive, as for this test we would conclude that the difference between sample 1 and sample 2 likely falls into the range $[-0.005.+0.005]$. For smaller margins, the results become insignificant. Thus, also with equivalence testing it matters how large the corresponding samples are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is also important to note that the equivalence bounds $\\Delta$ should be choosen application-specific and not treated as a mere hyperparameter. This is somewhat akin to some criticism of conditional independence testing methods for causal discovery where one can fall at risk of treating $\\alpha$ as a hyperparameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, let's use the test on the different type of shifts as before:\n",
    "- Shift only in W -> expect equivalence within margins $[-0.01,+0.01]$\n",
    "- Shift in W and Y -> do not expect equivalence within margins $[-0.01,+0.01]$\n",
    "- Y not depending on W anymore -> do not expect equivalence within margins $[-0.01,+0.01]$\n",
    "- Shift in Y such that it is constant -> do not expect equivalence within margins $[-0.01,+0.01]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#List of different shifts\n",
    "sample_3_shifts = [sample_3_shift_only_w,sample_3_shift_w_and_y,sample_3_shift_y_not_depending_on_w,sample_3_shift_y_constant]\n",
    "shifts = ['W','W and Y','Y not depending on W','Y constant']\n",
    "#Perform the test for sample 1 vs sample 3 with different shifts\n",
    "p_values_less_ub = np.empty(len(sample_3_shifts))\n",
    "p_values_greater_lb = np.empty(len(sample_3_shifts))\n",
    "differences_s1s3 = np.empty((len(sample_3_shifts),20000))\n",
    "weighted_outcomes_s1s3 = np.empty((len(sample_3_shifts),2,20000))\n",
    "\n",
    "#loop over margins\n",
    "for i in range(len(sample_3_shifts)):\n",
    "    _, p_value_s1s3_less_ub, p_value_s1s3_greater_lb, weighted_outcomes_s1s3[i],differences_s1s3[i] = weighted_outcomes_bootstrap_test_schrouff(sample_1,sample_3_shifts[i], causal_parents_source,B=20000,split=0.5,delta=0.05)\n",
    "    p_values_less_ub[i] = p_value_s1s3_less_ub\n",
    "    p_values_greater_lb[i] = p_value_s1s3_greater_lb\n",
    "'''\n",
    "#Plot bootstrapped differences and weighted outcomes for different margins\n",
    "fig, axs = plt.subplots(4,2, figsize=(20, 20))\n",
    "\n",
    "for i in range(len(sample_3_shifts)):\n",
    "    axs[i,0].hist(differences_s1s3[i], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "    #Add mean difference as vertical line\n",
    "    axs[i,0].axvline(np.mean(differences_s1s3[i]), color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "    axs[i,0].set_title(f'Differences: Sample 1 vs Sample 3 with shift: {shifts[i]}')\n",
    "    axs[i,0].set_xlabel('Difference in weighted outcomes')\n",
    "    axs[i,0].set_ylabel('Frequency')\n",
    "    #Add mean values to legend and p-values\n",
    "    handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Mean')]\n",
    "    axs[i,0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value > lb = {p_values_greater_lb[i]}'), plt.Line2D([0], [0], color='none', lw=1, label=f'p-value < ub = {p_values_less_ub[i]}')])\n",
    "    axs[i,1].hist(weighted_outcomes_s1s3[i][0], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "    axs[i,1].hist(weighted_outcomes_s1s3[i][1], bins=50, alpha=0.3, color='red', edgecolor='black')\n",
    "    #Add mean values as vertical lines\n",
    "    axs[i,1].axvline(np.mean(weighted_outcomes_s1s3[i][0]), color='blue', linestyle='dashed', linewidth=2, label='Mean Sample 1')\n",
    "    axs[i,1].axvline(np.mean(weighted_outcomes_s1s3[i][1]), color='red', linestyle='dashed', linewidth=2, label='Mean Sample 3')\n",
    "    axs[i,1].set_title(f'Weighted outcomes: Sample 1 vs Sample 3 with shift: {shifts[i]}')\n",
    "    handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Sample 1'), plt.Line2D([0], [0], color='red', lw=1, label='Sample 3')]\n",
    "    axs[i,1].legend(handles=handles)\n",
    "    axs[i,1].set_xlabel('Weighted outcomes')\n",
    "    axs[i,1].set_ylabel('Frequency')\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also for the different shifts, we see that the test correctly detects that only the case where there is a shift in W allows for a significant equivalence bound at $\\Delta=0.01$. This somehwat underlines the promise of this method to also work in more general examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Rules based on divergences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section compared to the previous one, is more based on heuristics and not really grounded in testing theory. I still decided to keep it as it somewhat shows where I started off when exploring how to test that two conditionals might be the same.\n",
    "\n",
    "We've seen that one can use divergences to get a heuristic whether two conditionals might be similar to each other or different from each other. The larger and more representative the samples are, the more accurate is the heuristic. Thus, one could think of how can we turn this testing for a shift turn into testing for similarity.\n",
    "\n",
    "The idea is relatively straight-forward. For two samples a and b, we compute the sample divergence (i.e. KL or JS). Then we bootstrap the samples multiple times to compute bootstrapped sample divergences. Using those, we compute a $95\\%$ bootstrap confidence interval for the divergence. Finally, we need to set a threeshold. Let's say we set the threeshold at $\\delta=0.05$, then the idea was to follow two rules:\n",
    "\n",
    "1) The complete CI must be $<\\Delta$.\n",
    "2) The CI must contain $0$.\n",
    "\n",
    "If both criterias hold, we could see that as some evidence that two samples have been drawn from at least very similar distributions. However, it is also easy to see that this should not be treated as more than a rule-of-thumb. It is not grounded in any of testing theory and has some clear disadvantages.\n",
    "\n",
    "First of all, the procedure essentially tries to test whether (in case of KL divergence) $KL(Sample 1||Sample 2) < \\Delta$ using bootstrapped confidence intervals. The same argument as before for not accepting the null in case of non-rejection applies here as well. We essentially computed a 95% range of likely values. If $0$ is in the CI, the true KL might be $0$ (same distributions), but it is not more than one of many other possible values. Secondly, while in the TOST-procedure for differences we have somewhat interpretable equivalence bounds, the case is not the same here anymore. \n",
    "\n",
    "However, one can already see that the idea is somewhat akin to equivalence testing. Where the approach is to try to establish if any differences exist, there are likely very small. This is also how I came to the attention to the field of equivalence testing.\n",
    "\n",
    "So, lets quickly  run through a simulation of the just laid out approach for the original three samples (no shift, shift of $0.09$) with sample sizes $n=1,000$ and $n=10,000$. We set the threeshold for the upper threeshold at $0.05$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Draw the initial samples again\n",
    "sample_1, sample_2 = hiring_data(0,n=1000)\n",
    "_, sample_3 = hiring_data(1)\n",
    "#Now with n=10,000\n",
    "sample_1_n10k,  sample_2n10k = hiring_data(0,n=10000)\n",
    "_, sample_3_n10k = hiring_data(1,n=10000)\n",
    "\n",
    "#bootstrap the divergences \n",
    "#N = 1,000\n",
    "_, js_s1s2, _, js_boot_s1s2, _, ci_js_s1s2 =  bootstrap_divergences(sample_1, sample_2, B=20000, pooling=False)\n",
    "_, js_s1s3, _, js_boot_s1s3, _, ci_js_s1s3 =  bootstrap_divergences(sample_1, sample_3, B=20000, pooling=False)\n",
    "#N = 10,000\n",
    "_, js_s1s2_n10k, _, js_boot_s1s2_n10k, _, ci_js_s1s2_n10k =  bootstrap_divergences(sample_1_n10k, sample_2n10k, B=20000, pooling=False)\n",
    "_, js_s1s3_n10k, _, js_boot_s1s3_n10k, _, ci_js_s1s3_n10k =  bootstrap_divergences(sample_1_n10k, sample_3_n10k, B=20000, pooling=False)\n",
    "'''\n",
    "#Plot the JS divergences\n",
    "fig, axs = plt.subplots(2,2, figsize=(20, 10))\n",
    "threeshold = 0.05\n",
    "axs[0,0].hist(js_boot_s1s2, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "#Add mean\n",
    "axs[0,0].axvline(js_s1s2, color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "#Add vertical line for threeshold\n",
    "axs[0,0].axvline(threeshold, color='Black', linestyle='dashed', linewidth=2, label='Threeshold')\n",
    "axs[0,0].set_title('JS :Sample 1 vs Sample 2 (n=1,000)')\n",
    "axs[0,0].set_xlabel('JS divergence')\n",
    "axs[0,0].set_ylabel('Frequency')\n",
    "#Add CIs as vertical lines\n",
    "axs[0,0].axvline(0 if ci_js_s1s2[0] < 0 else ci_js_s1s2[0], color='green', linestyle='dashed', linewidth=2, label='CI lower bound')\n",
    "axs[0,0].axvline(ci_js_s1s2[1], color='green', linestyle='dashed', linewidth=2, label='CI upper bound')\n",
    "#Add mean values to legend\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Mean')]\n",
    "axs[0,0].legend(handles=handles + [plt.Line2D([0], [0], color='green', lw=1, label='CI'), plt.Line2D([0], [0], color='black', lw=1, label='Threeshold')])\n",
    "axs[0,1].hist(js_boot_s1s3, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "#Add mean\n",
    "axs[0,1].axvline(js_s1s3, color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "axs[0,1].set_title('JS :Sample 1 vs Sample 3 (n=1,000)')\n",
    "axs[0,1].set_xlabel('JS divergence')\n",
    "axs[0,1].set_ylabel('Frequency')\n",
    "#Vertical line for threeshold\n",
    "axs[0,1].axvline(threeshold, color='Black', linestyle='dashed', linewidth=2, label='Threeshold')\n",
    "#Add CIs as vertical lines\n",
    "axs[0,1].axvline(0 if ci_js_s1s3[0] < 0 else ci_js_s1s3[0], color='green', linestyle='dashed', linewidth=2, label='CI lower bound')\n",
    "axs[0,1].axvline(ci_js_s1s3[1], color='green', linestyle='dashed', linewidth=2, label='CI upper bound')\n",
    "#Add mean values to legend\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Mean'), plt.Line2D([0], [0], color='green', lw=1, label='CI'), plt.Line2D([0], [0], color='black', lw=1, label='Threeshold')]\n",
    "axs[0,1].legend(handles=handles)\n",
    "#N=10,000\n",
    "axs[1,0].hist(js_boot_s1s2_n10k, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "#Add mean\n",
    "axs[1,0].axvline(js_s1s2_n10k, color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "#Vertical line for threeshold¨\n",
    "axs[1,0].axvline(threeshold, color='Black', linestyle='dashed', linewidth=2, label='Threeshold')\n",
    "axs[1,0].set_title('JS :Sample 1 vs Sample 2 (n=10,000)')\n",
    "axs[1,0].set_xlabel('JS divergence')\n",
    "axs[1,0].set_ylabel('Frequency')\n",
    "#Add CIs as vertical lines\n",
    "axs[1,0].axvline(0 if ci_js_s1s2_n10k[0] < 0 else ci_js_s1s2_n10k[0], color='green', linestyle='dashed', linewidth=2, label='CI lower bound')\n",
    "axs[1,0].axvline(ci_js_s1s2_n10k[1], color='green', linestyle='dashed', linewidth=2, label='CI upper bound')\n",
    "#Add mean values to legend\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Mean'), plt.Line2D([0], [0], color='black', lw=1, label='Threeshold')]\n",
    "axs[1,0].legend(handles=handles + [plt.Line2D([0], [0], color='green', lw=1, label='CI')])\n",
    "axs[1,1].hist(js_boot_s1s3_n10k, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "#Add mean\n",
    "axs[1,1].axvline(js_s1s3_n10k, color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "axs[1,1].set_title('JS :Sample 1 vs Sample 3 (n=10,000)')\n",
    "axs[1,1].set_xlabel('JS divergence')\n",
    "axs[1,1].set_ylabel('Frequency')\n",
    "#Vertical line for threeshold\n",
    "axs[1,1].axvline(threeshold, color='Black', linestyle='dashed', linewidth=2, label='Threeshold')\n",
    "#Add CIs as vertical lines\n",
    "axs[1,1].axvline(0 if ci_js_s1s3_n10k[0] < 0 else ci_js_s1s3_n10k[0], color='green', linestyle='dashed', linewidth=2, label='CI lower bound')\n",
    "axs[1,1].axvline(ci_js_s1s3_n10k[1], color='green', linestyle='dashed', linewidth=2, label='CI upper bound')\n",
    "#Add mean values to legend\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Mean'), plt.Line2D([0], [0], color='black', lw=1, label='Threeshold')]\n",
    "axs[1,1].legend(handles=handles + [plt.Line2D([0], [0], color='green', lw=1, label='CI')])\n",
    "plt.show()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we clearly see that the rules based on divergences are problematic. Only when $n=10,000$, using the rules we would consider samples 1 and sample 2 likely similar. For $n=1,000$, we see that not the entire interval fits below the threeshold.\n",
    "\n",
    "Thus, this highlights that it is quite difficult to use divergences to establish equivalence between samples. However, it is a good starting point to think about procedures that allow to test for equivalence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Summary\n",
    "\n",
    "In this section, we shifted our focus to ask the question: When can we assume two conditionals to be the same?\n",
    "\n",
    "We looked into the topic of equivalence testing and introduced the method of two-one-sided tests which stems from the field of clinical/pharamceutical trials where it is used, for instance, to establish equivalence in effects between drugs. Suprisingly, this somewhat simple method has not been adapted yet to be used to test whether two samples might have been (partially) drawn from the same underlying distributions. It seems most focus is given to methods that detect differences rather than equivalence.\n",
    "\n",
    "We adapted the procedure introduced by [Schrouff et al (2022)](https://proceedings.neurips.cc/paper_files/paper/2022/hash/7a969c30dc7e74d4e891c8ffb217cf79-Abstract-Conference.html) to a TOST-format and used it on our engineered hiring example. It out of the back performed quite well on the given examples, being able to establish  significant and tight equivalence bounds. This is a promising first step, as to the best of my knowledge, this is one of the first attempts to use equivalence testing methods to test for equivalence between distributions. The write-up will contain more details on the formalities of TOST-methods. The approach here was more empirical and future work should definetly explore the topic of equivalence testing in presence of distribution shifts more.\n",
    "\n",
    "This also has important implications for (Causal) Fairness Analyis under distribution shfits. Firstly, by being able to establish a equivalence bound between conditionals (i.e. $Y|X,Z,W$), we can somewhat provide reasonable bounds on how causal fairness measures might differ between environments. We have seen before that for x-specific effects (and others), we would need to recompute conditional $V_i|pa(V_i)$ for domains where there is a shift in $V_i$. By being able to test whether there is a interval $[-\\Delta,+\\Delta]$ around 0 in which the difference for the conditional between domains falls, we effectively can use that to compute a range of likely differences in the x-specific effects.  \n",
    "\n",
    "$\\Delta$ plays a key role in this. If we find significant equivalence with $\\Delta=0.05$, we would probably not conclude that the difference between domains is likely irrelevant. However, if we find significant equivalence with $\\Delta\\leq0.01$, we might be tempted to conclude that the differences between domains are likely irrelevant and treat them as equal.\n",
    "\n",
    "Then, using the property of markovianity, we can also make statements about the functional causal mechanisms behind the variables. Ultimately, these testing methods together with knowledge of how shifts affect our measures can be used as a springboard to investigate fairness under distribution shifts. For instance, it will tell us which components need to be reestimated to be able in order to transport causal fairness measures across domains. Another use-case is that it finally pays a way towards understanding when fairness transfers are possible. The title of the [Schrouff et al (2022)](https://proceedings.neurips.cc/paper_files/paper/2022/hash/7a969c30dc7e74d4e891c8ffb217cf79-Abstract-Conference.html) paper starts with ''Diagnosing failures of fairness transfer''. By now showing a way on how to test whether two conditionals might be almost equivalent, it paves the way for being able to diagnosing possibility of fairness transfers, instead of only being able to decide when its not possible.\n",
    "\n",
    "\n",
    "Lastly, given the experience of the last sections, we can derive a high-level instruction on how to proceed when testing over the observational distributions of two samples.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{} \\quad & \\textbf{Test for equivalence} \\ \\\\\n",
    "\\text{} \\quad & \\textbf{Input:} \\ \\text{equivalence margin } \\Delta \\text{,graph G, samples a and b and obs. distr. } P_a(V) \\text{ and } P_b(V)\\\\\n",
    "\\text{1.} \\quad &\\text{For each } P(V|pa(V)) \\text{ do:}\\\\\n",
    "\\text{2.} \\quad &\\quad \\text{Test for distribution shifts}\\\\\n",
    "\\text{3.} \\quad &\\text{For each } P(V|pa(V)) \\text{ for which no shift was detected, do:}\\\\\n",
    "\\text{4.} \\quad &\\quad \\text{Test for equivalence at margin } \\Delta \\\\\n",
    "\\end{align*}\n",
    "\n",
    "First, we want to perform the somewhat easier task of detecting shifts. We've seen that the methods proposed did not show any proneness for excessive type I errors and reasonable power. Thus, we first want to rule out equivalent conditionals by identifying shifted ones. Then, on the remaining conditionals we will perform equivalence testing at the specified margin. This should give a reasonable check of distribution shifts and if no shifts were detected if the remaining conditionals are almost equivalent to each other.\n",
    "\n",
    "\n",
    "This is something which is being explored in section 3, where we apply it to real-life data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Real-Life data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have worked with engineered examples and shifts. Now, we will apply the methods from the previous sections to investigate real-life data to which we do not have access to the ground-truth SCMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables import ACSDataSource, ACSIncome\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import entropy, chi2_contingency, norm, ttest_1samp\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Construct the data source\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "#Get the data for California, Michigan, Florida and Utah, Illinois, Texas, New York\n",
    "ca_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "mi_data = data_source.get_data(states=[\"MI\"], download=True)\n",
    "fl_data = data_source.get_data(states=[\"FL\"], download=True)\n",
    "ut_data = data_source.get_data(states=[\"UT\"], download=True)\n",
    "il_data = data_source.get_data(states=[\"IL\"], download=True)\n",
    "tx_data = data_source.get_data(states=[\"TX\"], download=True)\n",
    "ny_data = data_source.get_data(states=[\"NY\"], download=True)\n",
    "'''\n",
    "\n",
    "#Filter columns 'AGEP' to contain only people older than 18\n",
    "ca_data = ca_data[ca_data['AGEP'] > 18]\n",
    "mi_data = mi_data[mi_data['AGEP'] > 18]\n",
    "fl_data = fl_data[fl_data['AGEP'] > 18]\n",
    "ut_data = ut_data[ut_data['AGEP'] > 18]\n",
    "il_data = il_data[il_data['AGEP'] > 18]\n",
    "tx_data = tx_data[tx_data['AGEP'] > 18]\n",
    "ny_data = ny_data[ny_data['AGEP'] > 18]\n",
    "\n",
    "#Filter column 'PINCP' to contain only people with income > 100\n",
    "ca_data = ca_data[ca_data['PINCP'] > 100]\n",
    "mi_data = mi_data[mi_data['PINCP'] > 100]\n",
    "fl_data = fl_data[fl_data['PINCP'] > 100]\n",
    "ut_data = ut_data[ut_data['PINCP'] > 100]\n",
    "il_data = il_data[il_data['PINCP'] > 100]\n",
    "tx_data = tx_data[tx_data['PINCP'] > 100]\n",
    "ny_data = ny_data[ny_data['PINCP'] > 100]\n",
    "\n",
    "#Filter column WKHP to contain only people working more than 0 hours\n",
    "ca_data = ca_data[ca_data['WKHP'] > 0]\n",
    "mi_data = mi_data[mi_data['WKHP'] > 0]\n",
    "fl_data = fl_data[fl_data['WKHP'] > 0]\n",
    "ut_data = ut_data[ut_data['WKHP'] > 0]\n",
    "il_data = il_data[il_data['WKHP'] > 0]\n",
    "tx_data = tx_data[tx_data['WKHP'] > 0]\n",
    "ny_data = ny_data[ny_data['WKHP'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data for the task\n",
    "Y = \"PINCP\"\n",
    "X = \"SEX\"\n",
    "Z_demographic = [\"AGEP\",\"RAC1P\"]\n",
    "W_intermediate = [\"MAR\",\"SCHL\",\"COW\",\"OCCP\",\"RELP\"]\n",
    "\n",
    "#Extract the data\n",
    "ca_Y = ca_data[Y]\n",
    "ca_X = ca_data[X]\n",
    "ca_Z = ca_data[Z_demographic]\n",
    "ca_W = ca_data[W_intermediate]\n",
    "ca_df = pd.concat([ca_Y, ca_Z, ca_W,ca_X], axis=1)\n",
    "\n",
    "mi_Y = mi_data[Y]\n",
    "mi_X = mi_data[X]\n",
    "mi_Z = mi_data[Z_demographic]\n",
    "mi_W = mi_data[W_intermediate]\n",
    "mi_df = pd.concat([mi_Y, mi_Z, mi_W,mi_X], axis=1)\n",
    "\n",
    "fl_Y = fl_data[Y]\n",
    "fl_X = fl_data[X]\n",
    "fl_Z = fl_data[Z_demographic]\n",
    "fl_W = fl_data[W_intermediate]\n",
    "fl_df = pd.concat([fl_Y, fl_Z, fl_W,fl_X], axis=1)\n",
    "\n",
    "ut_Y = ut_data[Y]\n",
    "ut_X = ut_data[X]\n",
    "ut_Z = ut_data[Z_demographic]\n",
    "ut_W = ut_data[W_intermediate]\n",
    "ut_df = pd.concat([ut_Y, ut_Z, ut_W,ut_X], axis=1)\n",
    "\n",
    "il_Y = il_data[Y]\n",
    "il_X = il_data[X]\n",
    "il_Z = il_data[Z_demographic]\n",
    "il_W = il_data[W_intermediate]\n",
    "il_df = pd.concat([il_Y, il_Z, il_W,il_X], axis=1)\n",
    "\n",
    "tx_Y = tx_data[Y]\n",
    "tx_X = tx_data[X]\n",
    "tx_Z = tx_data[Z_demographic]\n",
    "tx_W = tx_data[W_intermediate]\n",
    "tx_df = pd.concat([tx_Y, tx_Z, tx_W,tx_X], axis=1)\n",
    "\n",
    "ny_Y = ny_data[Y]\n",
    "ny_X = ny_data[X]\n",
    "ny_Z = ny_data[Z_demographic]\n",
    "ny_W = ny_data[W_intermediate]\n",
    "ny_df = pd.concat([ny_Y, ny_Z, ny_W,ny_X], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#Discretize the Age variable in bins of 20 years\n",
    "# Define the bin edges\n",
    "bins = [18, 40, 60, 80, 100]\n",
    "# Create labels for each bin\n",
    "labels = ['18-40', '40-60', '60-80', '80-100']\n",
    "# Use pd.cut to bin the 'AGEP' column into categories\n",
    "ca_df['AGEP'] = pd.cut(ca_df['AGEP'], bins=bins, labels=labels, right=False)\n",
    "mi_df['AGEP'] = pd.cut(mi_df['AGEP'], bins=bins, labels=labels, right=False)\n",
    "fl_df['AGEP'] = pd.cut(fl_df['AGEP'], bins=bins, labels=labels, right=False)\n",
    "ut_df['AGEP'] = pd.cut(ut_df['AGEP'], bins=bins, labels=labels, right=False)\n",
    "il_df['AGEP'] = pd.cut(il_df['AGEP'], bins=bins, labels=labels, right=False)\n",
    "tx_df['AGEP'] = pd.cut(tx_df['AGEP'], bins=bins, labels=labels, right=False)\n",
    "ny_df['AGEP'] = pd.cut(ny_df['AGEP'], bins=bins, labels=labels, right=False)\n",
    "#Drop rows with NaN values\n",
    "ca_df = ca_df.dropna()\n",
    "mi_df = mi_df.dropna()\n",
    "fl_df = fl_df.dropna()\n",
    "ut_df = ut_df.dropna()\n",
    "il_df = il_df.dropna()\n",
    "tx_df = tx_df.dropna()\n",
    "ny_df = ny_df.dropna()\n",
    "#Convert SCHL to integer\n",
    "ca_df['SCHL'] = ca_df['SCHL'].astype(int)\n",
    "mi_df['SCHL'] = mi_df['SCHL'].astype(int)\n",
    "fl_df['SCHL'] = fl_df['SCHL'].astype(int)\n",
    "ut_df['SCHL'] = ut_df['SCHL'].astype(int)\n",
    "il_df['SCHL'] = il_df['SCHL'].astype(int)\n",
    "tx_df['SCHL'] = tx_df['SCHL'].astype(int)\n",
    "ny_df['SCHL'] = ny_df['SCHL'].astype(int)\n",
    "#Discretize PINCP in >50K and <=50K\n",
    "ca_df['PINCP'] = ca_df['PINCP'].apply(lambda x: 1 if x>56000 else 0)\n",
    "mi_df['PINCP'] = mi_df['PINCP'].apply(lambda x: 1 if x>56000 else 0)\n",
    "fl_df['PINCP'] = fl_df['PINCP'].apply(lambda x: 1 if x>56000 else 0)\n",
    "ut_df['PINCP'] = ut_df['PINCP'].apply(lambda x: 1 if x>56000 else 0)\n",
    "il_df['PINCP'] = il_df['PINCP'].apply(lambda x: 1 if x>56000 else 0)\n",
    "tx_df['PINCP'] = tx_df['PINCP'].apply(lambda x: 1 if x>56000 else 0)\n",
    "ny_df['PINCP'] = ny_df['PINCP'].apply(lambda x: 1 if x>56000 else 0)\n",
    "\n",
    "#Discretize education \n",
    "# Define a function to discretize SCHL values\n",
    "def discretize_schl(value):\n",
    "    if value == 0 or value == 1 or value == 2 or value == 3:  # Early Childhood Education\n",
    "        return 0\n",
    "    elif 4 <= value <= 8:  # Elementary School\n",
    "        return 1\n",
    "    elif 9 <= value <= 11:  # Middle School\n",
    "        return 2\n",
    "    elif 12 <= value <= 17:  # High School\n",
    "        return 3\n",
    "    elif 18 <= value <= 19:  # Some College\n",
    "        return 4\n",
    "    elif value == 20:  # Associate’s Degree\n",
    "        return 5\n",
    "    elif value == 21:  # Bachelor’s Degree\n",
    "        return 6\n",
    "    elif value >= 22:  # Graduate Degrees\n",
    "        return 7\n",
    "    else:\n",
    "        return None  # Handle unexpected values\n",
    "\n",
    "# Apply the function to the SCHL column\n",
    "ca_df['SCHL'] = ca_df['SCHL'].apply(discretize_schl)\n",
    "mi_df['SCHL'] = mi_df['SCHL'].apply(discretize_schl)\n",
    "fl_df['SCHL'] = fl_df['SCHL'].apply(discretize_schl)\n",
    "ut_df['SCHL'] = ut_df['SCHL'].apply(discretize_schl)\n",
    "il_df['SCHL'] = il_df['SCHL'].apply(discretize_schl)\n",
    "tx_df['SCHL'] = tx_df['SCHL'].apply(discretize_schl)\n",
    "ny_df['SCHL'] = ny_df['SCHL'].apply(discretize_schl)\n",
    "\n",
    "# Function to discretize RELP based on level of dependence\n",
    "def discretize_relp(relp_value):\n",
    "    if relp_value in [0, 1]:\n",
    "        return 'Self and Spouse'\n",
    "    elif relp_value in [2, 3, 4, 7, 14]:\n",
    "        return 'Direct Dependents'\n",
    "    elif relp_value in [5, 12, 13]:\n",
    "        return 'Peers and Similar Relations'\n",
    "    elif relp_value in [6, 8]:\n",
    "        return 'Older Generation'\n",
    "    elif relp_value in [9, 10]:\n",
    "        return 'Extended and Distant Relatives'\n",
    "    elif relp_value in [11, 15]:\n",
    "        return 'Lodgers and Non-Family Members'\n",
    "    elif relp_value in [16, 17]:\n",
    "        return 'Institutional Residents'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "for df in [ca_df, mi_df, fl_df, ut_df, il_df, tx_df, ny_df]:\n",
    "    df['RELP_category'] = df['RELP'].apply(discretize_relp)\n",
    "\n",
    "#SAVE ALL DF as CSV\n",
    "ca_df.to_csv('ca_df.csv')\n",
    "mi_df.to_csv('mi_df.csv')\n",
    "fl_df.to_csv('fl_df.csv')\n",
    "ut_df.to_csv('ut_df.csv')\n",
    "il_df.to_csv('il_df.csv')\n",
    "tx_df.to_csv('tx_df.csv')\n",
    "ny_df.to_csv('ny_df.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['18-40' '60-80' '40-60' '80-100']\n",
      "[1 0]\n",
      "[3 7 6 4 5 0 2 1]\n",
      "[1 5 3 4 2]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "#Read the data from the CSV¨\n",
    "ca_df = pd.read_csv('ca_df.csv')\n",
    "mi_df = pd.read_csv('mi_df.csv')\n",
    "fl_df = pd.read_csv('fl_df.csv')\n",
    "ut_df = pd.read_csv('ut_df.csv')\n",
    "il_df = pd.read_csv('il_df.csv')\n",
    "tx_df = pd.read_csv('tx_df.csv')\n",
    "ny_df = pd.read_csv('ny_df.csv')\n",
    "#Drop errorous column\n",
    "ca_df = ca_df.drop(columns=['Unnamed: 0'])\n",
    "mi_df = mi_df.drop(columns=['Unnamed: 0'])\n",
    "fl_df = fl_df.drop(columns=['Unnamed: 0'])\n",
    "ut_df = ut_df.drop(columns=['Unnamed: 0'])\n",
    "il_df = il_df.drop(columns=['Unnamed: 0'])\n",
    "tx_df = tx_df.drop(columns=['Unnamed: 0'])\n",
    "ny_df = ny_df.drop(columns=['Unnamed: 0'])\n",
    "#Change the RAC1P column such that it is binary: 0 if white, 1 if non-white\n",
    "ca_df['RAC1P'] = ca_df['RAC1P'].apply(lambda x: 0 if x == 1 else 1)\n",
    "mi_df['RAC1P'] = mi_df['RAC1P'].apply(lambda x: 0 if x == 1 else 1)\n",
    "fl_df['RAC1P'] = fl_df['RAC1P'].apply(lambda x: 0 if x == 1 else 1)\n",
    "ut_df['RAC1P'] = ut_df['RAC1P'].apply(lambda x: 0 if x == 1 else 1)\n",
    "il_df['RAC1P'] = il_df['RAC1P'].apply(lambda x: 0 if x == 1 else 1)\n",
    "tx_df['RAC1P'] = tx_df['RAC1P'].apply(lambda x: 0 if x == 1 else 1)\n",
    "ny_df['RAC1P'] = ny_df['RAC1P'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Print unique values for each column for california and florida\n",
    "print(ca_df['AGEP'].unique())\n",
    "print(ca_df['RAC1P'].unique())\n",
    "print(ca_df['SCHL'].unique())\n",
    "print(ca_df['MAR'].unique())\n",
    "print(ca_df['PINCP'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column PINCP has 2 unique values\n",
      "Column AGEP has 4 unique values\n",
      "Column RAC1P has 2 unique values\n",
      "Column MAR has 5 unique values\n",
      "Column SCHL has 8 unique values\n",
      "Column COW has 8 unique values\n",
      "Column OCCP has 529 unique values\n",
      "Column RELP has 18 unique values\n",
      "Column SEX has 2 unique values\n",
      "Column RELP_category has 7 unique values\n"
     ]
    }
   ],
   "source": [
    "for col in ca_df.columns:\n",
    "    print(f\"Column {col} has {len(ca_df[col].unique())} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loop through age: 100%|██████████| 5/5 [00:45<00:00,  9.03s/it]\n",
      "Loop through age: 100%|██████████| 5/5 [00:14<00:00,  2.95s/it]\n",
      "Loop through age: 100%|██████████| 5/5 [00:27<00:00,  5.49s/it]\n",
      "Loop through age: 100%|██████████| 5/5 [00:07<00:00,  1.48s/it]\n",
      "Loop through age: 100%|██████████| 5/5 [00:19<00:00,  3.82s/it]\n",
      "Loop through age: 100%|██████████| 5/5 [00:34<00:00,  6.86s/it]\n",
      "Loop through age: 100%|██████████| 5/5 [00:26<00:00,  5.35s/it]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California: Total Variation: 0.13\n",
      "California: Population level direct effect: 0.14\n",
      "California: Population level indirect effect: 0.01\n",
      "California: Experimental Spurious effect, male -0.00, female 0.00\n",
      "California: Total Variation at population level: 0.13\n",
      "\n",
      "\n",
      "California: X-specific direct effect: 0.14\n",
      "California: X-specific indirect effect: 0.01\n",
      "California: X-specific spurious effect: 0.00\n",
      "California: Total Variation at X-specific level: 0.13\n",
      "\n",
      "\n",
      "\n",
      "Michigan: Total Variation: 0.15\n",
      "Michigan: Population level direct effect: 0.17\n",
      "Michigan: Population level indirect effect: 0.02\n",
      "Michigan: Experimental Spurious effect, male -0.00, female -0.00\n",
      "Michigan: Total Variation at population level: 0.15\n",
      "\n",
      "\n",
      "Michigan: X-specific direct effect: 0.17\n",
      "Michigan: X-specific indirect effect: 0.02\n",
      "Michigan: X-specific spurious effect: 0.00\n",
      "Michigan: Total Variation at X-specific level: 0.15\n",
      "\n",
      "\n",
      "\n",
      "Florida: Total Variation: 0.13\n",
      "Florida: Population level direct effect: 0.14\n",
      "Florida: Population level indirect effect: 0.01\n",
      "Florida: Experimental Spurious effect: male -0.00, female 0.00\n",
      "Florida: Total Variation at population level: 0.13\n",
      "\n",
      "\n",
      "Florida: X-specific direct effect: 0.14\n",
      "Florida: X-specific indirect effect: 0.01\n",
      "Florida: X-specific spurious effect: 0.00\n",
      "Florida: Total Variation at X-specific level: 0.13\n",
      "\n",
      "\n",
      "\n",
      "Utah: Total Variation: 0.25\n",
      "Utah: Population level direct effect: 0.24\n",
      "Utah: Population level indirect effect: -0.01\n",
      "Utah: Experimental Spurious effect: male 0.00, female -0.00\n",
      "Utah: Total Variation at population level: 0.25\n",
      "\n",
      "\n",
      "Utah: X-specific direct effect: 0.24\n",
      "Utah: X-specific indirect effect: -0.01\n",
      "Utah: X-specific spurious effect: -0.00\n",
      "Utah: Total Variation at X-specific level: 0.25\n",
      "\n",
      "\n",
      "\n",
      "Illinois: Total Variation: 0.16\n",
      "Illinois: Population level direct effect: 0.18\n",
      "Illinois: Population level indirect effect: 0.02\n",
      "Illinois: Experimental Spurious effect: male 0.00, female -0.00\n",
      "Illinois: Total Variation at population level: 0.16\n",
      "\n",
      "\n",
      "Illinois: X-specific direct effect: 0.18\n",
      "Illinois: X-specific indirect effect: 0.02\n",
      "Illinois: X-specific spurious effect: -0.00\n",
      "Illinois: Total Variation at X-specific level: 0.16\n",
      "\n",
      "\n",
      "\n",
      "Texas: Total Variation: 0.16\n",
      "Texas: Population level direct effect: 0.17\n",
      "Texas: Population level indirect effect: 0.01\n",
      "Texas: Experimental Spurious: male -0.00, female -0.00\n",
      "Texas: Total Variation at population level: 0.16\n",
      "\n",
      "\n",
      "Texas: X-specific direct effect: 0.17\n",
      "Texas: X-specific indirect effect: 0.01\n",
      "Texas: X-specific spurious effect: 0.00\n",
      "Texas: Total Variation at X-specific level: 0.16\n",
      "\n",
      "\n",
      "\n",
      "New York: Total Variation: 0.12\n",
      "New York: Population level direct effect: 0.13\n",
      "New York: Population level indirect effect: 0.02\n",
      "New York: Experimental Spurious: male 0.00, female -0.00\n",
      "New York: Total Variation at population level: 0.12\n",
      "\n",
      "\n",
      "New York: X-specific direct effect: 0.13\n",
      "New York: X-specific indirect effect: 0.02\n",
      "New York: X-specific spurious effect: -0.00\n",
      "New York: Total Variation at X-specific level: 0.12\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "      \n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We assume, CA is our source domain and MI and FL are target domains\n",
    "\n",
    "##We want to test whether the variables are invariant across the domains (no shift in the distribution)\n",
    "##Lets first check the size of the data:\n",
    "print(f\"California data size: {ca_df.shape[0]}\")\n",
    "print(f\"Michigan data size: {mi_df.shape[0]}\")\n",
    "print(f\"Florida data size: {fl_df.shape[0]}\")\n",
    "##It is fairly large\n",
    "##Lets check the dimensionality of our individual variables\n",
    "for column in ca_df.columns:\n",
    "    print(f\"Column: {column}, California: {ca_df[column].nunique()}, Michigan: {mi_df[column].nunique()}, Florida: {fl_df[column].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From theory, we know that there are a few conditionals we need to check in order to decide whether we can directly transport over our results from California to other states.\n",
    "\n",
    "Concretely, for population and x-specific effects:\n",
    "\\begin{align*}\n",
    "& P(y|x_1,z,w), P(y|x_0,z,w), P(w|x_0,z,w),P(w|x_1,z,w), P(z),P(z|x_0),P(z|x_1)  \n",
    "\\end{align*} \n",
    "\n",
    "Let's begin with testing  whether $P(y|x_1,z,w) and P(y|x_0,z,w)$ differ substantially between California and Florida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def weighted_outcomes_bootstrap_test_schrouff(sample_a, sample_b, causal_parents_source,target, B=10000, split=0.75):\n",
    "    # Split initial samples into weighting and testing sets\n",
    "    sample_a_weights = sample_a.sample(frac=split)\n",
    "    sample_b_weights = sample_b.sample(frac=split)\n",
    "    sample_a_testing = sample_a[~sample_a.index.isin(sample_a_weights.index)]\n",
    "    sample_b_testing = sample_b[~sample_b.index.isin(sample_b_weights.index)]\n",
    "    \n",
    "    bootstrapped_t_stats = np.empty(B)\n",
    "    bootstrapped_mean_probabilities = np.empty(B)\n",
    "    bootstrapped_weights_mean = np.empty((2, B))\n",
    "    bootstrapped_weighted_outcome = np.empty((2, B))\n",
    "    bootstrapped_pooled_se = np.empty(B)\n",
    "    successful_iterations = 0\n",
    "    i = 0\n",
    "    while successful_iterations < B:\n",
    "\n",
    "        if successful_iterations % 100 == 0:\n",
    "            print(f'Iteration {successful_iterations} of {B}')\n",
    "\n",
    "        try:\n",
    "            # Sample with replacement from the weights and testing sets\n",
    "            bootstrap_sample_a_weights = sample_a_weights.sample(n=len(sample_a_weights), replace=True)\n",
    "            bootstrap_sample_b_weights = sample_b_weights.sample(n=len(sample_b_weights), replace=True)\n",
    "            bootstrap_sample_a_testing = sample_a_testing.sample(n=len(sample_a_testing), replace=True)\n",
    "            bootstrap_sample_b_testing = sample_b_testing.sample(n=len(sample_b_testing), replace=True)\n",
    "\n",
    "            # Pool the samples for fitting the model\n",
    "            pooled_weights = pd.concat([bootstrap_sample_a_weights, bootstrap_sample_b_weights])\n",
    "            pooled_weights['Sample'] = ['Sample A'] * len(bootstrap_sample_a_weights) + ['Sample B'] * len(bootstrap_sample_b_weights)\n",
    "\n",
    "            # Prepare the feature matrix and target variable\n",
    "            X = pooled_weights[causal_parents_source]\n",
    "            Y = pooled_weights['Sample'].replace({'Sample A': 0, 'Sample B': 1})\n",
    "    \n",
    "            encoder = OneHotEncoder(drop='first')\n",
    "            X_encoded = encoder.fit_transform(X)\n",
    "            X_encoded_df = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names_out(X.columns), index=pooled_weights.index)\n",
    "            X_encoded_df = sm.add_constant(X_encoded_df).astype(int)\n",
    "\n",
    "            # Fit the logistic regression model\n",
    "            logit_model = sm.Logit(Y, X_encoded_df).fit(disp=0)\n",
    "\n",
    "            # Extract weights on the testing set\n",
    "            X_a = bootstrap_sample_a_testing[causal_parents_source]\n",
    "            X_a_encoded = encoder.transform(X_a)\n",
    "            X_a_encoded_df = pd.DataFrame(X_a_encoded.toarray(), columns=encoder.get_feature_names_out(X.columns), index=bootstrap_sample_a_testing.index)\n",
    "            X_a_encoded_df = sm.add_constant(X_a_encoded_df).astype(int)\n",
    "\n",
    "            X_b = bootstrap_sample_b_testing[causal_parents_source]\n",
    "            X_b_encoded = encoder.transform(X_b)\n",
    "            X_b_encoded_df = pd.DataFrame(X_b_encoded.toarray(), columns=encoder.get_feature_names_out(X.columns), index=bootstrap_sample_b_testing.index)\n",
    "            X_b_encoded_df = sm.add_constant(X_b_encoded_df).astype(int)\n",
    "\n",
    "            # Compute probabilities\n",
    "            p_a = logit_model.predict(X_a_encoded_df)\n",
    "            p_a = 1 - p_a  # Adjust for class probability\n",
    "            p_b = logit_model.predict(X_b_encoded_df)\n",
    "\n",
    "            bootstrapped_mean_probabilities[successful_iterations] = np.mean(p_a)\n",
    "\n",
    "            if np.any(p_a == 0) or np.any(p_b == 0):\n",
    "                print(f'Warning: Zero values in p_a or p_b in iteration {successful_iterations}')\n",
    "            if np.any(p_a < 1e-10) or np.any(p_b < 1e-10):\n",
    "                print(f'Warning: Near-zero values in p_a or p_b in iteration {successful_iterations}')\n",
    "\n",
    "            # Compute weights and normalize\n",
    "            w_a = 1 / p_a\n",
    "            w_b = 1 / p_b\n",
    "            w_a_norm = w_a / np.sum(w_a)\n",
    "            w_b_norm = w_b / np.sum(w_b)\n",
    "\n",
    "            if np.any(np.isnan(w_a)) or np.any(np.isnan(w_b)):\n",
    "                print(f'Warning: NaN values in w_a or w_b in iteration {successful_iterations}')\n",
    "            if np.any(np.isinf(w_a)) or np.any(np.isinf(w_b)):\n",
    "                print(f'Warning: Inf values in w_a or w_b in iteration {successful_iterations}')\n",
    "\n",
    "            Y_a = bootstrap_sample_a_testing[target].values\n",
    "            Y_b = bootstrap_sample_b_testing[target].values\n",
    "            product_a = w_a_norm.values * Y_a\n",
    "            product_b = w_b_norm.values * Y_b\n",
    "            together = np.concatenate([product_a, -product_b])\n",
    "            bootstrapped_t_stats[successful_iterations] = ttest_1samp(together, 0)[0]\n",
    "\n",
    "            bootstrapped_weighted_outcome[0, successful_iterations] = np.sum(product_a)\n",
    "            bootstrapped_weighted_outcome[1, successful_iterations] = np.sum(product_b)\n",
    "            bootstrapped_weights_mean[0, successful_iterations] = np.mean(w_a)\n",
    "            bootstrapped_weights_mean[1, successful_iterations] = np.mean(w_b)\n",
    "\n",
    "            # Increment successful iterations\n",
    "            successful_iterations += 1\n",
    "            i += 1  # Keep track of total iterations attempted\n",
    "\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(f'Iteration {i} skipped due to singular matrix.')\n",
    "            i += 1  # Increment regardless of failure\n",
    "            continue  # Try again without incrementing successful iterations\n",
    "        except Exception as e:\n",
    "            print(f'Iteration {i} failed due to: {e}')\n",
    "            i += 1  # Increment regardless of failure\n",
    "            continue  # Try again without incrementing successful iterations\n",
    "    # Step 3: Compute p-values\n",
    "    mean = np.mean(bootstrapped_t_stats)\n",
    "    std = np.std(bootstrapped_t_stats)\n",
    "    p_value = (2 * norm.cdf(-abs(mean / std)))\n",
    "\n",
    "    return bootstrapped_t_stats, p_value, bootstrapped_weighted_outcome, bootstrapped_mean_probabilities, bootstrapped_weights_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of 1000\n",
      "Iteration 100 of 1000\n",
      "Iteration 200 of 1000\n",
      "Iteration 300 of 1000\n",
      "Iteration 400 of 1000\n",
      "Iteration 500 of 1000\n",
      "Iteration 600 of 1000\n",
      "Iteration 700 of 1000\n",
      "Iteration 800 of 1000\n",
      "Iteration 900 of 1000\n",
      "Iteration 0 of 1000\n",
      "Iteration 100 of 1000\n",
      "Iteration 200 of 1000\n",
      "Iteration 300 of 1000\n",
      "Iteration 400 of 1000\n",
      "Iteration 500 of 1000\n",
      "Iteration 600 of 1000\n",
      "Iteration 700 of 1000\n",
      "Iteration 800 of 1000\n",
      "Iteration 900 of 1000\n"
     ]
    }
   ],
   "source": [
    "#Compare P(Income => 50k|male,age,race,marital,education) and  P(Income => 50k|female,age,race,marital,education) for California and Florida\n",
    "##filter both samples for male and female\n",
    "male_ca_df = ca_df[ca_df['SEX']==1]\n",
    "female_ca_df = ca_df[ca_df['SEX']==2]\n",
    "male_fl_df = fl_df[fl_df['SEX']==1]\n",
    "female_fl_df = fl_df[fl_df['SEX']==2]\n",
    "##perform the test with calfiornia as source and florida as target\n",
    "causal_parents_source = ['AGEP','RAC1P','MAR','SCHL']\n",
    "target = 'PINCP'\n",
    "male_ca_fl_t_stats, male_ca_fl_p_value, male_ca_fl_weighted_outcome, _, _ = weighted_outcomes_bootstrap_test_schrouff(male_ca_df,male_fl_df, causal_parents_source,target, B=1000, split=0.5)\n",
    "female_ca_fl_t_stats, female_ca_fl_p_value, female_ca_fl_weighted_outcome, _, _ = weighted_outcomes_bootstrap_test_schrouff(female_ca_df,female_fl_df, causal_parents_source,target, B=1000, split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the bootstrapped t-statistics and weighted outcomes\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 12))\n",
    "#Histograms of the differences in weighted outcomes and the weighted outcomes itself\n",
    "axs[0,0].hist(male_ca_fl_t_stats, bins=50, alpha=0.3, color='blue', edgecolor='black', label = \"Bootstrapped\")\n",
    "#Add mean difference as vertical line\n",
    "axs[0,0].axvline(np.mean(male_ca_fl_t_stats), color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "axs[0,0].set_title('Male: Bootstrapped t-statistics')\n",
    "axs[0,0].set_xlabel('t-statistics')\n",
    "axs[0,0].set_ylabel('Frequency')\n",
    "#Add mean values to legend and p-values\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Mean')]\n",
    "axs[0,0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {male_ca_fl_p_value}')])  \n",
    "\n",
    "axs[0,1].hist(male_ca_fl_weighted_outcome[0], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[0,1].hist(male_ca_fl_weighted_outcome[1], bins=50, alpha=0.3, color='red', edgecolor='black')\n",
    "#Add mean values as vertical lines\n",
    "axs[0,1].axvline(np.mean(male_ca_fl_weighted_outcome[0]), color='blue', linestyle='dashed', linewidth=2, label='Mean California')\n",
    "axs[0,1].axvline(np.mean(male_ca_fl_weighted_outcome[1]), color='red', linestyle='dashed', linewidth=2, label='Mean Florida')\n",
    "axs[0,1].set_title('Weighted outcomes: California vs Florida (male)')\n",
    "axs[0,1].set_xlabel('Weighted outcomes')\n",
    "axs[0,1].set_ylabel('Frequency')\n",
    "#Add mean values to legend\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='California'), plt.Line2D([0], [0], color='red', lw=1, label='Florida')]\n",
    "axs[0,1].legend(handles=handles)\n",
    "\n",
    "axs[1,0].hist(female_ca_fl_t_stats, bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "#Add mean difference as vertical line\n",
    "axs[1,0].axvline(np.mean(female_ca_fl_t_stats), color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "axs[1,0].set_title('Female: Bootstrapped t-statistics')\n",
    "axs[1,0].set_xlabel('t-statistics')\n",
    "axs[1,0].set_ylabel('Frequency')\n",
    "#Add mean values to legend and p-values\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='Bootstrap'), plt.Line2D([0], [0], color='red', lw=1, label='Mean')]\n",
    "axs[1,0].legend(handles=handles + [plt.Line2D([0], [0], color='none', lw=1, label=f'p-value = {female_ca_fl_p_value}')])\n",
    "\n",
    "axs[1,1].hist(female_ca_fl_weighted_outcome[0], bins=50, alpha=0.3, color='blue', edgecolor='black')\n",
    "axs[1,1].hist(female_ca_fl_weighted_outcome[1], bins=50, alpha=0.3, color='red', edgecolor='black')\n",
    "#Add mean values as vertical lines\n",
    "axs[1,1].axvline(np.mean(female_ca_fl_weighted_outcome[0]), color='blue', linestyle='dashed', linewidth=2, label='Mean California')\n",
    "axs[1,1].axvline(np.mean(female_ca_fl_weighted_outcome[1]), color='red', linestyle='dashed', linewidth=2, label='Mean Florida')\n",
    "axs[1,1].set_title('Weighted outcomes: California vs Florida (female)')\n",
    "axs[1,1].set_xlabel('Weighted outcomes')\n",
    "axs[1,1].set_ylabel('Frequency')\n",
    "#Add mean values to legend\n",
    "handles = [plt.Line2D([0], [0], color='blue', lw=1, label='California'), plt.Line2D([0], [0], color='red', lw=1, label='Florida')]\n",
    "axs[1,1].legend(handles=handles)\n",
    "plt.show()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PINCP</th>\n",
       "      <th>AGEP</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>MAR</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>SEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>60-80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0-20</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>20-40</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0-20</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>60-80</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172521</th>\n",
       "      <td>0</td>\n",
       "      <td>40-60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172524</th>\n",
       "      <td>0</td>\n",
       "      <td>40-60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172525</th>\n",
       "      <td>0</td>\n",
       "      <td>40-60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172527</th>\n",
       "      <td>0</td>\n",
       "      <td>40-60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172528</th>\n",
       "      <td>0</td>\n",
       "      <td>80-100</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83294 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PINCP    AGEP  RAC1P  MAR  SCHL  SEX\n",
       "0           0   60-80      1    1     3    1\n",
       "1           0    0-20      1    5     3    1\n",
       "2           0   20-40      1    5     3    1\n",
       "3           0    0-20      1    5     3    1\n",
       "4           0   60-80      0    5     0    1\n",
       "...       ...     ...    ...  ...   ...  ...\n",
       "172521      0   40-60      0    1     3    1\n",
       "172524      0   40-60      0    1     4    1\n",
       "172525      0   40-60      0    1     6    1\n",
       "172527      0   40-60      1    1     3    1\n",
       "172528      0  80-100      0    2     3    1\n",
       "\n",
       "[83294 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 312816), (40, 172529))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unique combinations of marital and education status\n",
    "ca_df['MAR_SCHL'] = ca_df['MAR'].astype(str) + '_' + ca_df['SCHL'].astype(str)\n",
    "fl_df['MAR_SCHL'] = fl_df['MAR'].astype(str) + '_' + fl_df['SCHL'].astype(str)\n",
    "male_ca_df['MAR_SCHL'] = male_ca_df['MAR'].astype(str) + '_' + male_ca_df['SCHL'].astype(str)\n",
    "male_fl_df['MAR_SCHL'] = male_fl_df['MAR'].astype(str) + '_' + male_fl_df['SCHL'].astype(str)\n",
    "female_ca_df['MAR_SCHL'] = female_ca_df['MAR'].astype(str) + '_' + female_ca_df['SCHL'].astype(str)\n",
    "female_fl_df['MAR_SCHL'] = female_fl_df['MAR'].astype(str) + '_' + female_fl_df['SCHL'].astype(str)\n",
    "\n",
    "male_probs_ca = np.empty((len(ca_df['MAR_SCHL'].unique()), len(ca_df['Age_Race'])))\n",
    "male_probs_fl = np.empty((len(fl_df['MAR_SCHL'].unique()), len(fl_df['Age_Race'])))\n",
    "\n",
    "male_probs_ca.shape,male_probs_fl.shape\n",
    "\n",
    "\n",
    "                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test P(age,race) and their conditional versions based on SEX, by using Chi-Squared Testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California vs Florida: Chi-square statistic = 21285.02, p-value = 0.00\n",
      "California vs Florida (male): Chi-square statistic = 10255.76, p-value = 0.00\n",
      "California vs Florida (female): Chi-square statistic = 11035.29, p-value = 0.00\n"
     ]
    }
   ],
   "source": [
    "#Unique combinations of age,race\n",
    "ca_df['Age_Race'] = ca_df['AGEP'].astype(str) + '_' + ca_df['RAC1P'].astype(str)\n",
    "male_ca_df['Age_Race'] = male_ca_df['AGEP'].astype(str) + '_' + male_ca_df['RAC1P'].astype(str)\n",
    "female_ca_df['Age_Race'] = female_ca_df['AGEP'].astype(str) + '_' + female_ca_df['RAC1P'].astype(str)\n",
    "fl_df['Age_Race'] = fl_df['AGEP'].astype(str) + '_' + fl_df['RAC1P'].astype(str)\n",
    "male_fl_df['Age_Race'] = male_fl_df['AGEP'].astype(str) + '_' + male_fl_df['RAC1P'].astype(str)\n",
    "female_fl_df['Age_Race'] = female_fl_df['AGEP'].astype(str) + '_' + female_fl_df['RAC1P'].astype(str)\n",
    "##Create all possible contingency tables\n",
    "pooled_ca_fl_df = pd.concat([ca_df,fl_df])\n",
    "pooled_ca_fl_df['Sample'] = ['California'] * ca_df.shape[0] + ['Florida'] * fl_df.shape[0]\n",
    "contingency_ca_fl = pd.crosstab(pooled_ca_fl_df['Age_Race'], pooled_ca_fl_df['Sample'])\n",
    "pooled_male_ca_fl_df = pd.concat([male_ca_df,male_fl_df])\n",
    "pooled_male_ca_fl_df['Sample'] = ['California']*male_ca_df.shape[0] + ['Florida']*male_fl_df.shape[0]\n",
    "pooled_female_ca_fl_df = pd.concat([female_ca_df,female_fl_df])\n",
    "pooled_female_ca_fl_df['Sample'] = ['California']*female_ca_df.shape[0] + ['Florida']*female_fl_df.shape[0]\n",
    "contingency_male_ca_fl = pd.crosstab(pooled_male_ca_fl_df['Age_Race'], pooled_male_ca_fl_df['Sample'])\n",
    "contingency_female_ca_fl = pd.crosstab(pooled_female_ca_fl_df['Age_Race'], pooled_female_ca_fl_df['Sample'])\n",
    "##Compute the chi-square statistics and p-values\n",
    "chi2_stat_ca_fl, p_val_ca_fl, _, _ = chi2_contingency(contingency_ca_fl)\n",
    "chi2_stat_male_ca_fl, p_val_male_ca_fl,_,_ = chi2_contingency(contingency_male_ca_fl)\n",
    "chi2_stat_female_ca_fl, p_val_female_ca_fl,_,_ = chi2_contingency(contingency_female_ca_fl)\n",
    "##Print results\n",
    "print(f'California vs Florida: Chi-square statistic = {chi2_stat_ca_fl:.2f}, p-value = {p_val_ca_fl:.2f}')\n",
    "print(f'California vs Florida (male): Chi-square statistic = {chi2_stat_male_ca_fl:.2f}, p-value = {p_val_male_ca_fl:.2f}')\n",
    "print(f'California vs Florida (female): Chi-square statistic = {chi2_stat_female_ca_fl:.2f}, p-value = {p_val_female_ca_fl:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.275"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11/40"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
